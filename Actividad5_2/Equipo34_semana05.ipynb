{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Tecnologico-de-Monterrey-MNA/nlp-2023_Equipo-6/blob/main/Equipo_06_Semana_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xq_UEIE1t75y"
   },
   "source": [
    "# **Objetivo de la Actividad**\n",
    "\n",
    "\n",
    "> Trabajar con estos modelos pre-entrenados, generando el vocabulario a partir de tu conjunto de datos de entrenamiento.\n",
    "\n",
    "Para cada palabra de tu vocabulario, podrás sustituirlo por su correspondiente\n",
    "vector continuo. En caso de que no exista el vector para una palabra en particular, se puede eliminar dicha palabra, o bien sustituirla por el vector continuo más cercano.\n",
    "\n",
    "En esta actividad deberás aplicar esta segunda opción.\n",
    "\n",
    "-----\n",
    "\n",
    "*   Existen diversas propuestas para utilizar dichos vectores continuos como entrada para modelos de aprendizaje automático. En particular, en esta actividad cada enunciado será sustituido por el vector promedio de todos los tokens que lo forman.\n",
    "\n",
    "**Modelos:**\n",
    "\n",
    "Modelo de vectores **continuos/embebidos FastText**, es decir, el modelo desarrollado por Facebook en 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3D1AhgmjvJ9C"
   },
   "source": [
    "## **Pregunta 1**\n",
    "\n",
    "Descarga los **3 archivos de Canvas**. En particular, el archivo de datos de **IMDb** ya no requiere transformarse **para obtener sus 1000 registros**. Al cargar los datos de los tres archivos deberás tener un **DataFrame de Pandas de 3000 registros**, con sus etiquetas. Los archivos los encuentras en Canvas y se llaman: **amazon5.txt, imdb5.txt, yelp5.txt.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "tZ_8l4dhwEWV",
    "outputId": "2759246b-9316-46dc-c8ee-dff6378bcd9e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ivan9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, RegexpStemmer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "5JFdsxNv67X0",
    "outputId": "f871100e-ad5e-44d6-8054-0b3f14506b87"
   },
   "outputs": [],
   "source": [
    "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
    "# !gunzip cc.en.300.vec.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uGjkDFP4vtr"
   },
   "source": [
    "#**Aplicando NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "3nyPbMQ5v7Ly",
    "outputId": "a004dd72-af59-45a6-d205-b809d96b7065"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ivan9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ivan9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')        # Tokenizador que ayuda a dividr el texto en enunciados\n",
    "nltk.download('stopwords')    # Acceso a \"stopwords\" en varios idiomas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "uIq_dbSr0EmT",
    "outputId": "729679a0-ee9e-4209-a7a8-970a7772fd57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Lista de stopwords que se incluyen de manera predeterminada la suite de librerías de NLTK\n",
    "\n",
    "print(len(stopwords.words('english')))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "fzJpjSXPvdRC",
    "outputId": "ca1e4533-83ab-4174-a586-b30b97a25031"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ivan9\\AppData\\Local\\Temp\\ipykernel_25924\\3513401339.py:8: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  dfi5 = pd.read_csv(url2, sep=' {3,4}', names=['review','label'], header=None, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de registros de Amazon_5: (1000, 2)\n",
      "Total de registros de IMBD_5: (1000, 2)\n",
      "Total de registros de Yelp_5: (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "#Se extraen los archivos desde repositorio público de GitHub\n",
    "url1 = 'https://raw.githubusercontent.com/Handrum/NLP_EQ_6/main/amazon5.txt'\n",
    "url2 = 'https://raw.githubusercontent.com/Handrum/NLP_EQ_6/main/imdb5.txt'\n",
    "url3 = 'https://raw.githubusercontent.com/Handrum/NLP_EQ_6/main/yelp5.txt'\n",
    "\n",
    "#Se cargan los archivos en dataframe de Pandas\n",
    "dfa5 = pd.read_csv(url1, sep='\\t', names=['review','label'], header=None, encoding='utf-8')\n",
    "dfi5 = pd.read_csv(url2, sep=' {3,4}', names=['review','label'], header=None, encoding='utf-8')\n",
    "dfy5 = pd.read_csv(url3, sep='\\t', names=['review','label'], header=None, encoding='utf-8')\n",
    "\n",
    "#Se imprime la forma de los dataframes\n",
    "print('Total de registros de Amazon_5:',dfa5.shape)\n",
    "print('Total de registros de IMBD_5:',dfi5.shape)\n",
    "print('Total de registros de Yelp_5:',dfy5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "dOAHRDNEz6sY",
    "outputId": "957f62e9-18c8-4e76-ca40-925d4b4668fc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  A very, very, very slow-moving, aimless movie ...      0\n",
       "1  Not sure who was more lost - the flat characte...      0\n",
       "2  Attempting artiness with black & white and cle...      0\n",
       "3         Very little music or anything to speak of.      0\n",
       "4  The best scene in the movie was when Gerardo i...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfi5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "CQyhZt_z_rg0",
    "outputId": "75815526-8b4b-4b36-afd3-71a61db89f8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de registros (3000, 2)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000 entries, 0 to 2999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   review  3000 non-null   object\n",
      " 1   label   3000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 47.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#Concatenar los 3,000 registros\n",
    "\n",
    "df = pd.concat([dfa5, dfi5, dfy5], ignore_index=True)\n",
    "print('Numero de registros',df.shape)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6A-W_SItAd_T"
   },
   "source": [
    "Al realizar la revisión del conjunto de datos se observa que el dataset de **imdb5** tiene 1.000 flotantes. Lo que requiere una limpieza especial para dicho dataset. --- **Sin embargo, se encontró que al cargar con el separador correcto, el siguiente paso no es necesario**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kryovbUFAF2F"
   },
   "outputs": [],
   "source": [
    "#dfi5.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "V1bTBFzOBjes"
   },
   "outputs": [],
   "source": [
    "#no necesario\n",
    "#print(dfi5_c.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1s3xzHiQB0HU"
   },
   "outputs": [],
   "source": [
    "#no necesario con la limpieza anterior\n",
    "#print(df['label'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "txUZL511EdAO"
   },
   "outputs": [],
   "source": [
    "#no necesario con la limpieza anterior\n",
    "#dfii5_l = dfi5.fillna(0)\n",
    "#dfii5_l.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Ci9f44O5GIA1"
   },
   "outputs": [],
   "source": [
    "#No necesario con la limpieza anterior\n",
    "#df = pd.concat([dfa5, dfii5_l, dfy5], ignore_index=True)\n",
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "N6dqTvNUIGqG"
   },
   "outputs": [],
   "source": [
    "X = df.review     # Serie de strings\n",
    "Y = df.label      # Serie de enteros 0s y 1s\n",
    "\n",
    "assert X.shape == (3000,)           # verificando que tenemos la dimensiones esperadas.\n",
    "assert Y.shape == (3000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bl9HFj-u4mnk"
   },
   "source": [
    "# **Pregunta 2**\n",
    "\n",
    "Realiza de nuevo un proceso de limpieza. Aplica el preprocesamiento que consideres adecuado, sin embargo, deberás aplicar necesariamente alguna de las técnicas de lematización. Como aplicaremos modelos embebidos pre-entrenados, queremos palabras lo más cercanas a las existentes en un idioma, inglés en este caso. Aplica y justifica cualquier otro proceso de limpieza que consideres\n",
    "adecuado. Recuerda que en esta actividad se usarán vectores embebidos para un problema de clasificación, por lo que deberás tomar de acuerdo a este contexto. Justifica todas las transformaciones que se apliquen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyZ7I3P12JbD"
   },
   "source": [
    "**Procedimiento para quitar las negaciones del conjunto de stopwords en caso de ser necesario**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "OYEURcKY1t-I",
    "outputId": "4c57f3d2-5dda-4d37-b955-3cd3e925264c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "['no', 'nor', 'not', 'ain', 'aren', \"aren't\", 'don', \"don't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "139\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ma']\n"
     ]
    }
   ],
   "source": [
    "# Consideremos la siguiente lista de palabras asociada a negaciones en inglés:\n",
    "\n",
    "mystopwords = stopwords.words('english')\n",
    "\n",
    "negwords = [ 'no', 'nor', 'not', 'ain', 'aren', \"aren't\", 'don', \"don't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "#Se asigna las stopwords de NLTK a mystopwords\n",
    "mystopwords_without_neg = stopwords.words('english')\n",
    "\n",
    "#Se revisa si cada una de las palabras en negwords está presente en mystopwords\n",
    "for word in negwords:\n",
    "    if word in mystopwords_without_neg:\n",
    "      mystopwords_without_neg.remove(word)           # si la palabra está presente, quitarla de la lista\n",
    "\n",
    "#Se imprime la longitud y elementos de negwords para verificar resultados\n",
    "print(len(negwords))\n",
    "print(negwords)\n",
    "\n",
    "#Se imprime la longitud y elementos de los stop words de NLTK para verificar resultados\n",
    "print(len(mystopwords))\n",
    "print(mystopwords)\n",
    "\n",
    "print(len(mystopwords_without_neg))\n",
    "print(mystopwords_without_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDyc80seLI_W"
   },
   "source": [
    "**NOTA**: Es importante tener en cuenta, que en el análisis de sentimientos debemos abarcar el vocabulario suficiente que nos permita entender la opinión del usuario y así mismo identificar aquellas opiniones que puden enriquecer y construir una interacción más amena con el cliente. Para ello, Se considera realizar una depuración de las negaciones que no agregan valor al resultado del diccionario que se requiere y que no definen a profundidad el sentimiento del cliente, estas se pueden ser reemplazadas por un vector continuo más cercano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4XLAEojC5Gbr"
   },
   "outputs": [],
   "source": [
    "def clean_tok(doc):\n",
    "    ##############################################################################\n",
    "    # AGREGA AQUÍ TUS LÍNEAS DE CÓDIGO - Pregunta 4:\n",
    "    \n",
    "    # Eliminación de signos de puntuación, caracteres especiales.\n",
    "    \n",
    "    doc = doc.lower()    #normalización a minúsculas\n",
    "    \n",
    "    #Sólo caracteres alfabéticos\n",
    "    puntuacion = re.sub(r'[^a-z]', ' ', doc)                  #considerar solo caracteres alfabéticos\n",
    "    puntuacion = re.sub(r'\\s{2, }', ' ', puntuacion.strip())  #eliminar todo tipo de espacios que se encuentren\n",
    "    \n",
    "    # Tokenizar\n",
    "    tokenizar = puntuacion.split()                            #tomar el resultado anterior y aplicar método de tokenización a partir del método split\n",
    "    \n",
    "    # Eliminación de Stopwords\n",
    "    \n",
    "    tokens = [token for token in tokenizar if (token not in mystopwords and len(token) >1)]  #en esta ocasión para elimianr los stopwords se toma la librería de corpus el mismo método.\n",
    "    \n",
    "    # FIN PARA AGREGAR TUS LÍNEAS DE CÓDIGO.\n",
    "    ##############################################################################\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "h3cUq1k-LBXt"
   },
   "outputs": [],
   "source": [
    "Xcleantok = [clean_tok(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "QK2ETk5nMCOG",
    "outputId": "e3a3c5e6-cd15-4885-ef77-06ea306c6963"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  So there is no way for me to plug it in here i...      0\n",
       "1                        Good case, Excellent value.      1\n",
       "2                             Great for the jawbone.      1\n",
       "3  Tied to charger for conversations lasting more...      0\n",
       "4                                  The mic is great.      1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "lXM_3HnKLIxl",
    "outputId": "a41c7249-d159-4aed-d91c-c1f836c0b523"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['way', 'plug', 'us', 'unless', 'go', 'converter']\n",
      "['good', 'case', 'excellent', 'value']\n",
      "['great', 'jawbone']\n",
      "['tied', 'charger', 'conversations', 'lasting', 'minutes', 'major', 'problems']\n",
      "['mic', 'great']\n"
     ]
    }
   ],
   "source": [
    "for x in Xcleantok[0:5]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDs04xbyRyh2"
   },
   "source": [
    "# **Método Limpieza por Lematizazión**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "tSMPI-XoSELt",
    "outputId": "8505f8c5-7285-4b2f-cc83-b7158c2f4e3a"
   },
   "outputs": [],
   "source": [
    "#lista vacía para análisis visual\n",
    "palabras = []\n",
    "\n",
    "#ciclo para juntar todos los tokens previos en una lista\n",
    "for tokens1 in Xcleantok:\n",
    "\n",
    "  palabras.extend(tokens1)\n",
    "\n",
    "#Set para eliminar repeticiones\n",
    "dic = set(palabras)\n",
    "\n",
    "#Imprimir resultados para análisis visual\n",
    "#print(sorted(dic))\n",
    "\n",
    "\n",
    "#Se crea un objeto de la clase PorterStemmer\n",
    "#ps = PorterStemmer() #Se requiere usar lematización para utilizar el método de vectores embebidos\n",
    "\n",
    "#Se crea un objeto de la clase WordNetLemmatizer\n",
    "WNL = WordNetLemmatizer()\n",
    "\n",
    "#Definición de la función de limpieza adicional\n",
    "def clean_doc(doc):\n",
    "\n",
    "    #Se define lista vacía para los nuevos tokens\n",
    "    # tokens = stemmer_tokens(doc)\n",
    "    clean_tokens = []\n",
    "    \n",
    "    #Ciclo para la limpieza de los tokens\n",
    "    for token in doc:\n",
    "\n",
    "        no_dups_token = re.sub(r'([a-z])\\1{2,}', r'\\1\\1', token) \n",
    "        # Se aplica un filtro con regex para eliminar palabras donde se repita más de 2 veces seguidas la misma letra\n",
    "\n",
    "        \n",
    "        if len(no_dups_token) == 2 and no_dups_token.endswith('s'):\n",
    "            # Se aplica un filtro para las palabras como \"as\", \"is\", y \"us\", que no se les elimine la \"s\" final por la lematización\n",
    "            clean_tokens.append(no_dups_token)\n",
    "            continue\n",
    "        elif len(no_dups_token) == 3 and no_dups_token.endswith('ed'):\n",
    "            # Se aplica un filtro para las palabras como \"ted\", \"fed\", y \"ned\", que no se les eliminen las \"ed\" finales por la lematización\n",
    "            clean_tokens.append(no_dups_token)\n",
    "            continue\n",
    "        elif len(no_dups_token) == 4 and no_dups_token.endswith('ing'):\n",
    "            # Se aplica un filtro para las palabras como \"ping\", \"bing\", y \"ring\", que no se les eliminen las \"ing\" finales por la lematización\n",
    "            clean_tokens.append(no_dups_token)\n",
    "            continue\n",
    "            \n",
    "        #Se aplica stemming lo cual nos llevará las palabras a su base, incluyendo remover las terminaciones 'ing','ed','s'\n",
    "        #doc[j] = ps.stem(doc[j]) #Se aplica lematización en lugar de stemming\n",
    "    \n",
    "        #Sólo lematizar tokens mas de más de dos caracteres\n",
    "        if len(token) < 2:\n",
    "            continue\n",
    "    \n",
    "        #Se intenta lematizar verbo\n",
    "        lem_token = WNL.lemmatize(token,'v')\n",
    "        \n",
    "        #Si el token permanece sin cambios, intentar lematizar como sustantivo\n",
    "        if lem_token == no_dups_token:\n",
    "            lem_token = WNL.lemmatize(token, 'n')\n",
    "        \n",
    "        #Si el token permanece sin cambios, intentar lematizar como adjetivo\n",
    "        if lem_token == no_dups_token:\n",
    "            lem_token = WNL.lemmatize(token,'a')\n",
    "        \n",
    "        #Si el token permanece sin cambios, intentar lematizar como advervio\n",
    "        if lem_token == no_dups_token:\n",
    "            lem_token = WNL.lemmatize(token,'r')\n",
    "        \n",
    "        #Agregar el resultado a la lista de tokens límpios (clean_tokens)\n",
    "        clean_tokens.append(lem_token)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "1pIMQaRSSMfd"
   },
   "outputs": [],
   "source": [
    "# Aplicamos el proceso de limpieza/normalización adicionales:\n",
    "\n",
    "Xclean = [clean_doc(x) for x in Xcleantok]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparamos la información antes y después de la limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['way', 'plug', 'us', 'unless', 'go', 'converter'],\n",
       " ['good', 'case', 'excellent', 'value'],\n",
       " ['great', 'jawbone'],\n",
       " ['tied',\n",
       "  'charger',\n",
       "  'conversations',\n",
       "  'lasting',\n",
       "  'minutes',\n",
       "  'major',\n",
       "  'problems'],\n",
       " ['mic', 'great'],\n",
       " ['jiggle', 'plug', 'get', 'line', 'right', 'get', 'decent', 'volume'],\n",
       " ['several',\n",
       "  'dozen',\n",
       "  'several',\n",
       "  'hundred',\n",
       "  'contacts',\n",
       "  'imagine',\n",
       "  'fun',\n",
       "  'sending',\n",
       "  'one',\n",
       "  'one'],\n",
       " ['razr', 'owner', 'must'],\n",
       " ['needless', 'say', 'wasted', 'money'],\n",
       " ['waste', 'money', 'time']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xcleantok[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "F0OxOXN1STP1",
    "outputId": "0f41787f-61a9-4cdc-d03c-43858433042c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['way', 'plug', 'us', 'unless', 'go', 'converter'],\n",
       " ['good', 'case', 'excellent', 'value'],\n",
       " ['great', 'jawbone'],\n",
       " ['tie', 'charger', 'conversation', 'last', 'minute', 'major', 'problem'],\n",
       " ['mic', 'great'],\n",
       " ['jiggle', 'plug', 'get', 'line', 'right', 'get', 'decent', 'volume'],\n",
       " ['several',\n",
       "  'dozen',\n",
       "  'several',\n",
       "  'hundred',\n",
       "  'contact',\n",
       "  'imagine',\n",
       "  'fun',\n",
       "  'send',\n",
       "  'one',\n",
       "  'one'],\n",
       " ['razr', 'owner', 'must'],\n",
       " ['needle', 'say', 'waste', 'money'],\n",
       " ['waste', 'money', 'time']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xclean[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDOBBjn9Wp__"
   },
   "source": [
    "# **Pregunta 3**\n",
    "\n",
    "Llamar Xclean a los comentarios procesados y Y a las etiquetas. Realicemos una partición aleatoria con los mismos porcentajes de la práctica pasada para poder comparar dichos resultados con los de esta actividad, a saber, 70%, 15% y 15%, para entrenamiento, validación y prueba, respectivamente. Verifica que obtienes 2,100 registros de entrenamiento y 450 para cada uno de validación y prueba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['way', 'plug', 'us', 'unless', 'go', 'converter'],\n",
       " ['good', 'case', 'excellent', 'value'],\n",
       " ['great', 'jawbone'],\n",
       " ['tie', 'charger', 'conversation', 'last', 'minute', 'major', 'problem'],\n",
       " ['mic', 'great'],\n",
       " ['jiggle', 'plug', 'get', 'line', 'right', 'get', 'decent', 'volume'],\n",
       " ['several',\n",
       "  'dozen',\n",
       "  'several',\n",
       "  'hundred',\n",
       "  'contact',\n",
       "  'imagine',\n",
       "  'fun',\n",
       "  'send',\n",
       "  'one',\n",
       "  'one'],\n",
       " ['razr', 'owner', 'must'],\n",
       " ['needle', 'say', 'waste', 'money'],\n",
       " ['waste', 'money', 'time']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xclean[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "w88pAxNzX3Zu",
    "outputId": "8a8ad41c-8dfe-4e45-b758-50157807f75d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X,y Train: 2100 2100\n",
      "X,y Val: 450 450\n",
      "X,y Test 450 450\n"
     ]
    }
   ],
   "source": [
    "# Xclean = Comentarios procesados\n",
    "# Y = etiquetas\n",
    "\n",
    "x_train, x_val_and_test, y_train, y_val_and_test = train_test_split(Xclean, Y, train_size=.70, shuffle=True, random_state=1)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_val_and_test, y_val_and_test, test_size=.50, shuffle=True, random_state=17)\n",
    "\n",
    "print('X,y Train:', len(x_train), len(y_train))\n",
    "print('X,y Val:', len(x_val), len(y_val))\n",
    "print('X,y Test', len(x_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pregunta 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pregunta 4 A**\n",
    "\n",
    "***Usa el conjunto de entrenamiento para generar tu vocabulario con un tamaño que\n",
    "consideres adecuado. Puedes filtrar tu vocabulario por la frecuencia mínima de uso de cada\n",
    "palabra, así como por su longitud mínima en caracteres.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Definir parámetros de filtrado\n",
    "frecuencia_minima = 2\n",
    "longitud_minima = 3\n",
    "\n",
    "# Contar las palabras en el conjunto de entrenamiento\n",
    "contador_palabras = Counter()\n",
    "for comentario in x_train:\n",
    "    contador_palabras.update(comentario)\n",
    "\n",
    "# Filtrar el vocabulario\n",
    "vocabulario = {palabra: frecuencia for palabra, frecuencia in contador_palabras.items()\n",
    "               if frecuencia >= frecuencia_minima} #and len(palabra) >= longitud_minima}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pregunta 4 B**\n",
    "\n",
    "**Indica el tamaño del vocabulario generado.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario generado: 1420\n"
     ]
    }
   ],
   "source": [
    "# Mostrar el tamaño del vocabulario generado\n",
    "print(f\"Tamaño del vocabulario generado: {len(vocabulario)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pregunta 4 C**\n",
    "\n",
    "**¿Por qué debe usarse solamente el conjunto de entrenamiento para generar el vocabulario?**\n",
    "\n",
    "Usar solo el conjunto de entrenamiento para generar el vocabulario es importante por las siguientes razones:\n",
    "\n",
    "Evita la fuga de datos (data leakage): La fuga de datos ocurre cuando la información del conjunto de validación o prueba se usa para crear el modelo, lo que puede dar lugar a un rendimiento inflado que no se reflejará en datos nuevos.\n",
    "Representación realista: El vocabulario debe reflejar lo que el modelo encontrará en datos no vistos. Usar solo el conjunto de entrenamiento asegura que el modelo no se adapte específicamente a las características de los conjuntos de validación y prueba.\n",
    "Evaluación justa: Para evaluar de manera justa el rendimiento del modelo, los conjuntos de validación y prueba deben mantenerse independientes de cualquier proceso de modelado que involucre el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pregunta 4 D**\n",
    "\n",
    "\n",
    "**Con el vocabulario generado, filtra los conjuntos de entrenamiento, validación y prueba para\n",
    "que todos los comentarios usen solamente las palabras de este vocabulario.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento filtrado: 2100\n",
      "Tamaño del conjunto de validación filtrado: 450\n",
      "Tamaño del conjunto de prueba filtrado: 450\n"
     ]
    }
   ],
   "source": [
    "def filtrar_comentarios(comentarios, vocabulario):\n",
    "    return [[palabra for palabra in comentario if palabra in vocabulario] for comentario in comentarios]\n",
    "\n",
    "# Filtrar los conjuntos de datos\n",
    "x_train_filtrado = filtrar_comentarios(x_train, vocabulario)\n",
    "x_val_filtrado = filtrar_comentarios(x_val, vocabulario)\n",
    "x_test_filtrado = filtrar_comentarios(x_test, vocabulario)\n",
    "\n",
    "# Verificar el tamaño de los conjuntos filtrados\n",
    "print(f\"Tamaño del conjunto de entrenamiento filtrado: {len(x_train_filtrado)}\")\n",
    "print(f\"Tamaño del conjunto de validación filtrado: {len(x_val_filtrado)}\")\n",
    "print(f\"Tamaño del conjunto de prueba filtrado: {len(x_test_filtrado)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_ySQygSEdcK"
   },
   "source": [
    "# **Pregunta 5**\n",
    "Utilizarás los vectores embebidos FastText preentrenados por Facebook.\n",
    "\n",
    "Incluye una tabla comparativa de pros y contras entre los modelos FastText, word2vec de Google y Glove de Stanford. Puedes consultar sus páginas correspondientes:\n",
    "- https://fasttext.cc/\n",
    "- https://code.google.com/archive/p/word2vec/\n",
    "- https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DD8-dBWo9T9j"
   },
   "source": [
    "| Características | FastText | Word2Vec | GloVe |\n",
    "| ----------- | ----------- | ----------- | ----------- |\n",
    "| **Institución desarrolladora** | Facebook AI | Google | Stanford University |\n",
    "| **Modelo** | Basado en la arquitectura de skip-gram y CBOW | Basado en las arquitecturas de skip-gram y CBOW | Basado en la matriz de co-ocurrencia de palabras en el corpus |\n",
    "| **Manejo de palabras fuera del vocabulario (OOV)** | Buen manejo gracias al uso de n-grams subwords | No lo soporta | No lo soporta |\n",
    "| **Manejo de palabras raras** | Buen manejo gracias al uso de n-grams subwords | Puede tener dificultades con palabras raras | Buen manejo dependiendo de la frecuencia en la matriz de co-ocurrencia |\n",
    "| **Velocidad de entrenamiento** | Media | Rápido | Lento en comparación con FastText y Word2Vec debido al uso de la matriz de co-ocurrencia |\n",
    "| **Dimensiones de los vectores** | Configurable, hasta 300 dimensiones | Configurable, hasta 300 dimensiones | Configurable, hasta 300 dimensiones |\n",
    "| **Requisitos de memoria** | Moderados, debido a los subwords n-grams | Bajos, sólo necesita palabras y contextos | Altos, debido a la matriz de co-ocurrencia |\n",
    "| **Interpretación semántica** | Buena, similar a Word2Vec | Buena, basada en contextos cercanos | Buena, basada en relaciones semánticas y sintácticas |\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Es importante mencionar que cada uno de estos modelos puede ser el más adecuado dependiendo de las necesidades específicas del problema que se esté abordando.\n",
    "\n",
    "Por ejemplo, si necesitas manejar palabras fuera del vocabulario o palabras raras, FastText podría ser la mejor opción; si la memoria es una limitación, Word2Vec podría ser más adecuado; GloVe, por otro lado, podría ser más útil para capturar relaciones semánticas y sintácticas entre palabras.\n",
    "\n",
    "Por otro lado, GloVe podría desempeñarse mejor que Word2Vec para tareas de analogía de palabras, y también muestra un desempeño superior en tareas de similitud y de reconocimiento de nombre de entidades. Mientras que FastText logra un mejor desempeño en tareas sintácticas. Por último, Word2Vec sobrepasa a FastText en tareas semánticas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Bf1J3AiEt-d"
   },
   "source": [
    "# **Pregunta 6**\n",
    "Utiliza el modelo **FastText** de vectores embebidos pre-entrenados de dimensión 300 para generar un nuevo diccionario clave-valor, donde la “clave” será cada token o palabra de tu vocabulario y el “valor” será su vector embebido de dimensión 300. Este diccionario deberá ser del mismo tamaño que el vocabulario previo que hayas construido previamente.\n",
    "https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "\n",
    "**NOTA**: Debido a la cantidad de recursos computacionales que demanda cargar los vectores FastText (son 2 millones de vectores), es recomendable que una vez que generes el nuevo vocabulario de vectores embebidos, guardes dicho diccionario en un archivo (pickle, npz o el que consideres más adecuado). Una vez realizado lo anterior, puedes borrar la variable de FastText para liberar memoria RAM. De esta manera, ya tienes tu vocabulario de vectores embebidos de acuerdo a los tokens que consideras más adecuados para tu problema y puedes usarlo rápidamente\n",
    "cuando lo necesites. En dado caso apóyense entre los miembros del equipo de tener dificultades para generar el vocabulario y por mientras puedes usar el archivo del vocabulario que alguno haya generado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9H3SjaXtFjbJ"
   },
   "source": [
    "**Se instala el módulo de Cython y FastText**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "W8ZJ0jF1EcUq",
    "outputId": "b4cbd8d5-442c-47f8-e892-81230480001b"
   },
   "outputs": [],
   "source": [
    "# !pip install fasttext\n",
    "# !pip install Cython --install-option=\"--no-cython-compile\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QeNah30Fz6T"
   },
   "source": [
    "**Se importa el módulo de fasttext**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "WcwHjBoFF6sl",
    "outputId": "b031ab24-e668-45b9-caf3-e050fa51f3c5"
   },
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "#Se descargan los el modelo de idioma inglés\n",
    "# fasttext.util.download_model('en', if_exists='ignore')  # English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "KWrcylNjJVw4",
    "outputId": "fcfa4952-a72f-4860-8c69-c3250c8949ba"
   },
   "outputs": [],
   "source": [
    "#Se utiliza el modelo FastText de vectores embebidos pre-entrenados de dimensión 300\n",
    "ft = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Cizy_S4-LDa0",
    "outputId": "6a43a925-62ad-4f33-c6cd-feb164dc738b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud del vocabulario de vectores embebidos: 1420\n",
      "Las dimensiones de midicc_vec son: 1420,300\n",
      "[('star', array([-2.86828041e-01,  1.15123533e-01,  5.38213179e-02,  6.26803637e-02,\n",
      "        2.47128960e-02, -2.57342309e-02,  1.47269234e-01, -1.16852485e-01,\n",
      "        1.43789779e-02,  2.02018499e-01, -6.00243136e-02, -6.75183982e-02,\n",
      "       -4.77372715e-03,  4.89413664e-02,  1.16688116e-02, -3.23957461e-03,\n",
      "       -1.23359617e-02, -8.51226449e-02, -4.92033921e-02,  6.35383427e-02,\n",
      "        8.86741839e-03,  1.40952796e-01,  1.09847665e-01,  1.25429276e-02,\n",
      "       -4.25020084e-02, -8.08417723e-02,  2.70867231e-03,  3.24510969e-02,\n",
      "        1.41084492e-02,  1.06206566e-01,  1.08138800e-01, -5.41383438e-02,\n",
      "        5.69636673e-02,  5.37327491e-02, -7.06092343e-02, -9.78406295e-02,\n",
      "       -1.20484188e-01, -4.16465327e-02, -8.21356028e-02,  4.71536033e-02,\n",
      "        9.90687776e-03,  4.89758067e-02, -1.33870468e-01, -1.54928997e-01,\n",
      "       -1.81018680e-01,  1.78726465e-02,  7.79893203e-03, -8.59202892e-02,\n",
      "       -2.29937509e-02, -1.81513757e-01, -1.04256801e-01,  1.19395129e-01,\n",
      "        7.99314678e-03,  5.44246733e-02,  1.18487053e-01,  4.88866717e-02,\n",
      "        5.15220016e-02, -4.63055186e-02,  8.03847145e-03, -5.37553839e-02,\n",
      "       -1.83139741e-02, -9.42408741e-02, -1.01035193e-01,  3.56963836e-02,\n",
      "        1.93327591e-02,  2.09810555e-01,  2.19372492e-02,  2.13999581e-02,\n",
      "        1.16238985e-02, -1.17042363e-01, -1.48543967e-02,  3.18749472e-02,\n",
      "       -1.81292221e-01, -8.17631930e-02,  2.79697850e-02, -3.79535183e-02,\n",
      "        1.45776663e-02,  1.37915611e-01, -8.08674842e-02,  2.51751058e-02,\n",
      "       -6.64169192e-02,  9.05220862e-03,  1.35277987e-01,  2.18077898e-02,\n",
      "        3.70039642e-02, -1.99556798e-02, -1.50145322e-01, -8.03397596e-03,\n",
      "       -7.00799674e-02, -2.94313934e-02, -3.02728191e-02,  1.93609685e-01,\n",
      "        1.89876668e-02, -7.70288929e-02,  1.80713087e-02,  1.10033236e-01,\n",
      "        7.12299794e-02, -4.59963679e-02, -1.72077119e-01,  3.16679366e-02,\n",
      "       -5.98362945e-02, -8.74337852e-02, -8.86409208e-02, -2.87565850e-02,\n",
      "       -5.81971705e-02,  1.01937577e-01,  3.15561742e-02,  1.15741333e-02,\n",
      "       -1.06359929e-01,  1.87610969e-01,  1.45370346e-02,  5.99830151e-02,\n",
      "        9.92235392e-02,  1.91061702e-02, -2.38206740e-02, -2.17346236e-01,\n",
      "        7.70934811e-03, -3.35183665e-02, -1.03727773e-01, -4.16280292e-02,\n",
      "        1.53034225e-01,  8.58682245e-02,  3.14186066e-02,  1.54228285e-01,\n",
      "       -8.03690311e-03,  3.32052186e-02,  4.11878666e-03,  1.05484031e-01,\n",
      "        7.57255778e-02, -3.18720341e-02,  1.70250125e-02,  6.56677261e-02,\n",
      "        1.23368874e-01, -3.03245634e-02,  2.33012419e-02, -1.44071028e-01,\n",
      "        1.45928591e-01,  6.17721789e-02,  1.98618054e-01, -1.88623518e-01,\n",
      "       -6.05288260e-02,  7.98067078e-02,  3.88783626e-02, -1.24917820e-01,\n",
      "       -1.37684941e-01,  6.50793165e-02, -2.99405992e-01, -3.73671472e-04,\n",
      "        4.60229479e-02, -7.02350959e-02, -3.00859064e-02, -1.02555119e-01,\n",
      "       -8.37509986e-03, -3.45986453e-04,  9.31164846e-02, -3.90904620e-02,\n",
      "        1.14622459e-01,  1.26173794e-01,  5.39926952e-03,  2.46864315e-02,\n",
      "       -7.46607184e-02,  6.94717746e-03,  1.56693712e-01, -1.52236253e-01,\n",
      "        3.30955088e-02, -4.14339192e-02, -6.96366578e-02,  9.96302366e-02,\n",
      "       -6.03845380e-02,  9.29320753e-02, -2.12934427e-03, -6.01781867e-02,\n",
      "       -7.10362718e-02, -1.18315414e-01, -6.35388717e-02,  6.41066134e-02,\n",
      "       -5.91891073e-02,  1.27110809e-01,  7.15065151e-02,  6.29523173e-02,\n",
      "       -5.46345413e-02,  2.89074313e-02, -1.59469962e-01, -7.92971104e-02,\n",
      "       -1.03111260e-01, -8.40404853e-02,  6.54209182e-02,  6.68521225e-02,\n",
      "       -2.92568877e-02,  2.20480785e-01,  9.37886834e-02, -9.02144909e-02,\n",
      "       -4.41652164e-02,  8.60180557e-02, -1.45871434e-02,  4.97262180e-02,\n",
      "       -1.41114324e-01,  8.82492661e-02,  9.48378909e-03, -9.70369801e-02,\n",
      "        1.59810316e-02,  1.26596987e-02,  3.80288512e-02,  3.90619189e-02,\n",
      "        9.13637877e-02, -8.60894620e-02,  1.01811588e-01,  6.69082999e-02,\n",
      "        1.05031997e-01,  1.48328632e-01,  6.92542717e-02, -1.06625669e-01,\n",
      "       -1.52740449e-01, -7.56940758e-03,  3.13936025e-02,  3.49867865e-02,\n",
      "       -2.09618788e-02, -1.42902760e-02,  6.80341348e-02,  6.19825125e-02,\n",
      "       -2.84822695e-02,  1.18321627e-01, -1.13135815e-01, -1.77281257e-02,\n",
      "       -5.60616702e-03,  8.14713836e-02, -1.53869148e-02,  8.16864222e-02,\n",
      "       -1.04721271e-01,  1.58673406e-01, -7.53961354e-02,  3.07477619e-02,\n",
      "        9.25838854e-03, -1.48327835e-02,  7.91117456e-03, -3.18586826e-04,\n",
      "       -8.03159773e-02,  6.46696135e-04, -7.53018484e-02, -1.21981822e-01,\n",
      "        4.21633124e-02, -6.28293380e-02, -9.21363831e-02, -5.31597510e-02,\n",
      "        8.62716362e-02, -6.90141022e-02,  2.69403346e-02,  1.61921620e-01,\n",
      "       -4.50303815e-02,  2.98184454e-02, -3.96693758e-05, -7.90106505e-02,\n",
      "       -3.03728543e-02,  1.16063416e-01,  2.34403973e-03,  9.55789983e-02,\n",
      "       -2.87741404e-02, -9.72413719e-02,  3.45763527e-02,  1.02634273e-01,\n",
      "       -2.12719902e-01,  4.99785393e-02, -5.83627820e-02,  1.52304163e-02,\n",
      "       -1.22847423e-01,  4.66357172e-02,  1.46242663e-01,  1.13413274e-01,\n",
      "        1.09998845e-01, -2.39889026e-02,  3.31177339e-02,  4.13419865e-02,\n",
      "       -1.06826507e-01,  4.97730151e-02,  6.61367327e-02, -7.30071217e-02,\n",
      "       -2.99715679e-02,  6.65043890e-02, -5.69159463e-02,  4.16496157e-04,\n",
      "        1.42523021e-01, -4.14542109e-02, -1.05582647e-01, -1.55546203e-01,\n",
      "        6.32830709e-02,  1.06834993e-01, -9.86435339e-02,  7.51124090e-03,\n",
      "       -8.39522183e-02, -1.09830283e-01, -2.86104158e-02,  1.05289333e-01,\n",
      "       -1.09692931e-01,  1.05054498e-01,  1.07300431e-02,  1.82751417e-02,\n",
      "       -4.68385331e-02,  5.67049533e-02, -5.08587807e-03, -1.43625721e-01],\n",
      "      dtype=float32))]\n"
     ]
    }
   ],
   "source": [
    "#Copia de midicc para realizar operaciones\n",
    "midicc_vec = {}\n",
    "\n",
    "#asignar el vector resultante a cada elemento del diccionario\n",
    "for word in vocabulario:\n",
    "    vec_word = ft.get_word_vector(word)\n",
    "    midicc_vec[word] = vec_word\n",
    "\n",
    "print('Longitud del vocabulario de vectores embebidos:', len(midicc_vec))\n",
    "print('Las dimensiones de midicc_vec son: {},{}'.format(len(midicc_vec), len(list(midicc_vec.items())[0][1])))   # veamos algunos elementos del diccionario.\n",
    "print(list(midicc_vec.items())[0:1])     # veamos algunos elementos del diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fasttext\n",
    "# import fasttext.util\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diccionario de vectores embebidos guardado en 'diccionario_vectores.pkl'\n"
     ]
    }
   ],
   "source": [
    "with open('diccionario_vectores.pkl', 'wb') as archivo:\n",
    "    pickle.dump(midicc_vec, archivo)\n",
    "\n",
    "print(\"Diccionario de vectores embebidos guardado en 'diccionario_vectores.pkl'\")\n",
    "\n",
    "# Liberar memoria\n",
    "del ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpRFOjOvwArc"
   },
   "source": [
    "# **Pregunta 7**\n",
    "Una manera de utilizar los vectores embebidos con modelos de aprendizaje automático en\n",
    "documentos de texto, es asignar a cada comentario filtrado el vector embebido de dimensión 300\n",
    "que resulta de promediar todos sus tokens. Así, en este ejercicio deberás generar los arreglos\n",
    "correspondientes para los conjuntos de entrenamiento, validación y prueba. Los llamaremos\n",
    "trainEmb, valEmb y testEmb, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de trainEmb: (2100, 300)\n",
      "Dimensiones de valEmb: (450, 300)\n",
      "Dimensiones de testEmb: (450, 300)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Paso 1: Cargar el diccionario de vectores embebidos\n",
    "with open('diccionario_vectores.pkl', 'rb') as archivo:\n",
    "    diccionario_vectores = pickle.load(archivo)\n",
    "\n",
    "# Paso 2: Definir la función para promediar los vectores embebidos de un comentario\n",
    "def promedio_vectores(comentario, diccionario_vectores, dimension=300):\n",
    "    vectores = [diccionario_vectores[palabra] for palabra in comentario if palabra in diccionario_vectores]\n",
    "    if not len(vectores):\n",
    "        return np.zeros(dimension)\n",
    "    promedio = np.mean(vectores, axis=0)\n",
    "    return promedio\n",
    "\n",
    "# Paso 3: Generar los conjuntos de vectores embebidos promediados\n",
    "trainEmb = np.array([promedio_vectores(comentario, diccionario_vectores) for comentario in x_train_filtrado])\n",
    "valEmb = np.array([promedio_vectores(comentario, diccionario_vectores) for comentario in x_val_filtrado])\n",
    "testEmb = np.array([promedio_vectores(comentario, diccionario_vectores) for comentario in x_test_filtrado])\n",
    "\n",
    "# Verificar las dimensiones de los conjuntos generados\n",
    "print(f\"Dimensiones de trainEmb: {trainEmb.shape}\")\n",
    "print(f\"Dimensiones de valEmb: {valEmb.shape}\")\n",
    "print(f\"Dimensiones de testEmb: {testEmb.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-unf1wrx3aA"
   },
   "source": [
    "# **Pregunta 8**\n",
    "Utiliza los modelos de regresión lineal y bosque aleatorio (random forest) y encuentra sus\n",
    "desempeños (accuracy). Compara los resultados con los de la semana anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "_ByrS1Kvytfg"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100\n",
      "2100\n"
     ]
    }
   ],
   "source": [
    "# Definir rango inicial de hiperparámetros\n",
    "max_depth = [1, 2, 3]\n",
    "max_features = ['sqrt', 'log2', None]\n",
    "n_estimators = [10, 20, 30]\n",
    "criterion = ['gini', 'entropy', 'log_loss']\n",
    "\n",
    "# Crear modelo inicial\n",
    "rf_model = RandomForestClassifier()\n",
    "print(len(trainEmb))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 81 candidates, totalling 486 fits\n",
      "Mejor valor de exactitud obtenido con la mejor combinación: 0.73\n",
      "Mejor combinación de valores encontrados de los hiperparámetros: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 'log2', 'n_estimators': 30}\n",
      "Tabla con los resultados de GridSearch para RandomForest:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.089988</td>\n",
       "      <td>0.007353</td>\n",
       "      <td>0.008018</td>\n",
       "      <td>0.002213</td>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>10</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 1, 'max_fea...</td>\n",
       "      <td>0.652857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017032</td>\n",
       "      <td>65</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.716429</td>\n",
       "      <td>0.705714</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.689286</td>\n",
       "      <td>0.684286</td>\n",
       "      <td>0.693452</td>\n",
       "      <td>0.013736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.157357</td>\n",
       "      <td>0.014299</td>\n",
       "      <td>0.008635</td>\n",
       "      <td>0.003068</td>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>20</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 1, 'max_fea...</td>\n",
       "      <td>0.688571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014625</td>\n",
       "      <td>56</td>\n",
       "      <td>0.740714</td>\n",
       "      <td>0.704286</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.729286</td>\n",
       "      <td>0.720714</td>\n",
       "      <td>0.716905</td>\n",
       "      <td>0.015060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.225143</td>\n",
       "      <td>0.019160</td>\n",
       "      <td>0.008339</td>\n",
       "      <td>0.002603</td>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>30</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 1, 'max_fea...</td>\n",
       "      <td>0.661429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012954</td>\n",
       "      <td>50</td>\n",
       "      <td>0.718571</td>\n",
       "      <td>0.731429</td>\n",
       "      <td>0.709286</td>\n",
       "      <td>0.755714</td>\n",
       "      <td>0.727857</td>\n",
       "      <td>0.748571</td>\n",
       "      <td>0.731905</td>\n",
       "      <td>0.016076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.055397</td>\n",
       "      <td>0.003660</td>\n",
       "      <td>0.004667</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>log2</td>\n",
       "      <td>10</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 1, 'max_fea...</td>\n",
       "      <td>0.634286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020024</td>\n",
       "      <td>72</td>\n",
       "      <td>0.683571</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.686429</td>\n",
       "      <td>0.685000</td>\n",
       "      <td>0.710714</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.686905</td>\n",
       "      <td>0.012035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.100736</td>\n",
       "      <td>0.002734</td>\n",
       "      <td>0.005946</td>\n",
       "      <td>0.002032</td>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>log2</td>\n",
       "      <td>20</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 1, 'max_fea...</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011530</td>\n",
       "      <td>53</td>\n",
       "      <td>0.705714</td>\n",
       "      <td>0.711429</td>\n",
       "      <td>0.705714</td>\n",
       "      <td>0.715714</td>\n",
       "      <td>0.727143</td>\n",
       "      <td>0.703571</td>\n",
       "      <td>0.711548</td>\n",
       "      <td>0.008086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.344932</td>\n",
       "      <td>0.009897</td>\n",
       "      <td>0.008128</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>3</td>\n",
       "      <td>log2</td>\n",
       "      <td>20</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 3, 'max...</td>\n",
       "      <td>0.695714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013318</td>\n",
       "      <td>22</td>\n",
       "      <td>0.807857</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.799286</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.796429</td>\n",
       "      <td>0.817143</td>\n",
       "      <td>0.800119</td>\n",
       "      <td>0.009736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.514064</td>\n",
       "      <td>0.007989</td>\n",
       "      <td>0.010528</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>3</td>\n",
       "      <td>log2</td>\n",
       "      <td>30</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 3, 'max...</td>\n",
       "      <td>0.701429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022045</td>\n",
       "      <td>18</td>\n",
       "      <td>0.820714</td>\n",
       "      <td>0.811429</td>\n",
       "      <td>0.824286</td>\n",
       "      <td>0.819286</td>\n",
       "      <td>0.822857</td>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.820119</td>\n",
       "      <td>0.004194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>4.727888</td>\n",
       "      <td>0.164468</td>\n",
       "      <td>0.005680</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 3, 'max...</td>\n",
       "      <td>0.715714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018009</td>\n",
       "      <td>32</td>\n",
       "      <td>0.800714</td>\n",
       "      <td>0.764286</td>\n",
       "      <td>0.813571</td>\n",
       "      <td>0.795714</td>\n",
       "      <td>0.795714</td>\n",
       "      <td>0.790714</td>\n",
       "      <td>0.793452</td>\n",
       "      <td>0.014866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>9.414805</td>\n",
       "      <td>0.300178</td>\n",
       "      <td>0.009635</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>20</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 3, 'max...</td>\n",
       "      <td>0.702857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015527</td>\n",
       "      <td>37</td>\n",
       "      <td>0.802143</td>\n",
       "      <td>0.763571</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.807143</td>\n",
       "      <td>0.798571</td>\n",
       "      <td>0.802857</td>\n",
       "      <td>0.796310</td>\n",
       "      <td>0.014854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>13.159917</td>\n",
       "      <td>0.200328</td>\n",
       "      <td>0.008924</td>\n",
       "      <td>0.003739</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>30</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 3, 'max...</td>\n",
       "      <td>0.722857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020860</td>\n",
       "      <td>26</td>\n",
       "      <td>0.817143</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>0.792143</td>\n",
       "      <td>0.798571</td>\n",
       "      <td>0.819286</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.803690</td>\n",
       "      <td>0.012734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.089988      0.007353         0.008018        0.002213   \n",
       "1        0.157357      0.014299         0.008635        0.003068   \n",
       "2        0.225143      0.019160         0.008339        0.002603   \n",
       "3        0.055397      0.003660         0.004667        0.001520   \n",
       "4        0.100736      0.002734         0.005946        0.002032   \n",
       "..            ...           ...              ...             ...   \n",
       "76       0.344932      0.009897         0.008128        0.001006   \n",
       "77       0.514064      0.007989         0.010528        0.001301   \n",
       "78       4.727888      0.164468         0.005680        0.000537   \n",
       "79       9.414805      0.300178         0.009635        0.001082   \n",
       "80      13.159917      0.200328         0.008924        0.003739   \n",
       "\n",
       "   param_criterion param_max_depth param_max_features param_n_estimators  \\\n",
       "0             gini               1               sqrt                 10   \n",
       "1             gini               1               sqrt                 20   \n",
       "2             gini               1               sqrt                 30   \n",
       "3             gini               1               log2                 10   \n",
       "4             gini               1               log2                 20   \n",
       "..             ...             ...                ...                ...   \n",
       "76        log_loss               3               log2                 20   \n",
       "77        log_loss               3               log2                 30   \n",
       "78        log_loss               3               None                 10   \n",
       "79        log_loss               3               None                 20   \n",
       "80        log_loss               3               None                 30   \n",
       "\n",
       "                                               params  split0_test_score  ...  \\\n",
       "0   {'criterion': 'gini', 'max_depth': 1, 'max_fea...           0.652857  ...   \n",
       "1   {'criterion': 'gini', 'max_depth': 1, 'max_fea...           0.688571  ...   \n",
       "2   {'criterion': 'gini', 'max_depth': 1, 'max_fea...           0.661429  ...   \n",
       "3   {'criterion': 'gini', 'max_depth': 1, 'max_fea...           0.634286  ...   \n",
       "4   {'criterion': 'gini', 'max_depth': 1, 'max_fea...           0.657143  ...   \n",
       "..                                                ...                ...  ...   \n",
       "76  {'criterion': 'log_loss', 'max_depth': 3, 'max...           0.695714  ...   \n",
       "77  {'criterion': 'log_loss', 'max_depth': 3, 'max...           0.701429  ...   \n",
       "78  {'criterion': 'log_loss', 'max_depth': 3, 'max...           0.715714  ...   \n",
       "79  {'criterion': 'log_loss', 'max_depth': 3, 'max...           0.702857  ...   \n",
       "80  {'criterion': 'log_loss', 'max_depth': 3, 'max...           0.722857  ...   \n",
       "\n",
       "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0         0.017032               65            0.675000            0.716429   \n",
       "1         0.014625               56            0.740714            0.704286   \n",
       "2         0.012954               50            0.718571            0.731429   \n",
       "3         0.020024               72            0.683571            0.685714   \n",
       "4         0.011530               53            0.705714            0.711429   \n",
       "..             ...              ...                 ...                 ...   \n",
       "76        0.013318               22            0.807857            0.790000   \n",
       "77        0.022045               18            0.820714            0.811429   \n",
       "78        0.018009               32            0.800714            0.764286   \n",
       "79        0.015527               37            0.802143            0.763571   \n",
       "80        0.020860               26            0.817143            0.785000   \n",
       "\n",
       "    split2_train_score  split3_train_score  split4_train_score  \\\n",
       "0             0.705714            0.690000            0.689286   \n",
       "1             0.696429            0.710000            0.729286   \n",
       "2             0.709286            0.755714            0.727857   \n",
       "3             0.686429            0.685000            0.710714   \n",
       "4             0.705714            0.715714            0.727143   \n",
       "..                 ...                 ...                 ...   \n",
       "76            0.799286            0.790000            0.796429   \n",
       "77            0.824286            0.819286            0.822857   \n",
       "78            0.813571            0.795714            0.795714   \n",
       "79            0.803571            0.807143            0.798571   \n",
       "80            0.792143            0.798571            0.819286   \n",
       "\n",
       "    split5_train_score  mean_train_score  std_train_score  \n",
       "0             0.684286          0.693452         0.013736  \n",
       "1             0.720714          0.716905         0.015060  \n",
       "2             0.748571          0.731905         0.016076  \n",
       "3             0.670000          0.686905         0.012035  \n",
       "4             0.703571          0.711548         0.008086  \n",
       "..                 ...               ...              ...  \n",
       "76            0.817143          0.800119         0.009736  \n",
       "77            0.822143          0.820119         0.004194  \n",
       "78            0.790714          0.793452         0.014866  \n",
       "79            0.802857          0.796310         0.014854  \n",
       "80            0.810000          0.803690         0.012734  \n",
       "\n",
       "[81 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "kfold= RepeatedStratifiedKFold(n_splits=3, n_repeats=2, random_state=7)\n",
    "\n",
    "rf_params= {'max_depth': max_depth,\n",
    "            'max_features': max_features,\n",
    "            'n_estimators': n_estimators,\n",
    "            'criterion': criterion}\n",
    "\n",
    "rf_grid= GridSearchCV(rf_model, rf_params, scoring='accuracy', cv=kfold, n_jobs=-1, verbose=3, return_train_score=True)\n",
    "rf_grid.fit(trainEmb, y_train)\n",
    "rf_best = rf_grid.best_params_\n",
    "rf_grid_df = pd.DataFrame(rf_grid.cv_results_)\n",
    "\n",
    "print(f'Mejor valor de exactitud obtenido con la mejor combinación: {round(rf_grid.best_score_,2)}')\n",
    "print(f'Mejor combinación de valores encontrados de los hiperparámetros: {rf_best}')\n",
    "print('Tabla con los resultados de GridSearch para RandomForest:')\n",
    "display(rf_grid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud en el conjunto de entrenamiento: 0.802\n",
      "Exactitud en el conjunto de validación: 0.738\n"
     ]
    }
   ],
   "source": [
    "final_rf_model = RandomForestClassifier(max_depth=rf_best['max_depth'],\n",
    "                                        max_features=rf_best['max_features'],\n",
    "                                        n_estimators=rf_best['n_estimators'],\n",
    "                                        criterion= rf_best['criterion'])\n",
    "final_rf_model.fit(trainEmb, y_train)\n",
    "train_accuracy = final_rf_model.score(trainEmb, y_train)\n",
    "val_accuracy = final_rf_model.score(valEmb, y_val)\n",
    "\n",
    "print(f\"Exactitud en el conjunto de entrenamiento: {train_accuracy:.3f}\")\n",
    "print(f\"Exactitud en el conjunto de validación: {val_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Definir rango inicial de hiperparámetros\n",
    "C = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "solver = ['lbfgs', 'liblinear', 'newton-cholesky', 'newton-cg', 'sag', 'saga']\n",
    "multi_class= ['auto', 'ovr']\n",
    "\n",
    "# Crear modelo inicial\n",
    "lr_model = LogisticRegression(penalty= 'l2', max_iter=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 72 candidates, totalling 432 fits\n",
      "Mejor valor de exactitud obtenido con la mejor combinación: 0.79\n",
      "Mejor combinación de valores encontrados de los hiperparámetros: {'C': 10, 'multi_class': 'auto', 'solver': 'liblinear'}\n",
      "Tabla con los resultados de GridSearch para LogisticRegression:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_multi_class</th>\n",
       "      <th>param_solver</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.064413</td>\n",
       "      <td>0.003108</td>\n",
       "      <td>0.006146</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>0.001</td>\n",
       "      <td>auto</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'C': 0.001, 'multi_class': 'auto', 'solver': ...</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.086673</td>\n",
       "      <td>0.006862</td>\n",
       "      <td>0.004168</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.001</td>\n",
       "      <td>auto</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>{'C': 0.001, 'multi_class': 'auto', 'solver': ...</td>\n",
       "      <td>0.511429</td>\n",
       "      <td>0.507143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>61</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.512857</td>\n",
       "      <td>0.512143</td>\n",
       "      <td>0.512857</td>\n",
       "      <td>0.510714</td>\n",
       "      <td>0.512143</td>\n",
       "      <td>0.511786</td>\n",
       "      <td>0.001071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.115485</td>\n",
       "      <td>0.014968</td>\n",
       "      <td>0.004355</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.001</td>\n",
       "      <td>auto</td>\n",
       "      <td>newton-cholesky</td>\n",
       "      <td>{'C': 0.001, 'multi_class': 'auto', 'solver': ...</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.060851</td>\n",
       "      <td>0.006567</td>\n",
       "      <td>0.004930</td>\n",
       "      <td>0.001648</td>\n",
       "      <td>0.001</td>\n",
       "      <td>auto</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'C': 0.001, 'multi_class': 'auto', 'solver': ...</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.185716</td>\n",
       "      <td>0.040771</td>\n",
       "      <td>0.003273</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.001</td>\n",
       "      <td>auto</td>\n",
       "      <td>sag</td>\n",
       "      <td>{'C': 0.001, 'multi_class': 'auto', 'solver': ...</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.352382</td>\n",
       "      <td>0.049910</td>\n",
       "      <td>0.002690</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>100</td>\n",
       "      <td>ovr</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>{'C': 100, 'multi_class': 'ovr', 'solver': 'li...</td>\n",
       "      <td>0.774286</td>\n",
       "      <td>0.802857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013342</td>\n",
       "      <td>21</td>\n",
       "      <td>0.889286</td>\n",
       "      <td>0.882143</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880714</td>\n",
       "      <td>0.891429</td>\n",
       "      <td>0.866429</td>\n",
       "      <td>0.881667</td>\n",
       "      <td>0.008057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.165317</td>\n",
       "      <td>0.015772</td>\n",
       "      <td>0.002447</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>100</td>\n",
       "      <td>ovr</td>\n",
       "      <td>newton-cholesky</td>\n",
       "      <td>{'C': 100, 'multi_class': 'ovr', 'solver': 'ne...</td>\n",
       "      <td>0.774286</td>\n",
       "      <td>0.802857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>19</td>\n",
       "      <td>0.889286</td>\n",
       "      <td>0.882143</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880714</td>\n",
       "      <td>0.891429</td>\n",
       "      <td>0.866429</td>\n",
       "      <td>0.881667</td>\n",
       "      <td>0.008057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.105732</td>\n",
       "      <td>0.026719</td>\n",
       "      <td>0.002172</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>100</td>\n",
       "      <td>ovr</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'C': 100, 'multi_class': 'ovr', 'solver': 'ne...</td>\n",
       "      <td>0.777143</td>\n",
       "      <td>0.802857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012472</td>\n",
       "      <td>15</td>\n",
       "      <td>0.889286</td>\n",
       "      <td>0.881429</td>\n",
       "      <td>0.881429</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.891429</td>\n",
       "      <td>0.867143</td>\n",
       "      <td>0.881786</td>\n",
       "      <td>0.007822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>9.612797</td>\n",
       "      <td>2.666463</td>\n",
       "      <td>0.004513</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>100</td>\n",
       "      <td>ovr</td>\n",
       "      <td>sag</td>\n",
       "      <td>{'C': 100, 'multi_class': 'ovr', 'solver': 'sag'}</td>\n",
       "      <td>0.772857</td>\n",
       "      <td>0.802857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012868</td>\n",
       "      <td>18</td>\n",
       "      <td>0.888571</td>\n",
       "      <td>0.881429</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.879286</td>\n",
       "      <td>0.890714</td>\n",
       "      <td>0.865714</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.008057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>15.984344</td>\n",
       "      <td>2.807200</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>100</td>\n",
       "      <td>ovr</td>\n",
       "      <td>saga</td>\n",
       "      <td>{'C': 100, 'multi_class': 'ovr', 'solver': 'sa...</td>\n",
       "      <td>0.777143</td>\n",
       "      <td>0.802857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011547</td>\n",
       "      <td>13</td>\n",
       "      <td>0.889286</td>\n",
       "      <td>0.881429</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.877143</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.880476</td>\n",
       "      <td>0.008367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0        0.064413      0.003108         0.006146        0.001655   0.001   \n",
       "1        0.086673      0.006862         0.004168        0.000231   0.001   \n",
       "2        0.115485      0.014968         0.004355        0.000576   0.001   \n",
       "3        0.060851      0.006567         0.004930        0.001648   0.001   \n",
       "4        0.185716      0.040771         0.003273        0.001206   0.001   \n",
       "..            ...           ...              ...             ...     ...   \n",
       "67       0.352382      0.049910         0.002690        0.000550     100   \n",
       "68       0.165317      0.015772         0.002447        0.000795     100   \n",
       "69       0.105732      0.026719         0.002172        0.000626     100   \n",
       "70       9.612797      2.666463         0.004513        0.001232     100   \n",
       "71      15.984344      2.807200         0.002809        0.000851     100   \n",
       "\n",
       "   param_multi_class     param_solver  \\\n",
       "0               auto            lbfgs   \n",
       "1               auto        liblinear   \n",
       "2               auto  newton-cholesky   \n",
       "3               auto        newton-cg   \n",
       "4               auto              sag   \n",
       "..               ...              ...   \n",
       "67               ovr        liblinear   \n",
       "68               ovr  newton-cholesky   \n",
       "69               ovr        newton-cg   \n",
       "70               ovr              sag   \n",
       "71               ovr             saga   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'C': 0.001, 'multi_class': 'auto', 'solver': ...           0.508571   \n",
       "1   {'C': 0.001, 'multi_class': 'auto', 'solver': ...           0.511429   \n",
       "2   {'C': 0.001, 'multi_class': 'auto', 'solver': ...           0.508571   \n",
       "3   {'C': 0.001, 'multi_class': 'auto', 'solver': ...           0.508571   \n",
       "4   {'C': 0.001, 'multi_class': 'auto', 'solver': ...           0.508571   \n",
       "..                                                ...                ...   \n",
       "67  {'C': 100, 'multi_class': 'ovr', 'solver': 'li...           0.774286   \n",
       "68  {'C': 100, 'multi_class': 'ovr', 'solver': 'ne...           0.774286   \n",
       "69  {'C': 100, 'multi_class': 'ovr', 'solver': 'ne...           0.777143   \n",
       "70  {'C': 100, 'multi_class': 'ovr', 'solver': 'sag'}           0.772857   \n",
       "71  {'C': 100, 'multi_class': 'ovr', 'solver': 'sa...           0.777143   \n",
       "\n",
       "    split1_test_score  ...  std_test_score  rank_test_score  \\\n",
       "0            0.508571  ...        0.000000               63   \n",
       "1            0.507143  ...        0.001978               61   \n",
       "2            0.508571  ...        0.000000               63   \n",
       "3            0.508571  ...        0.000000               63   \n",
       "4            0.508571  ...        0.000000               63   \n",
       "..                ...  ...             ...              ...   \n",
       "67           0.802857  ...        0.013342               21   \n",
       "68           0.802857  ...        0.013000               19   \n",
       "69           0.802857  ...        0.012472               15   \n",
       "70           0.802857  ...        0.012868               18   \n",
       "71           0.802857  ...        0.011547               13   \n",
       "\n",
       "    split0_train_score  split1_train_score  split2_train_score  \\\n",
       "0             0.508571            0.508571            0.508571   \n",
       "1             0.510000            0.512857            0.512143   \n",
       "2             0.508571            0.508571            0.508571   \n",
       "3             0.508571            0.508571            0.508571   \n",
       "4             0.508571            0.508571            0.508571   \n",
       "..                 ...                 ...                 ...   \n",
       "67            0.889286            0.882143            0.880000   \n",
       "68            0.889286            0.882143            0.880000   \n",
       "69            0.889286            0.881429            0.881429   \n",
       "70            0.888571            0.881429            0.880000   \n",
       "71            0.889286            0.881429            0.880000   \n",
       "\n",
       "    split3_train_score  split4_train_score  split5_train_score  \\\n",
       "0             0.508571            0.508571            0.508571   \n",
       "1             0.512857            0.510714            0.512143   \n",
       "2             0.508571            0.508571            0.508571   \n",
       "3             0.508571            0.508571            0.508571   \n",
       "4             0.508571            0.508571            0.508571   \n",
       "..                 ...                 ...                 ...   \n",
       "67            0.880714            0.891429            0.866429   \n",
       "68            0.880714            0.891429            0.866429   \n",
       "69            0.880000            0.891429            0.867143   \n",
       "70            0.879286            0.890714            0.865714   \n",
       "71            0.877143            0.890000            0.865000   \n",
       "\n",
       "    mean_train_score  std_train_score  \n",
       "0           0.508571         0.000000  \n",
       "1           0.511786         0.001071  \n",
       "2           0.508571         0.000000  \n",
       "3           0.508571         0.000000  \n",
       "4           0.508571         0.000000  \n",
       "..               ...              ...  \n",
       "67          0.881667         0.008057  \n",
       "68          0.881667         0.008057  \n",
       "69          0.881786         0.007822  \n",
       "70          0.880952         0.008057  \n",
       "71          0.880476         0.008367  \n",
       "\n",
       "[72 rows x 25 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_params= {'C': C, \n",
    "            'solver': solver,\n",
    "            'multi_class': multi_class}\n",
    "\n",
    "lr_grid= GridSearchCV(lr_model, lr_params, scoring='accuracy', cv=kfold, n_jobs=-1, verbose=3, return_train_score=True)\n",
    "lr_grid.fit(trainEmb, y_train)\n",
    "lr_best = lr_grid.best_params_\n",
    "lr_grid_df = pd.DataFrame(lr_grid.cv_results_)\n",
    "\n",
    "print(f'Mejor valor de exactitud obtenido con la mejor combinación: {round(lr_grid.best_score_,2)}')\n",
    "print(f'Mejor combinación de valores encontrados de los hiperparámetros: {lr_grid.best_params_}')\n",
    "print('Tabla con los resultados de GridSearch para LogisticRegression:')\n",
    "lr_grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud en el conjunto de entrenamiento: 0.845\n",
      "Exactitud en el conjunto de validación: 0.787\n"
     ]
    }
   ],
   "source": [
    "lr_best = lr_grid.best_params_\n",
    "\n",
    "final_lr_model = LogisticRegression(C=lr_best['C'], \n",
    "                                    solver=lr_best['solver'],\n",
    "                                    multi_class=lr_best['multi_class'],\n",
    "                                    penalty= 'l2', max_iter=5000)\n",
    "final_lr_model.fit(trainEmb, y_train)\n",
    "train_accuracy = final_lr_model.score(trainEmb, y_train)\n",
    "val_accuracy = final_lr_model.score(valEmb, y_val)\n",
    "\n",
    "print(f\"Exactitud en el conjunto de entrenamiento: {train_accuracy:.3f}\")\n",
    "print(f\"Exactitud en el conjunto de validación: {val_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***FALTA MODIFICAR ESTE COMENTARIO***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcHKK4bIb2j-"
   },
   "source": [
    "*Se obtienen resultados similares a los obtenidos en la semana anterior. El modelo de regresión logística logra exactitudes por encima del 72% y el modelo de Bosques Aleatorios logra resultados similares y conserva su tendencia a sobreentrenarse. Para evitar lo anterios, se probaron varios métodos:*\n",
    "\n",
    "1. *Implementar validación cruzada.*\n",
    "2. *Reducir el número de estimadores.*\n",
    "\n",
    "*El primero no ayudó a reducir la tendencia a sobreentrenar el modelo. El seguno al reducirse a un valor de entr 2 y 10, logró con efectividad reducir el sobreentrenamiento. Sin embargo, la exactitud del conjunto de validación, también resultaba afectada negativamente, por lo tanto, se conservó el modelo sobreentrenado, ya que a pesar de esto, también logra los mejores resultados en el conjunto de validación. Entrenar el modelo con más datos, parece ser la mejor opción para lograr reducir el sobreentrenamiento en el modelo de bosques aleatorios.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3jy454oblEv"
   },
   "source": [
    "# **Pregunta 9**\n",
    "Obtener la matriz de confusión e interpretar sus valores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Mejor modelo RF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-accuracy con el mejor modelo de Random Forest 70.67%\n",
      "\n",
      "Matriz de confusión con el mejor modelo de Random Forest:\n",
      "[[160  56]\n",
      " [ 76 158]]\n",
      "\n",
      "Matriz de confusión con el mejor modelo de Random Forest en proporciones:\n",
      "[[0.35555556 0.12444444]\n",
      " [0.16888889 0.35111111]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('Test-accuracy con el mejor modelo de Random Forest %.2f%%' % (100*final_rf_model.score(testEmb, y_test)))\n",
    "\n",
    "pred = final_rf_model.predict(testEmb)\n",
    "print('\\nMatriz de confusión con el mejor modelo de Random Forest:')\n",
    "print(confusion_matrix(y_test, pred, labels=[0,1]))\n",
    "\n",
    "print('\\nMatriz de confusión con el mejor modelo de Random Forest en proporciones:')\n",
    "print(confusion_matrix(y_test, pred, labels=[0,1]) / pred.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Mejor modelo LR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-accuracy con el mejor modelo de Logistic Regression 80.89%\n",
      "\n",
      "Matriz de confusión con el mejor modelo de Logistic Regression:\n",
      "[[160  56]\n",
      " [ 76 158]]\n",
      "\n",
      "Matriz de confusión con el mejor modelo de Logistic Regression en proporciones:\n",
      "[[0.35555556 0.12444444]\n",
      " [0.16888889 0.35111111]]\n"
     ]
    }
   ],
   "source": [
    "print('Test-accuracy con el mejor modelo de Logistic Regression %.2f%%' % (100*final_lr_model.score(testEmb, y_test)))\n",
    "\n",
    "pred = final_rf_model.predict(testEmb)\n",
    "print('\\nMatriz de confusión con el mejor modelo de Logistic Regression:')\n",
    "print(confusion_matrix(y_test, pred, labels=[0,1]))\n",
    "\n",
    "print('\\nMatriz de confusión con el mejor modelo de Logistic Regression en proporciones:')\n",
    "print(confusion_matrix(y_test, pred, labels=[0,1]) / pred.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***FALTA MODIFICAR ESTE COMENTARIO***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QglSgTQkelAN"
   },
   "source": [
    "*Con relación a los resultados de las matrices de confusión por medio de ambos métodos, el modelo de bosques aleatorios obtuvo valor menor de clasificaciones verdaderas positivas, pero mayor en verdaderas negativas, logrando una cantidad neta de clasificaciones correctas mayor que el método de regresión logística. Se logró reducir el número de clasificaciones falsas negativas y se incrementó proporcionalmente el número falsos positivos. Al igual que lo descrito anteriormente, el valor neto clasificaciones incorrectas es menor que por el método de conteo. Lo anterior en resumen, implica que el método de bosques aleatorios es mejor para los algoritmos de clasificación, para diferenciar mejor las clases negativas en general que el método de conteo a cambio de una ligera degradación en la clasificación de clases positivas.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4CmKx0z6NNb"
   },
   "source": [
    "# **Pregunta 10**\n",
    "Comenta con tus compañeros de equipo los pasos realizados en esta actividad e incluyan sus conclusiones finales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***FALTA MODIFICAR ESTE COMENTARIO***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGcUa5OhB_oP"
   },
   "source": [
    ">>> *Agregar aquí conclusiones del trabajo*\n",
    "\n",
    "*Dentro de la limpieza realizada se encontraron algunas validaciones que mejoraron el desempeño del modelo dado que no se procesaron ciertos valores o caracteres que no son relevantes para el estudio y alteran el resultado esperado. Posteriormente, se ejecutaron las particiones de los elementos para el desarrollo de los modelos; para ello, se acotaron los caracteres dentro del diccionario del conjunto de entrenamiento dado que se involucraron únicamente palabras relevantes las cuales tienen una mayor cantidad de frecuencia dentro de los datos.*\n",
    "\n",
    "*Luego del preprocesamiento anterior, se inició la validación del modelo **FastText** con el fin de identificar la relación que tienen las palabras que contiene el diccionario definido. Luego de inicializarlo, se desarrollaron las funciones requeridas para obtener el promedio de los vectores y de allí otra función que transforme los documentos a vectores embebidos con el fin de asignarles valores a los vectores e identificar la correlación de cada uno entre ellos.*\n",
    "\n",
    "*Con los procedimientos anteriores, se utilizaron dichos vectores para optimizar los modelos de Random Forest y Linear Regression, obteniendo su desempeño y la validación de estos. Detectando los hiperparámetros requeridos para cada modelo y obteniendo los resultados del desempeño de cada uno. **FastText** permite una mejor predicción dado que asigna una mejor relación entre las palabras que únicamente valores de 1 y 0 para los caracteres obtenidos, distorsionando el valor del desempeño de los modelos.*\n",
    "\n",
    "*Para este caso, el método de transformar el corpus a sus vectores preentrenados embebidos, después de el proceso de limpieza y lematización, para después calcular el promedio de los vectores para cada comentario, resultó en una desempeño menor que el que se obtuvo para el mismo conjunto de datos y el uso de la matriz dispersa por conteo de frecuencias y de tf-idf. Con lo anterior, se pudo comprender el valor de tener cada palabra representada como un elemento de un espacio vectorial para encontrar la relación entre las palabras cercanas, sin embargo, para este caso, el uso del promedio de dichos vectores no resultó superior en exactitud a los resultados logrados con las matrices dispersas. Posiblemente, los resultados se pueden mejorar si emplea una simplificación del comentario diferente al promedio de los vectores, como lo sería un proceso de reducción tomando en cuenta la relación que tiene una palabra con otra.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fK_RJaCNBEoj"
   },
   "source": [
    "##### **Fuentes bibliográficas y de datos:**\n",
    "- https://raw.githubusercontent.com/Handrum/NLP_EQ_6/main/amazon5.txt\n",
    "- https://raw.githubusercontent.com/Handrum/NLP_EQ_6/main/imdb5.txt\n",
    "- https://raw.githubusercontent.com/Handrum/NLP_EQ_6/main/yelp5.txt\n",
    "- https://fasttext.cc/\n",
    "- https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "- https://code.google.com/archive/p/word2vec/\n",
    "- https://nlp.stanford.edu/projects/glove/\n",
    "- https://medium.com/analytics-vidhya/word2vec-glove-fasttext-and-baseline-word-embeddings-step-by-step-d0489c15d10b\n",
    "\n",
    "- Vajjala, S., Majumder, B., Gupta, A., y Surana, H. (2020). Practical Natural Language Processing: A Comprehensive Guide to Building Real-World NLP Systems. O'Reilly. https://learning.oreilly.com/library/view/practical-natural-language/9781492054047/\n",
    "\n",
    "\n",
    "- Khurana, D., Koli, A., Khatter, K., y Singh, S. (2023). Natural language processing: state of the art, current trends and challenges. Multimed Tools Appl 82, 3713–3744. https://link.springer.com/article/10.1007/s11042-022-13428-4Links to an external site.\n",
    "\n",
    "- Falcón Morales, L. E. (2023). Bolsa de palabras: BOW [PDF]. Maestría en Inteligencia Artificial Aplicada. ITESM. Acceso al material Download Acceso al material"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
