{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tecnologico-de-Monterrey-MNA/nlp-2023_Equipo-6/blob/main/Equipo_06_Semana_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq_UEIE1t75y"
      },
      "source": [
        "#**Objetivo de la Actividad**\n",
        "\n",
        "\n",
        "> Trabajar con estos modelos pre-entrenados, generando el vocabulario a partir de tu conjunto de datos de entrenamiento.\n",
        "\n",
        "Para cada palabra de tu vocabulario, podrás sustituirlo por su correspondiente\n",
        "vector continuo. En caso de que no exista el vector para una palabra en particular, se puede eliminar dicha palabra, o bien sustituirla por el vector continuo más cercano.\n",
        "\n",
        "En esta actividad deberás aplicar esta segunda opción.\n",
        "\n",
        "-----\n",
        "\n",
        "*   Existen diversas propuestas para utilizar dichos vectores continuos como entrada para modelos de aprendizaje automático. En particular, en esta actividad cada enunciado será sustituido por el vector promedio de todos los tokens que lo forman.\n",
        "\n",
        "**Modelos:**\n",
        "\n",
        "Modelo de vectores **continuos/embebidos FastText**, es decir, el modelo desarrollado por Facebook en 2016."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D1AhgmjvJ9C"
      },
      "source": [
        "##**Pregunta 1**\n",
        "\n",
        "Descarga los **3 archivos de Canvas**. En particular, el archivo de datos de **IMDb** ya no requiere transformarse **para obtener sus 1000 registros**. Al cargar los datos de los tres archivos deberás tener un **DataFrame de Pandas de 3000 registros**, con sus etiquetas. Los archivos los encuentras en Canvas y se llaman: **amazon5.txt, imdb5.txt, yelp5.txt.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tZ_8l4dhwEWV",
        "outputId": "2759246b-9316-46dc-c8ee-dff6378bcd9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer, RegexpStemmer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
        "\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5JFdsxNv67X0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f871100e-ad5e-44d6-8054-0b3f14506b87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-18 12:36:16--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.155.173.40, 18.155.173.116, 18.155.173.80, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.155.173.40|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1325960915 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.en.300.vec.gz.1’\n",
            "\n",
            "cc.en.300.vec.gz.1  100%[===================>]   1.23G  52.8MB/s    in 22s     \n",
            "\n",
            "2024-05-18 12:36:38 (57.5 MB/s) - ‘cc.en.300.vec.gz.1’ saved [1325960915/1325960915]\n",
            "\n",
            "gzip: cc.en.300.vec already exists; do you wish to overwrite (y or n)? ^C\n"
          ]
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
        "!gunzip cc.en.300.vec.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uGjkDFP4vtr"
      },
      "source": [
        "#**Aplicando NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3nyPbMQ5v7Ly",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a004dd72-af59-45a6-d205-b809d96b7065"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "nltk.download('punkt')        # Tokenizador que ayuda a dividr el texto en enunciados\n",
        "nltk.download('stopwords')    # Acceso a \"stopwords\" en varios idiomas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uIq_dbSr0EmT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "729679a0-ee9e-4209-a7a8-970a7772fd57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ],
      "source": [
        "# Lista de stopwords que se incluyen de manera predeterminada la suite de librerías de NLTK\n",
        "\n",
        "print(len(stopwords.words('english')))\n",
        "print(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fzJpjSXPvdRC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ca1e4533-83ab-4174-a586-b30b97a25031"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-dd740832e87d>:8: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  dfi5 = pd.read_csv(url2, sep=' {3,4}', names=['review','label'], header=None, encoding='utf-8')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de registros de Amazon_5: (1000, 2)\n",
            "Total de registros de IMBD_5: (1000, 2)\n",
            "Total de registros de Yelp_5: (1000, 2)\n"
          ]
        }
      ],
      "source": [
        "#Se extraen los archivos desde repositorio público de GitHub\n",
        "url1 = 'https://raw.githubusercontent.com/Handrum/NLP_EQ_6/main/amazon5.txt'\n",
        "url2 = 'https://raw.githubusercontent.com/Handrum/NLP_EQ_6/main/imdb5.txt'\n",
        "url3 = 'https://raw.githubusercontent.com/Handrum/NLP_EQ_6/main/yelp5.txt'\n",
        "\n",
        "#Se cargan los archivos en dataframe de Pandas\n",
        "dfa5 = pd.read_csv(url1, sep='\\t', names=['review','label'], header=None, encoding='utf-8')\n",
        "dfi5 = pd.read_csv(url2, sep=' {3,4}', names=['review','label'], header=None, encoding='utf-8')\n",
        "dfy5 = pd.read_csv(url3, sep='\\t', names=['review','label'], header=None, encoding='utf-8')\n",
        "\n",
        "#Se imprime la forma de los dataframes\n",
        "print('Total de registros de Amazon_5:',dfa5.shape)\n",
        "print('Total de registros de IMBD_5:',dfi5.shape)\n",
        "print('Total de registros de Yelp_5:',dfy5.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dOAHRDNEz6sY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "957f62e9-18c8-4e76-ca40-925d4b4668fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  label\n",
              "0  A very, very, very slow-moving, aimless movie ...      0\n",
              "1  Not sure who was more lost - the flat characte...      0\n",
              "2  Attempting artiness with black & white and cle...      0\n",
              "3         Very little music or anything to speak of.      0\n",
              "4  The best scene in the movie was when Gerardo i...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-280e7fe0-29c3-4b52-9862-9f3256890b51\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not sure who was more lost - the flat characte...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Very little music or anything to speak of.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The best scene in the movie was when Gerardo i...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-280e7fe0-29c3-4b52-9862-9f3256890b51')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-280e7fe0-29c3-4b52-9862-9f3256890b51 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-280e7fe0-29c3-4b52-9862-9f3256890b51');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-022fcb09-4468-483e-93cb-a2ad2cc14877\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-022fcb09-4468-483e-93cb-a2ad2cc14877')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-022fcb09-4468-483e-93cb-a2ad2cc14877 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dfi5",
              "summary": "{\n  \"name\": \"dfi5\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 997,\n        \"samples\": [\n          \"I wasn't expecting Oscar material, but this?\",\n          \"It was clear that she had the range and ability to pull off this part.\",\n          \"Jamie Foxx absolutely IS Ray Charles.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "dfi5.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CQyhZt_z_rg0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "75815526-8b4b-4b36-afd3-71a61db89f8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3000 entries, 0 to 2999\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   review  3000 non-null   object\n",
            " 1   label   3000 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 47.0+ KB\n"
          ]
        }
      ],
      "source": [
        "#Concatenar los 3,000 registros\n",
        "\n",
        "df = pd.concat([dfa5, dfi5, dfy5], ignore_index=True)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A-W_SItAd_T"
      },
      "source": [
        "Al realizar la revisión del conjunto de datos se observa que el dataset de **imdb5** tiene 1.000 flotantes. Lo que requiere una limpieza especial para dicho dataset. --- **Sin embargo, se encontró que al cargar con el separador correcto, el siguiente paso no es necesario**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kryovbUFAF2F"
      },
      "outputs": [],
      "source": [
        "#dfi5.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1bTBFzOBjes"
      },
      "outputs": [],
      "source": [
        "#no necesario\n",
        "#print(dfi5_c.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s3xzHiQB0HU"
      },
      "outputs": [],
      "source": [
        "#no necesario con la limpieza anterior\n",
        "#print(df['label'].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txUZL511EdAO"
      },
      "outputs": [],
      "source": [
        "#no necesario con la limpieza anterior\n",
        "#dfii5_l = dfi5.fillna(0)\n",
        "#dfii5_l.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ci9f44O5GIA1"
      },
      "outputs": [],
      "source": [
        "#No necesario con la limpieza anterior\n",
        "#df = pd.concat([dfa5, dfii5_l, dfy5], ignore_index=True)\n",
        "#df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "N6dqTvNUIGqG"
      },
      "outputs": [],
      "source": [
        "X = df.review     # Serie de strings\n",
        "Y = df.label      # Serie de enteros 0s y 1s\n",
        "\n",
        "assert X.shape == (3000,)           # verificando que tenemos la dimensiones esperadas.\n",
        "assert Y.shape == (3000,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl9HFj-u4mnk"
      },
      "source": [
        "#**Pregunta 2**\n",
        "\n",
        "Realiza de nuevo un proceso de limpieza. Aplica el preprocesamiento que consideres adecuado, sin embargo, deberás aplicar necesariamente alguna de las técnicas de lematización. Como aplicaremos modelos embebidos pre-entrenados, queremos palabras lo más cercanas a las existentes en un idioma, inglés en este caso. Aplica y justifica cualquier otro proceso de limpieza que consideres\n",
        "adecuado. Recuerda que en esta actividad se usarán vectores embebidos para un problema de clasificación, por lo que deberás tomar de acuerdo a este contexto. Justifica todas las transformaciones que se apliquen.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyZ7I3P12JbD"
      },
      "source": [
        "**Procedimiento para quitar las negaciones del conjunto de stopwords en caso de ser necesario**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OYEURcKY1t-I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4c57f3d2-5dda-4d37-b955-3cd3e925264c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40\n",
            "['no', 'nor', 'not', 'ain', 'aren', \"aren't\", 'don', \"don't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "139\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ma']\n"
          ]
        }
      ],
      "source": [
        "# Consideremos la siguiente lista de palabras asociada a negaciones en inglés:\n",
        "\n",
        "mystopwords = stopwords.words('english')\n",
        "\n",
        "negwords = [ 'no', 'nor', 'not', 'ain', 'aren', \"aren't\", 'don', \"don't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
        "\n",
        "#Se asigna las stopwords de NLTK a mystopwords\n",
        "mystopwords_without_neg = stopwords.words('english')\n",
        "\n",
        "#Se revisa si cada una de las palabras en negwords está presente en mystopwords\n",
        "for word in negwords:\n",
        "  if word in mystopwords_without_neg:\n",
        "    mystopwords_without_neg.remove(word)           # si la palabra está presente, quitarla de la lista\n",
        "\n",
        "#Se imprime la longitud y elementos de negwords para verificar resultados\n",
        "print(len(negwords))\n",
        "print(negwords)\n",
        "\n",
        "#Se imprime la longitud y elementos de los stop words de NLTK para verificar resultados\n",
        "print(len(mystopwords))\n",
        "print(mystopwords)\n",
        "\n",
        "print(len(mystopwords_without_neg))\n",
        "print(mystopwords_without_neg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDyc80seLI_W"
      },
      "source": [
        "**NOTA**: Es importante tener en cuenta, que en el análisis de sentimientos debemos abarcar el vocabulario suficiente que nos permita entender la opinión del usuario y así mismo identificar aquellas opiniones que puden enriquecer y construir una interacción más amena con el cliente. Para ello, Se considera realizar una depuración de las negaciones que no agregan valor al resultado del diccionario que se requiere y que no definen a profundidad el sentimiento del cliente, estas se pueden ser reemplazadas por un vector continuo más cercano."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4XLAEojC5Gbr"
      },
      "outputs": [],
      "source": [
        "def clean_tok(doc):\n",
        "  ##############################################################################\n",
        "  # AGREGA AQUÍ TUS LÍNEAS DE CÓDIGO - Pregunta 4:\n",
        "\n",
        "  # Eliminación de signos de puntuación, caracteres especiales.\n",
        "\n",
        "  doc = doc.lower()    #normalización a minúsculas\n",
        "\n",
        "  #Sólo caracteres alfabéticos\n",
        "  puntuacion = re.sub(r'[^a-z]', ' ', doc)                  #considerar solo caracteres alfabéticos\n",
        "  puntuacion = re.sub(r'\\d{2, }', ' ', puntuacion.strip())  #eliminar todo tipo de espacios que se encuentren\n",
        "\n",
        "# Tokenizar\n",
        "  tokenizar = puntuacion.split()                            #tomar el resultado anterior y aplicar método de tokenización a partir del método split\n",
        "\n",
        "# Eliminación de Stopwords\n",
        "\n",
        "  tokens = []\n",
        "\n",
        "  for i in tokenizar:\n",
        "    if i not in mystopwords:                                #en esta ocasión para elimianr los stopwords se toma la librería de corpus el mismo método.\n",
        "      tokens.append(i)\n",
        "\n",
        "  for nwtokens in tokens:\n",
        "    if len(nwtokens)<=1:\n",
        "      tokens.remove(nwtokens)\n",
        "\n",
        "  # FIN PARA AGREGAR TUS LÍNEAS DE CÓDIGO.\n",
        "  ##############################################################################\n",
        "\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "h3cUq1k-LBXt"
      },
      "outputs": [],
      "source": [
        "Xcleantok = [clean_tok(x) for x in X]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QK2ETk5nMCOG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "e3a3c5e6-cd15-4885-ef77-06ea306c6963"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  label\n",
              "0  So there is no way for me to plug it in here i...      0\n",
              "1                        Good case, Excellent value.      1\n",
              "2                             Great for the jawbone.      1\n",
              "3  Tied to charger for conversations lasting more...      0\n",
              "4                                  The mic is great.      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bf4c4de3-f311-4020-bdb9-9d3c3c0f13ce\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>So there is no way for me to plug it in here i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Good case, Excellent value.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Great for the jawbone.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tied to charger for conversations lasting more...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The mic is great.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bf4c4de3-f311-4020-bdb9-9d3c3c0f13ce')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bf4c4de3-f311-4020-bdb9-9d3c3c0f13ce button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bf4c4de3-f311-4020-bdb9-9d3c3c0f13ce');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-865d885b-a7ad-4269-b59a-085af3100e10\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-865d885b-a7ad-4269-b59a-085af3100e10')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-865d885b-a7ad-4269-b59a-085af3100e10 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2982,\n        \"samples\": [\n          \"We've tried to like this place but after 10+ times I think we're done with them.\",\n          \"The best example of how dumb the writing is when it's established that you can turn the zombie-students back into humans by removing a necklace containing a piece of the meteorite.\",\n          \"It was that loud.Glad to say that the Plantronics 510 maintains a flawless connection to my cell and with no static during normal use.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lXM_3HnKLIxl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a41c7249-d159-4aed-d91c-c1f836c0b523"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['way', 'plug', 'us', 'unless', 'go', 'converter']\n",
            "['good', 'case', 'excellent', 'value']\n",
            "['great', 'jawbone']\n",
            "['tied', 'charger', 'conversations', 'lasting', 'minutes', 'major', 'problems']\n",
            "['mic', 'great']\n"
          ]
        }
      ],
      "source": [
        "for x in Xcleantok[0:5]:\n",
        "  print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDs04xbyRyh2"
      },
      "source": [
        "#**Método Limpieza por Lematizazión**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tSMPI-XoSELt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8505f8c5-7285-4b2f-cc83-b7158c2f4e3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aailiyah', 'abandoned', 'abhor', 'ability', 'able', 'abound', 'abroad', 'absolute', 'absolutel', 'absolutely', 'absolutley', 'abstruse', 'abysmal', 'ac', 'academy', 'accents', 'accept', 'acceptable', 'access', 'accessable', 'accessible', 'accessing', 'accessory', 'accessoryone', 'accident', 'accidentally', 'acclaimed', 'accolades', 'accommodations', 'accomodate', 'accompanied', 'according', 'accordingly', 'accountant', 'accurate', 'accurately', 'accused', 'ache', 'achievement', 'achille', 'ackerman', 'acknowledged', 'across', 'act', 'acted', 'acting', 'action', 'actions', 'activate', 'activated', 'activesync', 'actor', 'actors', 'actress', 'actresses', 'actual', 'actually', 'ad', 'adams', 'adaptation', 'adapter', 'adapters', 'add', 'added', 'addition', 'additional', 'address', 'adhesive', 'admins', 'admiration', 'admitted', 'adorable', 'adrift', 'adventure', 'advertised', 'advise', 'aerial', 'aesthetically', 'affected', 'affleck', 'affordable', 'afraid', 'africa', 'afternoon', 'age', 'aged', 'ages', 'aggravating', 'ago', 'agree', 'agreed', 'ahead', 'aimless', 'air', 'aired', 'airline', 'airport', 'akasha', 'akin', 'ala', 'alarm', 'albondigas', 'alert', 'alexander', 'alike', 'allergy', 'allison', 'allot', 'allow', 'allowing', 'allows', 'almonds', 'almost', 'alone', 'along', 'alongside', 'alot', 'already', 'also', 'although', 'aluminum', 'always', 'amateurish', 'amaze', 'amazed', 'amazing', 'amazingly', 'amazon', 'ambiance', 'ambience', 'america', 'american', 'americans', 'among', 'amount', 'amp', 'ample', 'amusing', 'anatomist', 'andddd', 'angel', 'angela', 'angeles', 'angelina', 'angle', 'angles', 'angry', 'anguish', 'angus', 'animals', 'animated', 'animation', 'anita', 'ann', 'anne', 'anniversary', 'annoying', 'another', 'answer', 'ant', 'antena', 'anthony', 'anti', 'anticipated', 'antithesis', 'anymore', 'anyone', 'anything', 'anytime', 'anyway', 'anyways', 'anywhere', 'apart', 'apartment', 'apologize', 'apology', 'app', 'appalling', 'apparently', 'appealing', 'appearance', 'appears', 'appetite', 'appetizer', 'appetizers', 'applauded', 'applause', 'apple', 'applifies', 'appointments', 'appreciate', 'appropriate', 'approval', 'apt', 'area', 'arepas', 'argued', 'arguing', 'aria', 'armageddon', 'armand', 'armband', 'around', 'array', 'arrival', 'arrived', 'arrives', 'arriving', 'art', 'article', 'articulated', 'artiness', 'artist', 'artistic', 'artless', 'arts', 'asia', 'aside', 'ask', 'asked', 'asking', 'asleep', 'aspect', 'aspects', 'ass', 'assante', 'assaulted', 'assistant', 'assumed', 'assure', 'astonishingly', 'astronaut', 'ate', 'atleast', 'atmosphere', 'atrocious', 'atrocity', 'att', 'attached', 'attack', 'attacked', 'attempt', 'attempted', 'attempting', 'attempts', 'attention', 'attentive', 'attitudes', 'attractive', 'audience', 'audio', 'auju', 'aurv', 'austen', 'austere', 'authentic', 'author', 'auto', 'available', 'average', 'aversion', 'avocado', 'avoid', 'avoided', 'avoiding', 'award', 'awarded', 'awards', 'away', 'awesome', 'awful', 'awkward', 'awkwardly', 'awsome', 'ayce', 'aye', 'az', 'baaaaaad', 'baba', 'babbling', 'babie', 'baby', 'babysitting', 'bachi', 'back', 'backdrop', 'backed', 'background', 'backlight', 'bacon', 'bad', 'badly', 'bag', 'bagels', 'bailey', 'bakery', 'baklava', 'balance', 'balanced', 'ball', 'ballet', 'balls', 'bamboo', 'banana', 'band', 'bank', 'bar', 'barcelona', 'bare', 'barely', 'bargain', 'barking', 'barney', 'barren', 'bars', 'bartender', 'bartenders', 'baseball', 'based', 'basement', 'basic', 'basically', 'bat', 'batch', 'bates', 'bathroom', 'bathrooms', 'batter', 'batteries', 'battery', 'baxendale', 'bay', 'bbq', 'bean', 'beans', 'bear', 'beat', 'beateous', 'beats', 'beautiful', 'beautifully', 'beauty', 'bec', 'became', 'bechard', 'become', 'becomes', 'bed', 'beef', 'beep', 'beeping', 'beer', 'beers', 'began', 'begin', 'beginning', 'behind', 'behing', 'behold', 'bela', 'believable', 'believe', 'believed', 'bell', 'bellagio', 'bellies', 'bells', 'bellucci', 'belly', 'belmondo', 'belt', 'ben', 'bend', 'bendingly', 'bennett', 'bergen', 'bertolucci', 'besides', 'best', 'better', 'betty', 'beware', 'beyond', 'bible', 'big', 'bigger', 'biggest', 'bill', 'bills', 'billy', 'binge', 'biographical', 'bipolarity', 'bird', 'biscuit', 'biscuits', 'bisque', 'bit', 'bitches', 'bitchy', 'bite', 'bites', 'bitpim', 'bits', 'black', 'blackberry', 'blacktop', 'blah', 'blake', 'blame', 'bland', 'blandest', 'blandly', 'blanket', 'blare', 'blatant', 'blew', 'block', 'bloddy', 'blood', 'bloodiest', 'bloody', 'blow', 'blown', 'blows', 'blue', 'blueant', 'bluetoooth', 'bluetooth', 'bluetooths', 'blush', 'bmw', 'boasts', 'bob', 'boba', 'bodes', 'body', 'bohemian', 'boiled', 'boiling', 'bold', 'bombardments', 'bond', 'bonding', 'bone', 'bonus', 'bonuses', 'boobs', 'boogeyman', 'book', 'booking', 'boost', 'boot', 'bop', 'bordered', 'borderlines', 'borders', 'bore', 'bored', 'boring', 'borrowed', 'bose', 'boss', 'bother', 'bothersome', 'bottom', 'bottowm', 'bouchon', 'bought', 'bougth', 'bowl', 'box', 'boxes', 'boy', 'boyfriend', 'boyle', 'boys', 'brain', 'brainsucking', 'brand', 'brat', 'bread', 'break', 'breakage', 'breakfast', 'breaking', 'breaks', 'breeders', 'breeze', 'brevity', 'brian', 'brick', 'brief', 'brigand', 'bright', 'brilliance', 'brilliant', 'brilliantly', 'bring', 'brings', 'brisket', 'broad', 'broke', 'broken', 'brooding', 'brother', 'brought', 'brownish', 'browser', 'browsing', 'brunch', 'bruschetta', 'brushfire', 'brutal', 'bt', 'bubbling', 'bucks', 'buddy', 'budget', 'buds', 'buffalo', 'buffet', 'buffets', 'bug', 'build', 'builders', 'building', 'buildings', 'built', 'buldogis', 'bulky', 'bullock', 'bully', 'bumpers', 'bunch', 'burger', 'burgers', 'burned', 'burrittos', 'burton', 'bus', 'business', 'businesses', 'bussell', 'busy', 'butter', 'button', 'buttons', 'buy', 'buyer', 'buyers', 'buying', 'buyit', 'buzzing', 'bye', 'c', 'ca', 'caballero', 'cable', 'cables', 'caesar', 'caf', 'cafe', 'cailles', 'cake', 'cakes', 'calamari', 'calendar', 'california', 'call', 'called', 'calligraphy', 'callings', 'calls', 'came', 'camelback', 'cameo', 'camera', 'camerawork', 'camp', 'campy', 'canada', 'canal', 'cancan', 'cancellation', 'cancelling', 'candace', 'candle', 'cannoli', 'cannot', 'cant', 'capability', 'capacity', 'cape', 'capers', 'captain', 'captured', 'captures', 'car', 'carbs', 'card', 'cardboard', 'cardellini', 'care', 'careful', 'caring', 'carly', 'carol', 'carpaccio', 'carrell', 'carried', 'carriers', 'carries', 'carry', 'cars', 'cart', 'cartel', 'cartoon', 'cartoons', 'case', 'cases', 'cash', 'cashew', 'cashier', 'casing', 'casino', 'cassette', 'cast', 'casted', 'casting', 'cat', 'catching', 'catchy', 'caterpillar', 'caught', 'cause', 'caused', 'causing', 'cavier', 'cbr', 'cds', 'ceases', 'celebration', 'celebrity', 'cell', 'cellphone', 'cellphones', 'cellular', 'celluloid', 'cent', 'center', 'centers', 'central', 'century', 'certain', 'certainly', 'cg', 'cgi', 'chai', 'chains', 'chalkboard', 'challenges', 'chance', 'change', 'changes', 'changing', 'channel', 'char', 'character', 'characterisation', 'characters', 'charcoal', 'charge', 'charged', 'charger', 'chargers', 'charges', 'charging', 'charisma', 'charismatic', 'charles', 'charlie', 'charm', 'charming', 'chase', 'chasing', 'cheap', 'cheaper', 'cheaply', 'cheapy', 'cheated', 'check', 'checked', 'checking', 'cheek', 'cheekbones', 'cheerfull', 'cheerless', 'cheese', 'cheeseburger', 'cheesecurds', 'cheesiness', 'cheesy', 'chef', 'chefs', 'chemistry', 'chewy', 'chick', 'chicken', 'chickens', 'child', 'childhood', 'children', 'childrens', 'chills', 'chilly', 'chimp', 'china', 'chinese', 'chip', 'chipolte', 'chipotle', 'chips', 'chocolate', 'chodorov', 'choice', 'choices', 'choked', 'choose', 'chosen', 'choux', 'chow', 'christmas', 'christopher', 'church', 'cibo', 'cinema', 'cinematic', 'cinematographers', 'cinematography', 'cingulair', 'cingular', 'circumstances', 'claimed', 'clarity', 'class', 'classic', 'classical', 'classics', 'classy', 'clean', 'clear', 'clearer', 'clearly', 'clever', 'clich', 'cliche', 'clicks', 'clients', 'cliff', 'climax', 'climbing', 'clip', 'clipping', 'clips', 'clock', 'close', 'closed', 'clothes', 'club', 'clue', 'co', 'coach', 'coal', 'coastal', 'coaster', 'cocktail', 'cocktails', 'coconut', 'cod', 'coffee', 'coherent', 'cold', 'colder', 'cole', 'colleague', 'collect', 'collective', 'college', 'color', 'colored', 'colorful', 'colors', 'colours', 'columbo', 'combination', 'combo', 'combos', 'come', 'comedic', 'comedy', 'comes', 'comfort', 'comfortable', 'comfortably', 'comfortible', 'comforting', 'comical', 'coming', 'commands', 'comment', 'commentary', 'commented', 'comments', 'commercial', 'commercials', 'common', 'communicate', 'communication', 'communications', 'community', 'commuter', 'companions', 'company', 'comparably', 'compared', 'compelling', 'compete', 'competent', 'competitors', 'complain', 'complained', 'complaint', 'complaints', 'complete', 'completed', 'completely', 'complex', 'complexity', 'compliments', 'composed', 'composition', 'comprehensible', 'compromise', 'computer', 'con', 'concentrate', 'concept', 'conception', 'conceptually', 'concern', 'concerning', 'concerns', 'concert', 'conclusion', 'concrete', 'condescends', 'condiment', 'conditions', 'confidence', 'configuration', 'confirm', 'conflict', 'confortable', 'confuses', 'confusing', 'connect', 'connected', 'connecting', 'connection', 'connections', 'connery', 'connisseur', 'connoisseur', 'connor', 'conrad', 'consequences', 'consider', 'considerable', 'considered', 'considering', 'considers', 'consistent', 'consolations', 'constant', 'constantine', 'constantly', 'constructed', 'construction', 'consumer', 'contact', 'contacted', 'contacting', 'contacts', 'contain', 'contained', 'containers', 'containing', 'contains', 'content', 'continually', 'continuation', 'continue', 'continues', 'continuity', 'continuously', 'contract', 'contrast', 'contributing', 'contributory', 'contrived', 'control', 'controls', 'controversy', 'contstruct', 'convenient', 'convention', 'conversation', 'conversations', 'converter', 'convey', 'convince', 'convincing', 'convoluted', 'cook', 'cooked', 'cooking', 'cool', 'copier', 'coppola', 'copy', 'corded', 'cords', 'core', 'corn', 'corny', 'corporation', 'correct', 'correction', 'correctly', 'cost', 'costco', 'costs', 'costumes', 'cotta', 'cotton', 'could', 'couldnt', 'count', 'counter', 'counterfeit', 'couple', 'couples', 'coupon', 'coupons', 'course', 'court', 'courteous', 'courtroom', 'cover', 'coverage', 'covered', 'covers', 'cow', 'cowardice', 'cox', 'coziness', 'cr', 'crab', 'crack', 'cracked', 'crackles', 'cradle', 'cradles', 'crafted', 'cramming', 'cranberry', 'crap', 'crappy', 'crash', 'crashed', 'craving', 'crawfish', 'crawl', 'crayon', 'crayons', 'crazy', 'creaks', 'cream', 'creamy', 'create', 'created', 'creates', 'creative', 'creativity', 'creature', 'credible', 'credit', 'credits', 'crema', 'crepe', 'crew', 'crime', 'crisp', 'crispy', 'critic', 'critical', 'crocdodile', 'crocs', 'cross', 'crostini', 'croutons', 'crowd', 'crowds', 'crowe', 'cruel', 'cruise', 'crumby', 'crust', 'crusty', 'cry', 'crystals', 'cuisine', 'cult', 'culture', 'cumbersome', 'current', 'currently', 'curry', 'curtain', 'curve', 'custer', 'customer', 'customers', 'customize', 'cut', 'cute', 'cutest', 'cutie', 'cutouts', 'cuts', 'cutting', 'dads', 'daily', 'damage', 'damian', 'damn', 'dance', 'dancing', 'dangerous', 'dark', 'darn', 'darren', 'data', 'date', 'dates', 'daughter', 'daughters', 'day', 'days', 'de', 'dead', 'deadly', 'deadpan', 'deaf', 'deal', 'dealing', 'dealt', 'death', 'debated', 'debbie', 'debits', 'debut', 'decade', 'decay', 'decent', 'decide', 'decided', 'decidely', 'decipher', 'decision', 'decisions', 'decor', 'decorated', 'dedicated', 'dedication', 'dee', 'deep', 'deeply', 'def', 'defeats', 'defect', 'defective', 'defensemen', 'deffinitely', 'definately', 'defined', 'definitely', 'definitly', 'degree', 'del', 'delay', 'delete', 'delicate', 'delicioso', 'delicious', 'deliciously', 'delight', 'delightful', 'delights', 'delish', 'deliver', 'delivered', 'delivering', 'delivers', 'delivery', 'denny', 'dependant', 'depending', 'depends', 'depicted', 'depicts', 'depressing', 'depth', 'derivative', 'describe', 'described', 'describes', 'describing', 'description', 'descriptions', 'desert', 'deserved', 'deserves', 'deserving', 'design', 'designed', 'designer', 'designs', 'desired', 'desperately', 'desperation', 'despicable', 'despised', 'despite', 'dessert', 'desserts', 'destination', 'destroy', 'destroying', 'detachable', 'detailed', 'detailing', 'details', 'deuchebaggery', 'develop', 'development', 'developments', 'device', 'devices', 'devine', 'di', 'diabetic', 'dialing', 'dialog', 'dialogs', 'dialogue', 'diaper', 'dickens', 'die', 'died', 'dieing', 'difference', 'different', 'difficult', 'dignity', 'dime', 'dimensional', 'dine', 'dining', 'dinner', 'dinners', 'dipping', 'direct', 'directed', 'directing', 'direction', 'directions', 'directly', 'director', 'directorial', 'directors', 'dirt', 'dirty', 'disagree', 'disapoinment', 'disapointing', 'disappoint', 'disappointed', 'disappointing', 'disappointment', 'disapppointment', 'disaster', 'disbelief', 'discarded', 'discomfort', 'disconnected', 'discount', 'discovering', 'discovery', 'disgrace', 'disgraceful', 'disgust', 'disgusted', 'disgusting', 'dish', 'dishes', 'dislike', 'disliked', 'disney', 'disparate', 'dispenser', 'display', 'displeased', 'disposable', 'disrespected', 'dissapointed', 'dissapointing', 'distant', 'distinction', 'distorted', 'distract', 'distracting', 'distressed', 'disturbing', 'dit', 'diverse', 'diving', 'division', 'dna', 'docking', 'doctor', 'documentaries', 'documentary', 'dodge', 'dog', 'dogs', 'dollar', 'dollars', 'dominated', 'done', 'donlevy', 'dont', 'donut', 'doomed', 'door', 'dos', 'dosen', 'double', 'doubt', 'douchey', 'dough', 'doughy', 'download', 'downloading', 'downright', 'downs', 'downside', 'downtown', 'dozen', 'dozens', 'dr', 'dracula', 'draft', 'drag', 'drago', 'drain', 'drained', 'drains', 'drama', 'dramatic', 'drastically', 'drawback', 'drawing', 'drawings', 'drawn', 'dream', 'dreamed', 'dreams', 'dreary', 'drenched', 'dressed', 'dressing', 'dribble', 'dried', 'driest', 'drift', 'drifting', 'drink', 'drinking', 'drinks', 'dripping', 'drive', 'driving', 'drivng', 'droid', 'drooling', 'drop', 'dropped', 'dropping', 'drops', 'drunk', 'dry', 'dual', 'duck', 'dude', 'due', 'duet', 'dull', 'dumb', 'dumbest', 'duo', 'duper', 'durable', 'duris', 'dusted', 'dustin', 'dustpan', 'dvd', 'dwight', 'dying', 'dylan', 'dysfunction', 'ear', 'earbud', 'earbuds', 'earbugs', 'eargels', 'earlier', 'early', 'earpad', 'earphone', 'earphones', 'earpiece', 'earpieces', 'ears', 'earset', 'earth', 'ease', 'easier', 'easily', 'easy', 'eat', 'eaten', 'eating', 'ebay', 'ebola', 'eccleston', 'echo', 'eclectic', 'ed', 'edge', 'edible', 'edinburgh', 'editing', 'edition', 'educational', 'edward', 'eel', 'eew', 'effect', 'effective', 'effects', 'efficient', 'effort', 'efforts', 'egg', 'eggplant', 'eggs', 'egotism', 'eighth', 'eiko', 'either', 'elaborately', 'elderly', 'electronics', 'elegant', 'elegantly', 'element', 'elias', 'elk', 'eloquently', 'else', 'elsewhere', 'email', 'embarassing', 'embarrassed', 'embarrassing', 'embassy', 'embedded', 'emerge', 'emilio', 'emily', 'emoting', 'emotion', 'emotionally', 'emotions', 'emperor', 'employee', 'employees', 'empowerment', 'emptiness', 'empty', 'en', 'enchanting', 'encourage', 'end', 'endearing', 'ended', 'ending', 'endlessly', 'ends', 'energetic', 'energy', 'engaging', 'engineered', 'english', 'enhanced', 'enjoy', 'enjoyable', 'enjoyed', 'enjoyment', 'enough', 'ensued', 'enter', 'enterprise', 'entertained', 'entertaining', 'entertainment', 'enthusiastic', 'entire', 'entirely', 'entrance', 'entrees', 'env', 'episode', 'episodes', 'equally', 'equipment', 'equivalent', 'era', 'ergonomic', 'ericson', 'ericsson', 'errol', 'errors', 'escalating', 'escapism', 'especially', 'essence', 'essentially', 'establish', 'established', 'establishment', 'estate', 'estevez', 'etc', 'ethic', 'europe', 'european', 'evaluate', 'eve', 'even', 'evening', 'event', 'events', 'eventually', 'ever', 'every', 'everybody', 'everyday', 'everyone', 'everything', 'everywhere', 'evidently', 'evil', 'evinced', 'evokes', 'exactly', 'exaggerating', 'example', 'excalibur', 'exceeding', 'exceeds', 'excelent', 'excellent', 'excellently', 'excels', 'except', 'exceptional', 'exceptionally', 'excerpts', 'excessive', 'excessively', 'exchange', 'exchanged', 'excited', 'exciting', 'exclaim', 'excruciatingly', 'excrutiatingly', 'excuse', 'excuses', 'executed', 'exemplars', 'exercise', 'existent', 'existential', 'existing', 'expanded', 'expansive', 'expect', 'expectations', 'expected', 'expecting', 'expensive', 'experience', 'experienced', 'experiences', 'experiencing', 'expert', 'explain', 'explains', 'explanation', 'exploit', 'explorations', 'explosion', 'expression', 'exquisite', 'extant', 'extended', 'extensive', 'exterior', 'exteriors', 'external', 'extra', 'extraneous', 'extraordinary', 'extremely', 'eye', 'eyed', 'eyes', 'fabulous', 'face', 'faceplates', 'faces', 'facial', 'facing', 'fact', 'factor', 'factory', 'fail', 'failed', 'fails', 'fair', 'fairly', 'faithful', 'falafels', 'fall', 'falling', 'falls', 'falsely', 'falwell', 'fame', 'famed', 'familiar', 'family', 'famous', 'fan', 'fanciful', 'fancy', 'fans', 'fantastic', 'fantasy', 'far', 'farce', 'fare', 'fascinated', 'fascinating', 'fascination', 'fashioned', 'fast', 'faster', 'fat', 'father', 'faultless', 'fausa', 'faux', 'fav', 'favor', 'favorite', 'favourite', 'fear', 'feature', 'features', 'fee', 'feel', 'feeling', 'feelings', 'feels', 'feet', 'feisty', 'fell', 'fella', 'fellow', 'fellowes', 'felt', 'female', 'females', 'ferry', 'fest', 'fi', 'fianc', 'fields', 'fifteen', 'fifties', 'figure', 'figured', 'file', 'filet', 'fill', 'fillet', 'filling', 'film', 'filmed', 'filmiing', 'filmmaker', 'filmography', 'films', 'final', 'finale', 'finally', 'financial', 'find', 'finds', 'fine', 'finest', 'finger', 'fingernails', 'fingers', 'finish', 'finished', 'fire', 'fireball', 'firehouse', 'first', 'fish', 'fishnet', 'fisted', 'fit', 'fits', 'five', 'fixes', 'flag', 'flair', 'flakes', 'flaming', 'flash', 'flashbacks', 'flat', 'flavor', 'flavored', 'flavorful', 'flavorless', 'flavors', 'flavourful', 'flaw', 'flawed', 'flawless', 'flawlessly', 'flaws', 'fleshed', 'flick', 'flicks', 'flimsy', 'flip', 'flipphones', 'fliptop', 'flirting', 'floor', 'flop', 'floppy', 'flops', 'florida', 'flowed', 'flower', 'fluffy', 'flush', 'fly', 'flying', 'flynn', 'fm', 'fo', 'focus', 'focused', 'fodder', 'folks', 'follow', 'followed', 'following', 'follows', 'fond', 'fondue', 'food', 'foods', 'fooled', 'foolish', 'foot', 'footage', 'football', 'force', 'forced', 'forces', 'ford', 'foreign', 'foreigner', 'forever', 'forgeries', 'forget', 'forgettable', 'forgetting', 'forgot', 'forgotten', 'form', 'format', 'former', 'fort', 'forth', 'forty', 'forward', 'forwarded', 'found', 'four', 'fourth', 'fox', 'foxx', 'fraction', 'frances', 'francis', 'francisco', 'frankly', 'freaking', 'free', 'freedom', 'freeman', 'freeway', 'freezes', 'freezing', 'french', 'frenchman', 'frequently', 'frequentyly', 'fresh', 'freshness', 'fridays', 'fried', 'friend', 'friendly', 'friends', 'friendship', 'fries', 'frightening', 'frog', 'front', 'frontier', 'frost', 'frozen', 'fruit', 'frustrated', 'frustration', 'fry', 'fs', 'ft', 'fucking', 'fulci', 'fulfilling', 'fulfills', 'full', 'fully', 'fumbling', 'fun', 'function', 'functional', 'functionality', 'functions', 'fundamental', 'funniest', 'funny', 'furthermore', 'future', 'fuzzy', 'fx', 'g', 'gabriel', 'gadget', 'gadgets', 'gain', 'gake', 'galley', 'gallon', 'game', 'games', 'ganoush', 'garage', 'garbage', 'garbled', 'garbo', 'garden', 'garfield', 'garlic', 'gas', 'gaudi', 'gave', 'gay', 'gc', 'geek', 'geeky', 'gels', 'gem', 'general', 'generally', 'generates', 'generic', 'generous', 'genius', 'genre', 'gentle', 'gently', 'genuine', 'genuinely', 'george', 'gerardo', 'gere', 'get', 'gets', 'getting', 'ghibili', 'giallo', 'giant', 'gibberish', 'gifted', 'gimmick', 'giovanni', 'girl', 'girlfriend', 'girls', 'girolamo', 'give', 'given', 'gives', 'giving', 'glad', 'glance', 'glare', 'glass', 'glasses', 'gloriously', 'glove', 'gloves', 'glued', 'gluten', 'go', 'goalies', 'goat', 'god', 'godfathers', 'goes', 'going', 'gold', 'golden', 'gone', 'gonna', 'good', 'google', 'gooodd', 'gordon', 'gore', 'goremeister', 'gorman', 'gosh', 'got', 'goth', 'gotta', 'gotten', 'gourmet', 'government', 'grab', 'grace', 'grade', 'gradually', 'grain', 'grainy', 'grandmother', 'granted', 'graphics', 'grasp', 'grates', 'gratitude', 'gratuity', 'grease', 'greasy', 'great', 'greater', 'greatest', 'greatness', 'greedy', 'greek', 'green', 'greens', 'greenstreet', 'greeted', 'grew', 'grey', 'grill', 'grilled', 'grim', 'grimes', 'gringos', 'grip', 'gripping', 'gristle', 'grocery', 'groove', 'gross', 'grossed', 'ground', 'group', 'groups', 'grow', 'grtting', 'guacamole', 'guards', 'guess', 'guest', 'guests', 'guilt', 'gung', 'guy', 'guys', 'gx', 'gyro', 'gyros', 'ha', 'hackneyed', 'haggis', 'hair', 'hairsplitting', 'half', 'halfway', 'halibut', 'ham', 'hamburger', 'han', 'hand', 'handed', 'handheld', 'handle', 'handled', 'handles', 'handling', 'handmade', 'hands', 'handset', 'handsfree', 'handy', 'hang', 'hankering', 'hankies', 'hanks', 'happen', 'happened', 'happening', 'happens', 'happier', 'happiness', 'happy', 'hard', 'hardest', 'hardly', 'harris', 'hat', 'hate', 'hated', 'hatred', 'haul', 'haunt', 'havilland', 'hawaiian', 'hay', 'hayao', 'hayworth', 'hbo', 'head', 'headbands', 'headphones', 'heads', 'headset', 'headsets', 'healthy', 'hear', 'heard', 'hearing', 'heart', 'hearts', 'heartwarming', 'heat', 'heaven', 'heavy', 'heche', 'heels', 'heimer', 'heist', 'held', 'helen', 'hell', 'hella', 'hellish', 'hello', 'helms', 'help', 'helped', 'helpful', 'helping', 'helps', 'hence', 'hendrikson', 'hereas', 'hernandez', 'hero', 'heroes', 'heroine', 'heroism', 'hes', 'hey', 'hi', 'hide', 'high', 'higher', 'highest', 'highlight', 'highlighted', 'highlights', 'highly', 'highy', 'hilarious', 'hill', 'hilt', 'hinge', 'hip', 'hiro', 'history', 'hit', 'hitch', 'hitchcock', 'hits', 'ho', 'hockey', 'hoffman', 'hold', 'holder', 'holding', 'holds', 'hole', 'holes', 'holiday', 'hollander', 'hollow', 'hollywood', 'holster', 'home', 'homemade', 'homework', 'honeslty', 'honest', 'honestly', 'honor', 'hook', 'hooked', 'hoot', 'hope', 'hoped', 'hopefully', 'hopeless', 'hopes', 'hoping', 'horrendous', 'horrendously', 'horrible', 'horrid', 'horrified', 'horror', 'horse', 'hospitality', 'host', 'hostess', 'hosting', 'hot', 'hottest', 'hour', 'hours', 'hoursthe', 'house', 'houses', 'howdy', 'howe', 'howell', 'however', 'hs', 'huevos', 'huge', 'hugo', 'human', 'humanity', 'humans', 'humiliated', 'hummh', 'humming', 'hummus', 'humor', 'humorous', 'humour', 'hunan', 'hundred', 'hungry', 'hurry', 'hurt', 'husband', 'huston', 'hut', 'hybrid', 'hype', 'hypocrisy', 'iam', 'ians', 'ice', 'iced', 'idea', 'ideal', 'idealogical', 'identified', 'identifies', 'identify', 'idiot', 'idiotic', 'idyllic', 'iffy', 'ignore', 'ignored', 'igo', 'ill', 'im', 'imac', 'images', 'imaginable', 'imagination', 'imaginative', 'imagine', 'imagined', 'imdb', 'imitation', 'immediately', 'impact', 'impeccable', 'imperial', 'implausible', 'important', 'impossible', 'impressed', 'impression', 'impressive', 'improper', 'improve', 'improved', 'improvement', 'improvisation', 'impulse', 'inappropriate', 'incendiary', 'inch', 'inches', 'included', 'includes', 'including', 'incoming', 'incomprehensible', 'inconsiderate', 'inconsistencies', 'inconspicuous', 'incorrectness', 'increase', 'incrediable', 'incredible', 'incredibly', 'indeed', 'indescribably', 'indian', 'indicate', 'indication', 'indictment', 'indie', 'individual', 'indoor', 'indoors', 'indulgent', 'industrial', 'industry', 'ineptly', 'inexcusable', 'inexpensive', 'inexperience', 'inexplicable', 'infatuated', 'inflate', 'inform', 'informative', 'infra', 'infuriating', 'ingredients', 'initially', 'innocence', 'insane', 'insanely', 'insert', 'inside', 'insincere', 'insipid', 'insomniacs', 'inspiration', 'inspired', 'inspiring', 'install', 'installed', 'instance', 'instant', 'instantly', 'instead', 'instruction', 'instructions', 'instruments', 'insulin', 'insult', 'insulted', 'insults', 'intangibles', 'integral', 'integrated', 'integration', 'intelligence', 'intelligent', 'intended', 'intense', 'intensity', 'intentions', 'interacting', 'interest', 'interested', 'interesting', 'interface', 'interim', 'interior', 'intermittently', 'internet', 'interplay', 'interpretations', 'interview', 'intoning', 'intrigued', 'invented', 'inventive', 'investment', 'inviting', 'involved', 'involves', 'involving', 'iphone', 'ipod', 'ipods', 'iq', 'ir', 'irda', 'ireland', 'iriver', 'ironically', 'ironman', 'irons', 'ironside', 'irritating', 'ishioka', 'issue', 'issues', 'italian', 'item', 'items', 'ive', 'jabra', 'jack', 'jaclyn', 'jalapeno', 'jamaican', 'james', 'jamie', 'japanese', 'jason', 'jawbone', 'jay', 'jealousy', 'jean', 'jeff', 'jenni', 'jennifer', 'jerk', 'jerks', 'jerky', 'jerry', 'jessica', 'jessice', 'jet', 'jewel', 'jiggle', 'jim', 'jimmy', 'job', 'jobs', 'joe', 'joey', 'john', 'join', 'joins', 'joint', 'joke', 'jokes', 'jonah', 'jones', 'journey', 'joy', 'joyce', 'juano', 'judge', 'judging', 'judith', 'judo', 'juice', 'julian', 'june', 'junk', 'junkyard', 'juries', 'justice', 'jutland', 'jx', 'kabuki', 'kanaly', 'kathy', 'keen', 'keep', 'keeping', 'keeps', 'keira', 'keith', 'kept', 'kevin', 'key', 'keyboard', 'keypad', 'keypads', 'keys', 'khao', 'kid', 'kiddos', 'kidnapped', 'kids', 'kieslowski', 'kill', 'killer', 'killing', 'killings', 'kind', 'kinda', 'kindle', 'kirk', 'kitchen', 'kitchy', 'kits', 'knew', 'knightley', 'knock', 'knocked', 'know', 'known', 'knows', 'koteas', 'kris', 'kristoffersen', 'krussel', 'kudos', 'la', 'labute', 'lack', 'lacked', 'lacking', 'lacks', 'ladies', 'lady', 'lame', 'lance', 'land', 'landscapes', 'lane', 'lange', 'lap', 'laptop', 'large', 'largely', 'larger', 'las', 'laselva', 'lassie', 'last', 'lasted', 'lasting', 'lastly', 'lasts', 'latch', 'latched', 'late', 'lately', 'later', 'latest', 'latifa', 'latin', 'latte', 'latter', 'laugh', 'laughable', 'laughed', 'laughing', 'laughs', 'law', 'lawyers', 'layers', 'lazy', 'lb', 'lead', 'leading', 'leaf', 'leaks', 'leap', 'learn', 'learned', 'least', 'leather', 'leave', 'leaves', 'leaving', 'lee', 'left', 'leftover', 'legal', 'legendary', 'legit', 'legs', 'lemon', 'length', 'leni', 'lense', 'leopard', 'less', 'lesser', 'lesson', 'lestat', 'let', 'letdown', 'lets', 'letting', 'lettuce', 'level', 'levels', 'lewis', 'lg', 'lid', 'lie', 'lies', 'lieutenant', 'life', 'lifetime', 'light', 'lighter', 'lighting', 'lightly', 'lights', 'lightweight', 'like', 'liked', 'likes', 'liking', 'lil', 'lilli', 'lilt', 'limitations', 'limited', 'linda', 'line', 'linear', 'lined', 'lines', 'linked', 'linking', 'linksys', 'lino', 'lion', 'list', 'listed', 'listener', 'listening', 'lit', 'literally', 'littered', 'little', 'live', 'lived', 'lives', 'living', 'loads', 'lobster', 'local', 'located', 'location', 'locations', 'lock', 'locked', 'locks', 'loewenhielm', 'logic', 'logitech', 'london', 'loneliness', 'long', 'longer', 'look', 'looked', 'looking', 'looks', 'loop', 'loops', 'loose', 'loosely', 'looses', 'lord', 'lordy', 'los', 'lose', 'losing', 'lost', 'lot', 'lots', 'loud', 'louder', 'loudest', 'loudly', 'loudspeaker', 'lousy', 'lovable', 'love', 'loved', 'lovely', 'lover', 'lovers', 'loves', 'loving', 'low', 'lower', 'lox', 'loyal', 'loyalty', 'lucio', 'luck', 'lucy', 'lugosi', 'luke', 'lukewarm', 'lunch', 'lust', 'luv', 'lyrics', 'mac', 'macarons', 'macbeth', 'machine', 'mad', 'made', 'madhouse', 'madison', 'magazine', 'magic', 'magical', 'magnetic', 'magnificent', 'mail', 'main', 'maine', 'mainly', 'mains', 'maintain', 'maintaining', 'maintains', 'major', 'majority', 'make', 'maker', 'makers', 'makes', 'making', 'male', 'males', 'mall', 'malta', 'man', 'managed', 'management', 'manager', 'manages', 'mandalay', 'mango', 'manna', 'mansonites', 'manual', 'manufacturer', 'many', 'marbles', 'march', 'margaritas', 'maria', 'marine', 'marion', 'mark', 'market', 'marred', 'marriage', 'marrow', 'martin', 'martini', 'mary', 'masculine', 'masculinity', 'massive', 'master', 'masterful', 'masterpiece', 'masterpieces', 'match', 'material', 'matrix', 'matter', 'matthews', 'mature', 'max', 'may', 'maybe', 'mayo', 'mchattie', 'mclaglen', 'meagre', 'meal', 'meals', 'mean', 'meanders', 'meaning', 'meanings', 'means', 'meant', 'meat', 'meatballs', 'meatloaf', 'meats', 'mechanism', 'media', 'medical', 'mediocre', 'mediterranean', 'medium', 'meet', 'mega', 'megapixels', 'meh', 'mein', 'meld', 'mellow', 'melodrama', 'melt', 'melted', 'melville', 'member', 'members', 'memorable', 'memories', 'memorized', 'memory', 'menace', 'menacing', 'mention', 'mentioned', 'menu', 'menus', 'mercy', 'mere', 'meredith', 'merit', 'mesmerising', 'mesquite', 'mess', 'message', 'messages', 'messaging', 'messes', 'metal', 'meteorite', 'metro', 'mexican', 'mgm', 'mic', 'michael', 'mickey', 'microphone', 'microsoft', 'mid', 'middle', 'might', 'mighty', 'mile', 'military', 'milk', 'milkshake', 'min', 'mind', 'mindblowing', 'mine', 'miner', 'mini', 'minor', 'mins', 'minute', 'minutes', 'mirage', 'mirrormask', 'miserable', 'miserably', 'mishima', 'misleading', 'misplace', 'miss', 'missed', 'missing', 'mistake', 'mistakes', 'mixed', 'miyazaki', 'mmmm', 'mobile', 'mode', 'model', 'modern', 'modest', 'moist', 'mojitos', 'mollusk', 'mom', 'moment', 'moments', 'momentum', 'money', 'monica', 'monkeys', 'monolog', 'monotonous', 'monster', 'monstrous', 'month', 'months', 'monumental', 'mood', 'moods', 'moral', 'morgan', 'morning', 'morons', 'mortified', 'mostly', 'mother', 'motion', 'motivations', 'moto', 'motor', 'motorola', 'motorolas', 'mountain', 'mouse', 'mouth', 'mouthful', 'mouths', 'move', 'moved', 'movements', 'moves', 'movie', 'movies', 'moving', 'moz', 'mozzarella', 'mp', 'ms', 'mst', 'much', 'muddled', 'muddy', 'muffin', 'muffled', 'multi', 'multiple', 'muppets', 'murder', 'murdered', 'murdering', 'murky', 'mushroom', 'mushrooms', 'music', 'musician', 'mussels', 'must', 'mute', 'mystifying', 'naan', 'nachos', 'naked', 'name', 'nan', 'nano', 'nargile', 'narration', 'narrative', 'nasty', 'national', 'nationalities', 'native', 'natural', 'nature', 'naughty', 'navigate', 'nay', 'nc', 'near', 'nearly', 'neat', 'necklace', 'need', 'needed', 'needless', 'needlessly', 'needs', 'negative', 'negatively', 'negligent', 'negulesco', 'neighborhood', 'neighbour', 'neil', 'neither', 'nerves', 'nervous', 'net', 'netflix', 'network', 'never', 'nevertheless', 'nevsky', 'new', 'next', 'ngage', 'nice', 'nicely', 'nicer', 'nicest', 'nicola', 'night', 'nightmare', 'nigiri', 'nimoy', 'nine', 'ninja', 'noble', 'nobody', 'nobu', 'noca', 'noir', 'noise', 'noises', 'nokia', 'non', 'none', 'nonetheless', 'nonsense', 'noodles', 'normal', 'normally', 'north', 'northern', 'nostalgia', 'notable', 'notch', 'note', 'noted', 'noteworthy', 'nothing', 'notice', 'noticed', 'novella', 'nude', 'number', 'numbers', 'numerous', 'nun', 'nuns', 'nurse', 'nut', 'nuts', 'nutshell', 'nyc', 'obliged', 'obsessed', 'obvious', 'obviously', 'occasional', 'occasionally', 'occasions', 'occupied', 'occur', 'occurs', 'odd', 'oem', 'offend', 'offensive', 'offer', 'offered', 'offering', 'offers', 'official', 'officially', 'often', 'oh', 'ohhh', 'oil', 'ok', 'okay', 'old', 'olde', 'older', 'ole', 'olives', 'olivia', 'omelets', 'omg', 'omit', 'one', 'ones', 'onion', 'online', 'oozes', 'open', 'opened', 'opening', 'opens', 'operas', 'operate', 'operates', 'operation', 'opinion', 'opportunity', 'opposed', 'optimal', 'option', 'options', 'ordeal', 'order', 'ordered', 'ordering', 'orders', 'organizational', 'oriented', 'original', 'originality', 'originally', 'origins', 'ortolani', 'os', 'oscar', 'others', 'otherwise', 'otto', 'ought', 'outdoor', 'outgoing', 'outlandish', 'outlet', 'outlets', 'outperform', 'outrageously', 'outshining', 'outside', 'outstanding', 'outta', 'outward', 'oven', 'overacting', 'overall', 'overcome', 'overcooked', 'overdue', 'overhaul', 'overly', 'overnight', 'overnite', 'overpriced', 'override', 'overs', 'overt', 'overwhelmed', 'overwrought', 'owed', 'owls', 'owned', 'owner', 'owners', 'owning', 'owns', 'oy', 'oysters', 'pace', 'paced', 'pacing', 'pack', 'package', 'packaged', 'packed', 'pad', 'pads', 'paid', 'pain', 'painful', 'painfully', 'paint', 'painted', 'pair', 'paired', 'pairing', 'palance', 'palate', 'pale', 'palm', 'palms', 'palmtop', 'pan', 'pancake', 'pancakes', 'pandering', 'panna', 'pans', 'pants', 'paolo', 'pap', 'paper', 'papers', 'par', 'paradise', 'parents', 'park', 'parker', 'part', 'partaking', 'particular', 'particularly', 'parties', 'parts', 'party', 'passed', 'passion', 'past', 'pasta', 'pastas', 'pastry', 'patent', 'pathetic', 'patient', 'patio', 'patriotism', 'patron', 'pats', 'patty', 'paul', 'pause', 'pay', 'paying', 'pc', 'pcs', 'pda', 'pe', 'peach', 'peachy', 'peaking', 'peanut', 'peanuts', 'pearls', 'pears', 'peas', 'pecan', 'peculiarity', 'pedestal', 'peeling', 'pencil', 'penne', 'penny', 'pens', 'people', 'pepper', 'perabo', 'perfect', 'perfected', 'perfection', 'perfectly', 'performance', 'performances', 'performed', 'performing', 'perhaps', 'period', 'periodically', 'periods', 'perpared', 'perplexing', 'person', 'personable', 'personalities', 'personally', 'peter', 'petrified', 'petroleum', 'petty', 'pg', 'phantasm', 'phenomenal', 'philadelphia', 'philippa', 'pho', 'phoenix', 'phone', 'phones', 'phony', 'photo', 'photograph', 'photography', 'phrase', 'physical', 'pi', 'piano', 'picked', 'pics', 'picture', 'pictures', 'piece', 'pieces', 'pile', 'pillow', 'pine', 'pineapple', 'pink', 'pissd', 'pita', 'pitch', 'pitiful', 'pixar', 'pixel', 'pizza', 'pizzas', 'place', 'placed', 'places', 'plain', 'plan', 'plane', 'planned', 'planning', 'plans', 'plantains', 'plantronics', 'plantronincs', 'plants', 'plastic', 'plate', 'plater', 'platter', 'play', 'played', 'player', 'players', 'playing', 'plays', 'pleasant', 'pleasantly', 'please', 'pleased', 'pleaser', 'pleasing', 'pleasure', 'pleather', 'pledge', 'plenty', 'plethora', 'plmer', 'plot', 'plug', 'plugged', 'plugs', 'plus', 'pm', 'pneumatic', 'pocket', 'pockets', 'poet', 'poetry', 'poignant', 'point', 'pointillistic', 'pointless', 'poised', 'poisoning', 'poler', 'polite', 'political', 'politically', 'politics', 'ponyo', 'poop', 'poor', 'poorly', 'pop', 'popcorn', 'popular', 'pork', 'port', 'portable', 'portion', 'portions', 'portraits', 'portrayal', 'portrayals', 'portrayed', 'portraying', 'positive', 'possesed', 'possibility', 'possible', 'possibly', 'post', 'posted', 'postinos', 'potato', 'potatoes', 'potentially', 'potted', 'poured', 'powdered', 'power', 'powerful', 'powerhouse', 'practical', 'practically', 'practice', 'pray', 'precisely', 'predict', 'predictable', 'predictably', 'prefer', 'preferably', 'prejudice', 'prelude', 'premise', 'premium', 'prepare', 'prepared', 'preparing', 'presence', 'presentation', 'presents', 'preservation', 'president', 'pretentious', 'pretext', 'prettier', 'pretty', 'prevents', 'previous', 'price', 'priced', 'prices', 'pricey', 'pricing', 'primal', 'primary', 'prime', 'print', 'privileged', 'probably', 'problem', 'problems', 'procedure', 'procedures', 'proceedings', 'process', 'proclaimed', 'produce', 'produced', 'producer', 'producers', 'product', 'production', 'products', 'professional', 'professionals', 'professor', 'profiterole', 'profound', 'program', 'progresses', 'promise', 'promised', 'promote', 'prompt', 'prompted', 'promptly', 'prone', 'propaganda', 'properly', 'pros', 'protected', 'protection', 'protective', 'protector', 'protects', 'proud', 'proudly', 'proven', 'provide', 'provided', 'provides', 'providing', 'provokes', 'provoking', 'ps', 'pseudo', 'psyched', 'psychological', 'psychotic', 'pub', 'public', 'publicly', 'pucks', 'puff', 'pull', 'pulled', 'pulling', 'pulls', 'pumpkin', 'punched', 'punches', 'punish', 'punishment', 'puppet', 'puppets', 'pur', 'purcashed', 'purchase', 'purchased', 'purchases', 'purchasing', 'pure', 'puree', 'purity', 'purpose', 'push', 'pushed', 'put', 'putting', 'puzzle', 'pyromaniac', 'qu', 'quaid', 'quaint', 'qualified', 'qualities', 'quality', 'quantity', 'question', 'questioning', 'quick', 'quicker', 'quickly', 'quiet', 'quinn', 'quit', 'quite', 'qwerty', 'race', 'racial', 'racism', 'radiant', 'raging', 'ramsey', 'ranch', 'rancheros', 'random', 'randomly', 'range', 'ranks', 'rapidly', 'rare', 'rarely', 'raspberry', 'rate', 'rated', 'rather', 'rating', 'ratings', 'ratio', 'rave', 'raver', 'raving', 'ravoli', 'raw', 'ray', 'razor', 'razr', 'reach', 'reaching', 'reactions', 'read', 'readers', 'reading', 'ready', 'real', 'realised', 'realistic', 'reality', 'realize', 'realized', 'really', 'reason', 'reasonable', 'reasonably', 'reasons', 'reboots', 'recall', 'reccomendation', 'reccommend', 'receipt', 'receive', 'received', 'receives', 'receiving', 'recent', 'recently', 'reception', 'recessed', 'recharge', 'recieve', 'recognition', 'recognizes', 'recommend', 'recommendation', 'recommended', 'recommending', 'reconciliation', 'recover', 'recurring', 'red', 'redeemed', 'redeeming', 'reduction', 'reenactments', 'references', 'refill', 'reflected', 'refrained', 'refreshing', 'refried', 'refund', 'refurb', 'refuse', 'refused', 'regarding', 'regardless', 'register', 'regret', 'regrettable', 'regrettably', 'regretted', 'regular', 'regularly', 'reheated', 'rejection', 'relate', 'related', 'relation', 'relations', 'relationship', 'relationships', 'relative', 'relatively', 'relax', 'relaxed', 'relaxing', 'release', 'released', 'reliability', 'relief', 'relleno', 'relocated', 'relying', 'remaining', 'remake', 'remarkable', 'remember', 'reminded', 'reminds', 'remorse', 'remotely', 'removing', 'rendering', 'renders', 'rendition', 'renowned', 'rent', 'reoccure', 'repair', 'repeated', 'repeating', 'repeats', 'repertory', 'replace', 'replaced', 'replacement', 'replacementr', 'replenished', 'reporter', 'represents', 'requested', 'require', 'requirements', 'rescue', 'research', 'researched', 'reservation', 'resistant', 'resolution', 'resounding', 'respect', 'respecting', 'rest', 'restaraunt', 'restart', 'restaurant', 'restaurants', 'restocking', 'restored', 'restrained', 'rests', 'result', 'results', 'resume', 'retarded', 'retreat', 'return', 'returned', 'returning', 'revealing', 'revenge', 'revere', 'reverse', 'reversible', 'review', 'reviewer', 'reviewers', 'reviewing', 'reviews', 'revisiting', 'rge', 'ri', 'rib', 'ribeye', 'rice', 'rich', 'rick', 'rickman', 'ride', 'ridiculous', 'ridiculousness', 'right', 'riingtones', 'ring', 'ringer', 'ringing', 'rings', 'ringtones', 'rinse', 'riot', 'rip', 'ripped', 'rips', 'rise', 'risk', 'risotto', 'rita', 'rivalry', 'riveted', 'riz', 'road', 'roam', 'roast', 'roasted', 'robert', 'robotic', 'rochon', 'rock', 'rocked', 'rocketed', 'rocks', 'roeg', 'role', 'roles', 'roll', 'rolled', 'roller', 'rolls', 'romantic', 'room', 'roosevelt', 'rotating', 'roth', 'rough', 'round', 'routine', 'row', 'rowdy', 'rpg', 'rpger', 'rubber', 'rubbish', 'rubin', 'rude', 'rudely', 'rumbles', 'run', 'running', 'runs', 'rushed', 'ruthless', 'ryan', 'ryans', 'sabotages', 'sack', 'sacrifice', 'sad', 'sadly', 'saffron', 'saganaki', 'saggy', 'said', 'sake', 'salad', 'salads', 'salesman', 'salmon', 'sals', 'salsa', 'salt', 'salty', 'sam', 'sample', 'samsung', 'san', 'sand', 'sandra', 'sandwich', 'sandwiches', 'sangria', 'sanyo', 'sappiest', 'sarcophage', 'sashimi', 'sat', 'satanic', 'satifying', 'satisfied', 'satisfying', 'satisifed', 'sauce', 'sauces', 'sause', 'savalas', 'savant', 'save', 'saved', 'saving', 'savor', 'saw', 'say', 'saying', 'says', 'scale', 'scallop', 'scamp', 'scare', 'scared', 'scares', 'scary', 'scene', 'scenery', 'scenes', 'sch', 'schilling', 'schizophrenic', 'school', 'schoolers', 'schrader', 'schultz', 'sci', 'science', 'scientist', 'score', 'scot', 'scottsdale', 'scratch', 'scratched', 'scream', 'screams', 'screamy', 'screen', 'screened', 'screenplay', 'screens', 'screenwriter', 'screwed', 'scrimm', 'script', 'scripted', 'scripting', 'scripts', 'sculpture', 'sea', 'seafood', 'seal', 'seamless', 'seamlessly', 'sean', 'searched', 'season', 'seasonal', 'seasoned', 'seasoning', 'seat', 'seated', 'seating', 'second', 'secondary', 'secondly', 'seconds', 'section', 'secure', 'securely', 'securly', 'see', 'seeen', 'seeing', 'seem', 'seemed', 'seems', 'seen', 'selection', 'selections', 'self', 'seller', 'sells', 'semi', 'send', 'sending', 'senior', 'sense', 'senses', 'sensibility', 'sensitive', 'sensitivities', 'sensor', 'sent', 'sentiment', 'seperate', 'seperated', 'sequel', 'sequels', 'sequence', 'sequences', 'sergeant', 'series', 'serious', 'seriously', 'serivce', 'serve', 'served', 'server', 'servers', 'serves', 'service', 'services', 'serving', 'set', 'sets', 'setting', 'settings', 'setup', 'seuss', 'sever', 'several', 'severe', 'sewer', 'sex', 'sexy', 'shakespear', 'shakespears', 'shall', 'shallow', 'shame', 'shameful', 'shape', 'share', 'sharing', 'sharp', 'sharply', 'shatner', 'shattered', 'shawarrrrrrma', 'shed', 'sheer', 'shelf', 'shell', 'shelves', 'shenanigans', 'shepard', 'shield', 'shifting', 'shine', 'shined', 'shiny', 'shipment', 'shipped', 'shipping', 'shirley', 'shirt', 'shocked', 'shocking', 'shoe', 'shooters', 'shooting', 'shoots', 'shop', 'shopping', 'shops', 'short', 'shortlist', 'shot', 'shots', 'shouldve', 'shouting', 'show', 'showcasing', 'showed', 'shower', 'shows', 'shrimp', 'shut', 'sibling', 'sick', 'side', 'sidelined', 'sides', 'sight', 'sign', 'signal', 'signals', 'significant', 'significantly', 'signs', 'silent', 'silently', 'silly', 'sim', 'similar', 'similarly', 'simmering', 'simple', 'simpler', 'simplifying', 'simply', 'since', 'sincere', 'sing', 'singing', 'single', 'sinister', 'sink', 'sinking', 'sins', 'sister', 'sisters', 'sit', 'sitcoms', 'site', 'sites', 'sits', 'sitting', 'situation', 'situations', 'six', 'size', 'sizes', 'sketchy', 'skilled', 'skimp', 'skip', 'skype', 'slackers', 'slavic', 'slaw', 'sleek', 'sleep', 'sliced', 'slices', 'slid', 'slide', 'slider', 'slideshow', 'sliding', 'slightest', 'slightly', 'slim', 'slimy', 'slipping', 'sloppy', 'slow', 'slowly', 'slurs', 'smack', 'small', 'smaller', 'smallest', 'smart', 'smartphone', 'smashburger', 'smeared', 'smell', 'smelled', 'smells', 'smile', 'smiling', 'smith', 'smoke', 'smoking', 'smooth', 'smoother', 'smoothies', 'smoothly', 'smudged', 'snap', 'snider', 'snow', 'snug', 'soap', 'sobering', 'social', 'soft', 'software', 'soggy', 'soi', 'sold', 'soldiers', 'sole', 'solid', 'solidify', 'solidifying', 'solving', 'somehow', 'someone', 'somethat', 'something', 'sometimes', 'somewhat', 'somewhere', 'son', 'song', 'songs', 'sony', 'soon', 'sooner', 'soooo', 'sooooo', 'soooooo', 'sophisticated', 'sore', 'sorely', 'sorrentino', 'sorry', 'sort', 'sos', 'soul', 'sound', 'sounded', 'sounds', 'soundtrack', 'soup', 'soups', 'sour', 'source', 'sources', 'south', 'southern', 'southwest', 'soyo', 'space', 'spacek', 'spacey', 'spaghetti', 'span', 'speak', 'speaker', 'speakerphone', 'speaking', 'special', 'specially', 'specials', 'specs', 'speed', 'speedy', 'spend', 'spends', 'spent', 'spew', 'sphere', 'spice', 'spices', 'spicier', 'spicy', 'spiffy', 'spinach', 'spinn', 'splendid', 'spock', 'spoil', 'spoiled', 'spoiler', 'spoilers', 'sporting', 'spot', 'spots', 'spotty', 'spring', 'sprint', 'sprouts', 'spy', 'squibs', 'stable', 'staff', 'stage', 'stagey', 'stagy', 'stale', 'stand', 'standard', 'standout', 'stanwyck', 'star', 'starlet', 'starring', 'stars', 'start', 'startac', 'started', 'starter', 'starts', 'starving', 'state', 'stated', 'static', 'station', 'stay', 'stayed', 'staying', 'stays', 'steak', 'steakhouse', 'steaks', 'stealing', 'steamboat', 'steele', 'steep', 'steer', 'steiners', 'step', 'stephen', 'stepped', 'stereo', 'stereotypes', 'stereotypically', 'steve', 'stewart', 'stick', 'sticks', 'still', 'stinker', 'stinks', 'stir', 'stocking', 'stockings', 'stoic', 'stomach', 'stood', 'stop', 'stopped', 'stops', 'storage', 'store', 'stories', 'storm', 'story', 'storyline', 'storytelling', 'stowe', 'strange', 'stranger', 'strangers', 'strap', 'stratus', 'straw', 'strawberry', 'stream', 'street', 'strength', 'stress', 'stretch', 'strident', 'strike', 'string', 'strings', 'strip', 'strives', 'strokes', 'strong', 'struck', 'structure', 'struggle', 'stuart', 'stuck', 'student', 'students', 'studio', 'study', 'stuff', 'stuffed', 'stunning', 'stupid', 'stupidity', 'sturdiness', 'sturdy', 'style', 'styles', 'styling', 'stylish', 'stylized', 'styrofoam', 'sub', 'subject', 'subjects', 'sublime', 'sublimely', 'submerged', 'subpar', 'subplots', 'subtitles', 'subtle', 'subversive', 'subverting', 'subway', 'succeeded', 'succeeds', 'success', 'succulent', 'suck', 'sucked', 'sucker', 'sucks', 'sudden', 'suddenly', 'suffered', 'suffering', 'suffers', 'sugar', 'sugary', 'suggest', 'suggestions', 'suggests', 'suited', 'sum', 'summarize', 'summary', 'summer', 'sun', 'sunday', 'sundays', 'sunglasses', 'super', 'superb', 'superbad', 'superbly', 'superfast', 'superficial', 'superlative', 'supernatural', 'supertooth', 'support', 'supporting', 'supposed', 'supposedly', 'suprised', 'sure', 'surefire', 'surely', 'surf', 'surface', 'surprise', 'surprised', 'surprises', 'surprising', 'surprisingly', 'surrounding', 'surroundings', 'survived', 'survivors', 'sushi', 'suspense', 'suspension', 'sven', 'swamp', 'sweep', 'sweet', 'sweetest', 'switch', 'switched', 'swivel', 'swords', 'swung', 'sydney', 'sympathetic', 'sync', 'synchronization', 'syrupy', 'system', 'table', 'tables', 'tacky', 'taco', 'tacos', 'taelons', 'tailored', 'take', 'taken', 'takeout', 'takes', 'taking', 'tale', 'talent', 'talented', 'talents', 'talk', 'talking', 'tanks', 'tap', 'tapas', 'tape', 'taped', 'tardis', 'tartar', 'tartare', 'task', 'taste', 'tasted', 'tasteless', 'tastings', 'tasty', 'tater', 'taxidermists', 'taylor', 'tea', 'teacher', 'teaches', 'team', 'teamwork', 'tear', 'tears', 'tech', 'technically', 'technology', 'teddy', 'tedium', 'teen', 'teenagers', 'teeth', 'telephone', 'television', 'tell', 'telly', 'temp', 'temperaments', 'ten', 'tender', 'tenders', 'tension', 'tensions', 'tepid', 'terminology', 'terms', 'terrible', 'terribly', 'terrific', 'terror', 'texas', 'text', 'texture', 'th', 'thai', 'thank', 'thanks', 'thats', 'theater', 'theatre', 'theatres', 'theatrical', 'theft', 'theme', 'themes', 'theory', 'therapy', 'thereplacement', 'thick', 'thin', 'thing', 'things', 'think', 'thinking', 'thinly', 'third', 'thirty', 'thomerson', 'thorn', 'thoroughly', 'thorsen', 'though', 'thought', 'thoughts', 'thousand', 'thread', 'three', 'threshold', 'threw', 'thrilled', 'thriller', 'thrillers', 'throughout', 'throwback', 'thrown', 'thru', 'ths', 'thug', 'thumbs', 'thumper', 'thunderbirds', 'thus', 'tick', 'ticker', 'tickets', 'ticking', 'tied', 'tigerlilly', 'tight', 'tightly', 'time', 'timeframe', 'timeless', 'timely', 'timers', 'times', 'timing', 'tinny', 'tiny', 'tip', 'tips', 'tiramisu', 'tired', 'title', 'titta', 'tmobile', 'toactivate', 'toast', 'toasted', 'today', 'together', 'toilet', 'told', 'tolerable', 'tolerance', 'tolerate', 'tom', 'tomato', 'tomorrow', 'tone', 'tones', 'tongue', 'tonight', 'tons', 'tony', 'took', 'tool', 'tools', 'toons', 'tooth', 'top', 'topic', 'tops', 'toro', 'torture', 'tortured', 'total', 'totally', 'tots', 'touch', 'touched', 'touches', 'touching', 'tough', 'towards', 'towers', 'town', 'townsend', 'tracfone', 'tracfonewebsite', 'track', 'tracked', 'tracking', 'tract', 'traditional', 'traffic', 'tragedy', 'trailer', 'train', 'tranquillity', 'transceiver', 'transcend', 'transcendant', 'transfer', 'transfers', 'transformed', 'translate', 'translating', 'transmission', 'transmit', 'transmitters', 'trap', 'trash', 'trashy', 'travled', 'treachery', 'treasure', 'treat', 'treated', 'treatments', 'trek', 'tremendous', 'tremendously', 'treo', 'tribute', 'tricky', 'tried', 'tries', 'trilogy', 'trimmed', 'trinity', 'trip', 'trippy', 'trips', 'triumphed', 'trond', 'trooper', 'trouble', 'truck', 'true', 'truffle', 'truly', 'trumbull', 'trumpeter', 'trunk', 'trust', 'truth', 'try', 'trying', 'trysts', 'tsunami', 'tucson', 'tummy', 'tuna', 'tuneful', 'tungsten', 'turkey', 'turn', 'turned', 'turns', 'tv', 'twice', 'twirling', 'twist', 'twists', 'two', 'tying', 'type', 'typical', 'ue', 'ugliest', 'ugly', 'uhura', 'ultra', 'um', 'unacceptable', 'unacceptible', 'unaccompanied', 'unbearable', 'unbearably', 'unbelievable', 'unbelievably', 'uncalled', 'uncomfortable', 'unconditional', 'unconvincing', 'underacting', 'underappreciated', 'underbite', 'undercooked', 'underlines', 'underlying', 'underneath', 'underrated', 'understand', 'understanding', 'understated', 'understatement', 'understood', 'undertone', 'underwater', 'underwhelming', 'undoubtedly', 'uneasy', 'unemployed', 'unethical', 'unexperienced', 'unfaithful', 'unfolds', 'unforgettable', 'unfortunate', 'unfortunately', 'unfunny', 'unhappy', 'unhealthy', 'uninspired', 'unintelligible', 'unintentionally', 'uninteresting', 'union', 'unique', 'uniqueness', 'unit', 'units', 'universal', 'universe', 'unknown', 'unless', 'unlike', 'unlockable', 'unmatched', 'unmitigated', 'unmoving', 'unnecessary', 'unneeded', 'unoriginal', 'unpleasant', 'unpredictability', 'unpredictable', 'unprofessional', 'unreal', 'unrealistic', 'unrecognizable', 'unrecommended', 'unreliable', 'unremarkable', 'unrestrained', 'unsatisfactory', 'unsatisfying', 'untoasted', 'unusable', 'unwatchable', 'unwelcome', 'unwrapped', 'upa', 'upbeat', 'update', 'upgrade', 'upgrading', 'uplifting', 'upload', 'uploaded', 'upper', 'ups', 'upstairs', 'uptight', 'ursula', 'us', 'usable', 'usage', 'usb', 'use', 'used', 'useful', 'usefulness', 'useless', 'user', 'uses', 'using', 'ussr', 'usual', 'usually', 'utter', 'utterly', 'vacant', 'vain', 'valentine', 'valley', 'value', 'values', 'vampire', 'vandiver', 'vanilla', 'variation', 'veal', 'vegan', 'vegas', 'vegetables', 'vegetarian', 'veggie', 'veggitarian', 'vehicle', 'vehicles', 'velvet', 'ventilation', 'ventura', 'venture', 'venturing', 'venue', 'verbal', 'verbatim', 'verge', 'verizon', 'versatile', 'version', 'versus', 'vessel', 'veteran', 'vey', 'via', 'vibe', 'victor', 'video', 'videos', 'view', 'viewer', 'viewing', 'views', 'villain', 'villains', 'vinaigrette', 'vinegrette', 'violence', 'violin', 'violinists', 'virgin', 'virtue', 'virus', 'vision', 'visit', 'visited', 'visor', 'visual', 'visually', 'vitally', 'vivian', 'vivid', 'vocal', 'vodka', 'voice', 'volatile', 'volcano', 'voltage', 'volume', 'vomit', 'vomited', 'voodoo', 'voted', 'voyage', 'vulcan', 'vx', 'waaaaaayyyyyyyyyy', 'waaay', 'wagyu', 'wait', 'waited', 'waiter', 'waiting', 'waitress', 'waitresses', 'wake', 'walk', 'walked', 'walkman', 'wall', 'wallet', 'walls', 'want', 'wanted', 'wanting', 'wants', 'war', 'warm', 'warmer', 'warmth', 'warn', 'warning', 'warnings', 'warranty', 'wartime', 'warts', 'wash', 'washed', 'washing', 'waste', 'wasted', 'waster', 'wasting', 'watch', 'watchable', 'watched', 'watching', 'water', 'watered', 'waterproof', 'watkins', 'watson', 'wave', 'way', 'waylaid', 'wayne', 'ways', 'wayyy', 'wb', 'weak', 'weaker', 'wear', 'weariness', 'wearing', 'weaving', 'web', 'website', 'websites', 'wedding', 'wedges', 'week', 'weekend', 'weekly', 'weeks', 'weight', 'weird', 'welcome', 'well', 'welsh', 'went', 'whatever', 'whatsoever', 'whelm', 'whenever', 'whether', 'whine', 'whiny', 'whistles', 'white', 'whites', 'whoa', 'whoever', 'whole', 'wholesome', 'whose', 'wi', 'wide', 'widmark', 'wienerschnitzel', 'wife', 'wih', 'wild', 'wildly', 'wilkinson', 'william', 'willie', 'wily', 'win', 'wind', 'window', 'windows', 'wine', 'wines', 'wings', 'winner', 'wiping', 'wire', 'wired', 'wirefly', 'wireless', 'wise', 'wish', 'wit', 'within', 'without', 'witnessed', 'witticisms', 'witty', 'woa', 'wobbly', 'women', 'wonder', 'wondered', 'wonderful', 'wonderfully', 'wong', 'wont', 'wontons', 'woo', 'wood', 'wooden', 'word', 'words', 'work', 'worked', 'worker', 'workers', 'working', 'works', 'world', 'worn', 'worries', 'worry', 'worse', 'worst', 'worth', 'worthless', 'worthwhile', 'worthy', 'would', 'wouldnt', 'wound', 'woven', 'wow', 'wrap', 'wrapped', 'write', 'writer', 'writers', 'writing', 'written', 'wrong', 'wrongly', 'wrote', 'x', 'ya', 'yama', 'yardley', 'yawn', 'yay', 'yeah', 'year', 'years', 'yell', 'yellow', 'yellowtail', 'yelpers', 'yelps', 'yes', 'yet', 'young', 'younger', 'youthful', 'youtube', 'yucky', 'yukon', 'yum', 'yummy', 'yun', 'zero', 'zillion', 'zombie', 'zombiez']\n"
          ]
        }
      ],
      "source": [
        "#lista vacía para análisis visual\n",
        "palabras = []\n",
        "\n",
        "#ciclo para juntar todos los tokens previos en una lista\n",
        "for tokens1 in Xcleantok:\n",
        "\n",
        "  palabras.extend(tokens1)\n",
        "\n",
        "#Set para eliminar repeticiones\n",
        "dic = set(palabras)\n",
        "\n",
        "#Imprimir resultados para análisis visual\n",
        "print(sorted(dic))\n",
        "\n",
        "\n",
        "#Se crea un objeto de la clase PorterStemmer\n",
        "#ps = PorterStemmer() #Se requiere usar lematización para utilizar el método de vectores embebidos\n",
        "\n",
        "#Se crea un objeto de la clase WordNetLemmatizer\n",
        "WNL = WordNetLemmatizer()\n",
        "\n",
        "#Definición de la función de limpieza adicional\n",
        "def clean_doc(doc):\n",
        "\n",
        "  #Se define lista vacía para los nuevos tokens\n",
        "  # tokens = stemmer_tokens(doc)\n",
        "  tokens = []\n",
        "\n",
        "  #Ciclo para la limpieza de los tokens\n",
        "  for j in range(0,len(doc)):\n",
        "\n",
        "    #Encontar todas las variantes de so, good, fox, way y frequently\n",
        "    so = re.findall(r\"so{2,}\\b\", doc[j])\n",
        "    good = re.findall(r\"go{2,}d{1,}\\b\", doc[j])\n",
        "    fox = re.findall(r\"fo{1,}x{1,}\\b\", doc[j])\n",
        "    way = re.findall(r\"wa{1,}y{1,}\\b\", doc[j])\n",
        "    frequently = re.findall(r\"frequent*[a-z]+ly\\b\", doc[j])\n",
        "\n",
        "    #Si se encontraron variantes de 'so', reemplazarlas\n",
        "    if len(so) > 0:\n",
        "\n",
        "      doc[j] = 'so'\n",
        "\n",
        "    #Si se encontraron variantes de 'good', reemplazarlas\n",
        "    if len(good) > 0:\n",
        "\n",
        "      doc[j] = 'good'\n",
        "\n",
        "    #Si se encontraron variantes de 'fox', reemplazarlas\n",
        "    if len(fox) > 0:\n",
        "\n",
        "      doc[j] = 'fox'\n",
        "\n",
        "    #Si se encontraron variantes de 'way', reemplazarlas\n",
        "    if len(way) > 0:\n",
        "\n",
        "      doc[j] = 'way'\n",
        "\n",
        "    #Si se encontraron variantes de 'frequently', reemplazarlas\n",
        "    if len(frequently) > 0:\n",
        "\n",
        "      doc[j] = 'frequently'\n",
        "\n",
        "    #Se aplica stemming lo cual nos llevará las palabras a su base, incluyendo remover las terminaciones 'ing','ed','s'\n",
        "    #doc[j] = ps.stem(doc[j]) #Se aplica lematización en lugar de stemming\n",
        "\n",
        "    lem_token = doc[j]\n",
        "\n",
        "    #Sólo lematizar tokens mas de más de dos caracteres\n",
        "\n",
        "    if len(lem_token) > 2:\n",
        "\n",
        "      #Se intenta lematizar verbo\n",
        "      lem_token = WNL.lemmatize(doc[j],'v')\n",
        "\n",
        "      #Si el token permanece sin cambios, intentar lematizar como sustantivo\n",
        "      if lem_token == doc[j]:\n",
        "        lem_token = WNL.lemmatize(doc[j],'n')\n",
        "\n",
        "      #Si el token permanece sin cambios, intentar lematizar como adjetivo\n",
        "      elif lem_token == doc[j]:\n",
        "        lem_token = WNL.lemmatize(doc[j],'a')\n",
        "\n",
        "      #Si el token permanece sin cambios, intentar lematizar como advervio\n",
        "      elif lem_token == doc[j]:\n",
        "        lem_token = WNL.lemmatize(doc[j],'r')\n",
        "\n",
        "    #Asignar el resultado a doc[j]\n",
        "    doc[j] = lem_token\n",
        "\n",
        "  tokens.extend(doc)\n",
        "\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1pIMQaRSSMfd"
      },
      "outputs": [],
      "source": [
        "# Aplicamos el proceso de limpieza/normalización adicionales:\n",
        "\n",
        "Xclean = [clean_doc(x) for x in Xcleantok]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "F0OxOXN1STP1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "0f41787f-61a9-4cdc-d03c-43858433042c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['way', 'plug', 'us', 'unless', 'go', 'converter'],\n",
              " ['good', 'case', 'excellent', 'value'],\n",
              " ['great', 'jawbone'],\n",
              " ['tie', 'charger', 'conversation', 'last', 'minute', 'major', 'problem'],\n",
              " ['mic', 'great'],\n",
              " ['jiggle', 'plug', 'get', 'line', 'right', 'get', 'decent', 'volume'],\n",
              " ['several',\n",
              "  'dozen',\n",
              "  'several',\n",
              "  'hundred',\n",
              "  'contact',\n",
              "  'imagine',\n",
              "  'fun',\n",
              "  'send',\n",
              "  'one',\n",
              "  'one'],\n",
              " ['razr', 'owner', 'must'],\n",
              " ['needle', 'say', 'waste', 'money'],\n",
              " ['waste', 'money', 'time'],\n",
              " ['sound', 'quality', 'great'],\n",
              " ['impress', 'go', 'original', 'battery', 'extend', 'battery'],\n",
              " ['two',\n",
              "  'seperated',\n",
              "  'mere',\n",
              "  'ft',\n",
              "  'start',\n",
              "  'notice',\n",
              "  'excessive',\n",
              "  'static',\n",
              "  'garble',\n",
              "  'sound',\n",
              "  'headset'],\n",
              " ['good', 'quality', 'though'],\n",
              " ['design', 'odd', 'ear', 'clip', 'comfortable'],\n",
              " ['highly', 'recommend', 'one', 'blue', 'tooth', 'phone'],\n",
              " ['advise', 'everyone', 'fool'],\n",
              " ['far', 'good'],\n",
              " ['work', 'great'],\n",
              " ['click',\n",
              "  'place',\n",
              "  'way',\n",
              "  'make',\n",
              "  'wonder',\n",
              "  'long',\n",
              "  'mechanism',\n",
              "  'would',\n",
              "  'last']]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "Xclean[0:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDOBBjn9Wp__"
      },
      "source": [
        "#**Pregunta 3**\n",
        "\n",
        "Llamar Xclean a los comentarios procesados y Y a las etiquetas. Realicemos una partición aleatoria con los mismos porcentajes de la práctica pasada para poder comparar dichos resultados con los de esta actividad, a saber, 70%, 15% y 15%, para entrenamiento, validación y prueba, respectivamente. Verifica que obtienes 2,100 registros de entrenamiento y 450 para cada uno de validación y prueba.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "w88pAxNzX3Zu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8a8ad41c-8dfe-4e45-b758-50157807f75d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X,y Train: 2100 2100\n",
            "X,y Val: 450 450\n",
            "X,y Test 450 450\n"
          ]
        }
      ],
      "source": [
        "# Xclean = Comentarios procesados\n",
        "# Y = etiquetas\n",
        "\n",
        "x_train, x_val_and_test, y_train, y_val_and_test = train_test_split(Xclean, Y, train_size=.70, shuffle=True, random_state=1)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_val_and_test, y_val_and_test, test_size=.50, shuffle=True, random_state=17)\n",
        "\n",
        "print('X,y Train:', len(x_train), len(y_train))\n",
        "print('X,y Val:', len(x_val), len(y_val))\n",
        "print('X,y Test', len(x_test), len(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq3yjgMbYtmG"
      },
      "source": [
        "# **Pregunta 4**\n",
        "\n",
        "- Usando el conjunto de entrenamiento genera un vocabulario que no sea mayor a 1,500 palabras, ni menor a 1,000.\n",
        "\n",
        ">> **¿Por qué es importante acotar un vocabulario inferior y superiormente? ¿Por qué debe usarse solamente el conjunto de entrenamiento para generar el diccionario?**\n",
        "\n",
        ">>*Se acota el vocabulario porque se emplean las palabras que sólo aportan valor al análisis del corpus, generalmente las que tienen mayor frecuencia de aparición. Se emplea únicamente el diccionario del conjunto de entrenamiento para evitar el filtrado de información cuando se verifica el desempeño del modelo en los conjuntos de validación y prueba. Solamente estarán disponibles los elementos del diccionario del conjunto de entrenamiento para realizar las predicciones en los conjuntos posteriores.*\n",
        "\n",
        "- Con este vocabulario que obtienes, filtra los conjuntos de entrenamiento, validación y prueba, de esta manera todos los comentarios usarán solamente palabras válidas de acuerdo a este vocabulario.\n",
        "\n",
        "- Indica el tamaño del vocabulario obtenido.\n",
        "\n",
        "- Hasta este punto básicamente has realizado transformaciones muy análogas a las de la semana pasada y que son válidas para muchos de los procesos dentro del análisis de textos.\n",
        "\n",
        "- En dado caso comenta con tus compañeros de equipo qué diferencias has observado. Veamos ahora la diferencia con respecto a las matrices Tf-idf que aplicaste la semana pasada, con respecto a los vectores **preentrenados embebidos.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-_cb3UzBwtVb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "outputId": "465dd971-87a8-4b52-a121-8384dfc9c6d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitud del diccionario: 3265\n",
            "\n",
            "(word,frequency):\n",
            "[('good', 165), ('movie', 140), ('great', 138), ('phone', 134), ('film', 130), ('work', 113), ('like', 101), ('time', 101), ('one', 100), ('go', 89)]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Se importa el módulo Counter\n",
        "from collections import Counter\n",
        "\n",
        "#Se importa el módulo plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Se asigna un objeto de clase Counter\n",
        "midiccionario = Counter()\n",
        "\n",
        "#Se crea el diccionario con las palabras del conjunto de entrenamiento\n",
        "for k in range(len(x_train)):\n",
        "  midiccionario.update(x_train[k])\n",
        "\n",
        "\n",
        "print('Longitud del diccionario:', len(midiccionario))\n",
        "print('\\n(word,frequency):')\n",
        "print(midiccionario.most_common(10))\n",
        "\n",
        "# Veamos la gráfica de palabras nuestro diccionario con base a la frecuencia de las palabras/tokens:\n",
        "plt.plot(list(np.arange(len(midiccionario))), list(midiccionario.values()), color='blue')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNcZad_yyIvk"
      },
      "source": [
        "**Filtrado por frecuencia de aparición hasta llegar al tamaño de vocabulario deseado**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SBDoD_d0x8yz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ca487d7f-920b-4e0e-deb4-8d3ca6ce3843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nueva longitud del nuevo vocabulario: 1429\n",
            "[('star', 18), ('fare', 2), ('much', 39), ('better', 38), ('people', 23)]\n"
          ]
        }
      ],
      "source": [
        "#Frecuencia mínima deseada\n",
        "min_freq = 2\n",
        "\n",
        "#Copia de midiccionario para realizar operaciones\n",
        "midicc = midiccionario.copy()\n",
        "\n",
        "#Ciclo para filtar los elementos con base a su frecuencia\n",
        "for x in midiccionario:\n",
        "  if midiccionario[x] < min_freq:\n",
        "    del midicc[x]   #Eliminar tokens con menos frecuencia que la deseada\n",
        "\n",
        "print('Nueva longitud del nuevo vocabulario:', len(midicc))\n",
        "print(list(midicc.items())[0:5])     # veamos algunos elementos del diccionario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "oCg0nFcxG-pM"
      },
      "outputs": [],
      "source": [
        "#Se filtran los todos los conjuntos con el nuevo diccionario\n",
        "train_x = []\n",
        "train_y = []\n",
        "for ss, y in zip(x_train, y_train):\n",
        "  tokens = []\n",
        "  for w in ss:\n",
        "    if w in midicc:\n",
        "      tokens.append(w)\n",
        "\n",
        "  #Solo añadir a las listas si el token existe en el diccionario\n",
        "  if len(tokens) > 0:\n",
        "    train_x.append(tokens)\n",
        "    train_y.append(y)\n",
        "\n",
        "val_x = []\n",
        "val_y = []\n",
        "for ss, y in zip(x_val, y_val):\n",
        "  tokens = []\n",
        "  #train_x.append([w for w in ss if w in midicc])\n",
        "  for w in ss:\n",
        "    if w in midicc:\n",
        "      tokens.append(w)\n",
        "\n",
        "  #Solo añadir a las listas si el token existe en el diccionario\n",
        "  if len(tokens) > 0:\n",
        "    val_x.append(tokens)\n",
        "    val_y.append(y)\n",
        "\n",
        "test_x = []\n",
        "test_y = []\n",
        "for ss, y in zip(x_test, y_test):\n",
        "  tokens = []\n",
        "  #train_x.append([w for w in ss if w in midicc])\n",
        "  for w in ss:\n",
        "    if w in midicc:\n",
        "      tokens.append(w)\n",
        "\n",
        "  #Solo añadir a las listas si el token existe en el diccionario\n",
        "  if len(tokens) > 0:\n",
        "    test_x.append(tokens)\n",
        "    test_y.append(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nRoQLNtZNL0y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "fc527de1-9f6e-4fcf-b366-feb8093dd138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Antes: ['co', 'star', 'fare', 'much', 'better', 'people', 'like', 'morgan', 'freeman', 'jonah', 'hill', 'ed', 'helm', 'waste']\n",
            "Antes x_train: 2100\n",
            "Antes y_train: 2100\n",
            "Después: ['star', 'fare', 'much', 'better', 'people', 'like', 'morgan', 'ed', 'waste']\n",
            "Después: 2090\n",
            "Después: 2090\n",
            "Antes: ['tonight', 'elk', 'filet', 'special', 'suck']\n",
            "Antes x_train: 2100\n",
            "Antes y_train: 2100\n",
            "Después: ['tonight', 'filet', 'special', 'suck']\n",
            "Después: 2090\n",
            "Después: 2090\n",
            "Antes: ['pay', 'bill', 'tip', 'felt', 'server', 'terrible', 'job']\n",
            "Antes x_train: 2100\n",
            "Antes y_train: 2100\n",
            "Después: ['pay', 'bill', 'tip', 'felt', 'server', 'terrible', 'job']\n",
            "Después: 2090\n",
            "Después: 2090\n"
          ]
        }
      ],
      "source": [
        "# Podemos ver algunos de los comentarios de entrenamiento, antes y después\n",
        "# de incluir la condición de la frecuencia mínima de ocurrencia de un token:\n",
        "\n",
        "for k in range(3):\n",
        "  print('Antes:', x_train[k])\n",
        "  print('Antes x_train:', len(x_train))\n",
        "  print('Antes y_train:', len(y_train))\n",
        "  print('Después:', train_x[k])\n",
        "  print('Después:', len(train_x))\n",
        "  print('Después:', len(train_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_ySQygSEdcK"
      },
      "source": [
        "# **Pregunta 5**\n",
        "Utilizarás los vectores embebidos FastText preentrenados por Facebook.\n",
        "\n",
        "Incluye una tabla comparativa de pros y contras entre los modelos FastText, word2vec de Google y Glove de Stanford. Puedes consultar sus páginas correspondientes:\n",
        "- https://fasttext.cc/\n",
        "- https://code.google.com/archive/p/word2vec/\n",
        "- https://nlp.stanford.edu/projects/glove/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD8-dBWo9T9j"
      },
      "source": [
        "| Características | FastText | Word2Vec | GloVe |\n",
        "| ----------- | ----------- | ----------- | ----------- |\n",
        "| **Institución desarrolladora** | Facebook AI | Google | Stanford University |\n",
        "| **Modelo** | Basado en la arquitectura de skip-gram y CBOW | Basado en las arquitecturas de skip-gram y CBOW | Basado en la matriz de co-ocurrencia de palabras en el corpus |\n",
        "| **Manejo de palabras fuera del vocabulario (OOV)** | Buen manejo gracias al uso de n-grams subwords | No lo soporta | No lo soporta |\n",
        "| **Manejo de palabras raras** | Buen manejo gracias al uso de n-grams subwords | Puede tener dificultades con palabras raras | Buen manejo dependiendo de la frecuencia en la matriz de co-ocurrencia |\n",
        "| **Velocidad de entrenamiento** | Media | Rápido | Lento en comparación con FastText y Word2Vec debido al uso de la matriz de co-ocurrencia |\n",
        "| **Dimensiones de los vectores** | Configurable, hasta 300 dimensiones | Configurable, hasta 300 dimensiones | Configurable, hasta 300 dimensiones |\n",
        "| **Requisitos de memoria** | Moderados, debido a los subwords n-grams | Bajos, sólo necesita palabras y contextos | Altos, debido a la matriz de co-ocurrencia |\n",
        "| **Interpretación semántica** | Buena, similar a Word2Vec | Buena, basada en contextos cercanos | Buena, basada en relaciones semánticas y sintácticas |\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "Es importante mencionar que cada uno de estos modelos puede ser el más adecuado dependiendo de las necesidades específicas del problema que se esté abordando.\n",
        "\n",
        "Por ejemplo, si necesitas manejar palabras fuera del vocabulario o palabras raras, FastText podría ser la mejor opción; si la memoria es una limitación, Word2Vec podría ser más adecuado; GloVe, por otro lado, podría ser más útil para capturar relaciones semánticas y sintácticas entre palabras.\n",
        "\n",
        "Por otro lado, GloVe podría desempeñarse mejor que Word2Vec para tareas de analogía de palabras, y también muestra un desempeño superior en tareas de similitud y de reconocimiento de nombre de entidades. Mientras que FastText logra un mejor desempeño en tareas sintácticas. Por último, Word2Vec sobrepasa a FastText en tareas semánticas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Bf1J3AiEt-d"
      },
      "source": [
        "# **Pregunta 6**\n",
        "Utiliza el modelo **FastText** de vectores embebidos pre-entrenados de dimensión 300 para generar un nuevo diccionario clave-valor, donde la “clave” será cada token o palabra de tu vocabulario y el “valor” será su vector embebido de dimensión 300. Este diccionario deberá ser del mismo tamaño que el vocabulario previo que hayas construido previamente.\n",
        "https://fasttext.cc/docs/en/crawl-vectors.html\n",
        "\n",
        "**NOTA**: Debido a la cantidad de recursos computacionales que demanda cargar los vectores FastText (son 2 millones de vectores), es recomendable que una vez que generes el nuevo vocabulario de vectores embebidos, guardes dicho diccionario en un archivo (pickle, npz o el que consideres más adecuado). Una vez realizado lo anterior, puedes borrar la variable de FastText para liberar memoria RAM. De esta manera, ya tienes tu vocabulario de vectores embebidos de acuerdo a los tokens que consideras más adecuados para tu problema y puedes usarlo rápidamente\n",
        "cuando lo necesites. En dado caso apóyense entre los miembros del equipo de tener dificultades para generar el vocabulario y por mientras puedes usar el archivo del vocabulario que alguno haya generado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H3SjaXtFjbJ"
      },
      "source": [
        "**Se instala el módulo de Cython y FastText**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "W8ZJ0jF1EcUq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b4cbd8d5-442c-47f8-e892-81230480001b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.12.0-py3-none-any.whl (234 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.25.2)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4227136 sha256=088394a3a7875f24b96121e8d171087822e05eceaa5e44d7afeea48763720479\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.12.0\n",
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: --install-option\n"
          ]
        }
      ],
      "source": [
        "!pip install fasttext\n",
        "!pip install Cython --install-option=\"--no-cython-compile\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QeNah30Fz6T"
      },
      "source": [
        "**Se importa el módulo de fasttext**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "WcwHjBoFF6sl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "b031ab24-e668-45b9-caf3-e050fa51f3c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cc.en.300.bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "import fasttext\n",
        "import fasttext.util\n",
        "\n",
        "#Se descargan los el modelo de idioma inglés\n",
        "fasttext.util.download_model('en', if_exists='ignore')  # English"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "KWrcylNjJVw4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "fcfa4952-a72f-4860-8c69-c3250c8949ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "#Se utiliza el modelo FastText de vectores embebidos pre-entrenados de dimensión 300\n",
        "ft = fasttext.load_model('cc.en.300.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Cizy_S4-LDa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6a43a925-62ad-4f33-c6cd-feb164dc738b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitud del vocabulario de vectores embebidos: 1429\n",
            "Las dimensiones de midicc_vec son: 1429,300\n",
            "[('star', array([-2.86828041e-01,  1.15123533e-01,  5.38213179e-02,  6.26803637e-02,\n",
            "        2.47128960e-02, -2.57342309e-02,  1.47269234e-01, -1.16852485e-01,\n",
            "        1.43789779e-02,  2.02018499e-01, -6.00243136e-02, -6.75183982e-02,\n",
            "       -4.77372715e-03,  4.89413664e-02,  1.16688116e-02, -3.23957461e-03,\n",
            "       -1.23359617e-02, -8.51226449e-02, -4.92033921e-02,  6.35383427e-02,\n",
            "        8.86741839e-03,  1.40952796e-01,  1.09847665e-01,  1.25429276e-02,\n",
            "       -4.25020084e-02, -8.08417723e-02,  2.70867231e-03,  3.24510969e-02,\n",
            "        1.41084492e-02,  1.06206566e-01,  1.08138800e-01, -5.41383438e-02,\n",
            "        5.69636673e-02,  5.37327491e-02, -7.06092343e-02, -9.78406295e-02,\n",
            "       -1.20484188e-01, -4.16465327e-02, -8.21356028e-02,  4.71536033e-02,\n",
            "        9.90687776e-03,  4.89758067e-02, -1.33870468e-01, -1.54928997e-01,\n",
            "       -1.81018680e-01,  1.78726465e-02,  7.79893203e-03, -8.59202892e-02,\n",
            "       -2.29937509e-02, -1.81513757e-01, -1.04256801e-01,  1.19395129e-01,\n",
            "        7.99314678e-03,  5.44246733e-02,  1.18487053e-01,  4.88866717e-02,\n",
            "        5.15220016e-02, -4.63055186e-02,  8.03847145e-03, -5.37553839e-02,\n",
            "       -1.83139741e-02, -9.42408741e-02, -1.01035193e-01,  3.56963836e-02,\n",
            "        1.93327591e-02,  2.09810555e-01,  2.19372492e-02,  2.13999581e-02,\n",
            "        1.16238985e-02, -1.17042363e-01, -1.48543967e-02,  3.18749472e-02,\n",
            "       -1.81292221e-01, -8.17631930e-02,  2.79697850e-02, -3.79535183e-02,\n",
            "        1.45776663e-02,  1.37915611e-01, -8.08674842e-02,  2.51751058e-02,\n",
            "       -6.64169192e-02,  9.05220862e-03,  1.35277987e-01,  2.18077898e-02,\n",
            "        3.70039642e-02, -1.99556798e-02, -1.50145322e-01, -8.03397596e-03,\n",
            "       -7.00799674e-02, -2.94313934e-02, -3.02728191e-02,  1.93609685e-01,\n",
            "        1.89876668e-02, -7.70288929e-02,  1.80713087e-02,  1.10033236e-01,\n",
            "        7.12299794e-02, -4.59963679e-02, -1.72077119e-01,  3.16679366e-02,\n",
            "       -5.98362945e-02, -8.74337852e-02, -8.86409208e-02, -2.87565850e-02,\n",
            "       -5.81971705e-02,  1.01937577e-01,  3.15561742e-02,  1.15741333e-02,\n",
            "       -1.06359929e-01,  1.87610969e-01,  1.45370346e-02,  5.99830151e-02,\n",
            "        9.92235392e-02,  1.91061702e-02, -2.38206740e-02, -2.17346236e-01,\n",
            "        7.70934811e-03, -3.35183665e-02, -1.03727773e-01, -4.16280292e-02,\n",
            "        1.53034225e-01,  8.58682245e-02,  3.14186066e-02,  1.54228285e-01,\n",
            "       -8.03690311e-03,  3.32052186e-02,  4.11878666e-03,  1.05484031e-01,\n",
            "        7.57255778e-02, -3.18720341e-02,  1.70250125e-02,  6.56677261e-02,\n",
            "        1.23368874e-01, -3.03245634e-02,  2.33012419e-02, -1.44071028e-01,\n",
            "        1.45928591e-01,  6.17721789e-02,  1.98618054e-01, -1.88623518e-01,\n",
            "       -6.05288260e-02,  7.98067078e-02,  3.88783626e-02, -1.24917820e-01,\n",
            "       -1.37684941e-01,  6.50793165e-02, -2.99405992e-01, -3.73671472e-04,\n",
            "        4.60229479e-02, -7.02350959e-02, -3.00859064e-02, -1.02555119e-01,\n",
            "       -8.37509986e-03, -3.45986453e-04,  9.31164846e-02, -3.90904620e-02,\n",
            "        1.14622459e-01,  1.26173794e-01,  5.39926952e-03,  2.46864315e-02,\n",
            "       -7.46607184e-02,  6.94717746e-03,  1.56693712e-01, -1.52236253e-01,\n",
            "        3.30955088e-02, -4.14339192e-02, -6.96366578e-02,  9.96302366e-02,\n",
            "       -6.03845380e-02,  9.29320753e-02, -2.12934427e-03, -6.01781867e-02,\n",
            "       -7.10362718e-02, -1.18315414e-01, -6.35388717e-02,  6.41066134e-02,\n",
            "       -5.91891073e-02,  1.27110809e-01,  7.15065151e-02,  6.29523173e-02,\n",
            "       -5.46345413e-02,  2.89074313e-02, -1.59469962e-01, -7.92971104e-02,\n",
            "       -1.03111260e-01, -8.40404853e-02,  6.54209182e-02,  6.68521225e-02,\n",
            "       -2.92568877e-02,  2.20480785e-01,  9.37886834e-02, -9.02144909e-02,\n",
            "       -4.41652164e-02,  8.60180557e-02, -1.45871434e-02,  4.97262180e-02,\n",
            "       -1.41114324e-01,  8.82492661e-02,  9.48378909e-03, -9.70369801e-02,\n",
            "        1.59810316e-02,  1.26596987e-02,  3.80288512e-02,  3.90619189e-02,\n",
            "        9.13637877e-02, -8.60894620e-02,  1.01811588e-01,  6.69082999e-02,\n",
            "        1.05031997e-01,  1.48328632e-01,  6.92542717e-02, -1.06625669e-01,\n",
            "       -1.52740449e-01, -7.56940758e-03,  3.13936025e-02,  3.49867865e-02,\n",
            "       -2.09618788e-02, -1.42902760e-02,  6.80341348e-02,  6.19825125e-02,\n",
            "       -2.84822695e-02,  1.18321627e-01, -1.13135815e-01, -1.77281257e-02,\n",
            "       -5.60616702e-03,  8.14713836e-02, -1.53869148e-02,  8.16864222e-02,\n",
            "       -1.04721271e-01,  1.58673406e-01, -7.53961354e-02,  3.07477619e-02,\n",
            "        9.25838854e-03, -1.48327835e-02,  7.91117456e-03, -3.18586826e-04,\n",
            "       -8.03159773e-02,  6.46696135e-04, -7.53018484e-02, -1.21981822e-01,\n",
            "        4.21633124e-02, -6.28293380e-02, -9.21363831e-02, -5.31597510e-02,\n",
            "        8.62716362e-02, -6.90141022e-02,  2.69403346e-02,  1.61921620e-01,\n",
            "       -4.50303815e-02,  2.98184454e-02, -3.96693758e-05, -7.90106505e-02,\n",
            "       -3.03728543e-02,  1.16063416e-01,  2.34403973e-03,  9.55789983e-02,\n",
            "       -2.87741404e-02, -9.72413719e-02,  3.45763527e-02,  1.02634273e-01,\n",
            "       -2.12719902e-01,  4.99785393e-02, -5.83627820e-02,  1.52304163e-02,\n",
            "       -1.22847423e-01,  4.66357172e-02,  1.46242663e-01,  1.13413274e-01,\n",
            "        1.09998845e-01, -2.39889026e-02,  3.31177339e-02,  4.13419865e-02,\n",
            "       -1.06826507e-01,  4.97730151e-02,  6.61367327e-02, -7.30071217e-02,\n",
            "       -2.99715679e-02,  6.65043890e-02, -5.69159463e-02,  4.16496157e-04,\n",
            "        1.42523021e-01, -4.14542109e-02, -1.05582647e-01, -1.55546203e-01,\n",
            "        6.32830709e-02,  1.06834993e-01, -9.86435339e-02,  7.51124090e-03,\n",
            "       -8.39522183e-02, -1.09830283e-01, -2.86104158e-02,  1.05289333e-01,\n",
            "       -1.09692931e-01,  1.05054498e-01,  1.07300431e-02,  1.82751417e-02,\n",
            "       -4.68385331e-02,  5.67049533e-02, -5.08587807e-03, -1.43625721e-01],\n",
            "      dtype=float32))]\n"
          ]
        }
      ],
      "source": [
        "#Copia de midicc para realizar operaciones\n",
        "midicc_vec = midicc.copy()\n",
        "\n",
        "#asignar el vector resultante a cada elemento del diccionario\n",
        "for x in midicc_vec:\n",
        "  midicc_vec[x] = ft.get_word_vector(x)\n",
        "\n",
        "print('Longitud del vocabulario de vectores embebidos:', len(midicc_vec))\n",
        "print('Las dimensiones de midicc_vec son: {},{}'.format(len(midicc_vec), len(list(midicc_vec.items())[0][1])))   # veamos algunos elementos del diccionario.\n",
        "print(list(midicc_vec.items())[0:1])     # veamos algunos elementos del diccionario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpRFOjOvwArc"
      },
      "source": [
        "# **Pregunta 7**\n",
        "Una manera de utilizar los vectores embebidos con modelos de aprendizaje automático en\n",
        "documentos de texto, es asignar a cada comentario filtrado el vector embebido de dimensión 300\n",
        "que resulta de promediar todos sus tokens. Así, en este ejercicio deberás generar los arreglos\n",
        "correspondientes para los conjuntos de entrenamiento, validación y prueba. Los llamaremos\n",
        "trainEmb, valEmb y testEmb, respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ynopEhb0eMIg"
      },
      "outputs": [],
      "source": [
        "#Funcion para calcular el promedio de vectores en un documento\n",
        "def promedio_de_lista_de_vectores(vec_docs):\n",
        "\n",
        "  #Lista vacía\n",
        "  #vectores_promedio = []\n",
        "\n",
        "  #Ciclo para extraer el documento de vectores\n",
        "  for i in range(0,len(vec_docs)):\n",
        "\n",
        "    if i == 0:\n",
        "\n",
        "      #Se calcula el promedio de todos los vectores\n",
        "      vectores_promedio = [np.mean(vec_docs[i], axis = 0)]\n",
        "\n",
        "    else:\n",
        "      vector_promedio = [np.mean(vec_docs[i], axis = 0)]\n",
        "\n",
        "      #Se añade a la lista de vectores promedio\n",
        "      vectores_promedio = np.append(vectores_promedio, vector_promedio, axis = 0)\n",
        "\n",
        "  return vectores_promedio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "cLtl3z8SfU6O"
      },
      "outputs": [],
      "source": [
        "#función para transformar un conjunto de documentos a sus vectores embebidos en un diccionario\n",
        "def documento_a_documento_de_vectores_embebidos(docs, vect_dic):\n",
        "\n",
        "#Se inicializan lista vacía para el promedio de los vectores\n",
        "  emb_vect_doc = []\n",
        "\n",
        "  #Ciclo para extraer cada documento del conjunto\n",
        "  for doc in docs:\n",
        "\n",
        "    #Lista vacía temporal para juntar los vectores del diccionario\n",
        "    tmp_doc = []\n",
        "\n",
        "    #Ciclo para extraer todos los tokens del documento\n",
        "    for token in doc:\n",
        "\n",
        "      #Se obtiene y añade a la lista el vector correspondiente\n",
        "      tmp_doc.append(vect_dic[token])\n",
        "\n",
        "    #Se añade a la lista de vectores\n",
        "    emb_vect_doc.append(tmp_doc)\n",
        "\n",
        "  return emb_vect_doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "rSqw7RpbmSOt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "0b81a4a6-7d4a-4b47-c49b-d7c7c55cbf88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La cantidad de elementos train_x_vect es: 2090\n",
            "\n",
            "La cantidad de elementos val_x_vect es: 443\n",
            "\n",
            "La cantidad de elementos test_x_vect es: 443\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Se transforman todos los tokens de cada documento a su vector correspondiente en el diccionario\n",
        "train_x_vect = documento_a_documento_de_vectores_embebidos(train_x, midicc_vec)\n",
        "val_x_vect = documento_a_documento_de_vectores_embebidos(val_x, midicc_vec)\n",
        "test_x_vect = documento_a_documento_de_vectores_embebidos(test_x, midicc_vec)\n",
        "\n",
        "print('La cantidad de elementos train_x_vect es: {}\\n'.format(len(train_x_vect)))\n",
        "print('La cantidad de elementos val_x_vect es: {}\\n'.format(len(val_x_vect)))\n",
        "print('La cantidad de elementos test_x_vect es: {}\\n'.format(len(test_x_vect)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebsJniyVwOnr"
      },
      "source": [
        "**¿Cuáles son sus dimensiones?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "9trzJgjjjIGl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1449a6ad-5731-407c-9f56-2cf92fe5c20a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Las dimensiones de trainEmb son: (2090, 300)\n",
            "\n",
            "Las dimensiones de valEmb son: (443, 300)\n",
            "\n",
            "Las dimensiones de testEmb son: (443, 300)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Se calculan los vectores promedio de cada conjunto de documentos\n",
        "trainEmb = promedio_de_lista_de_vectores(train_x_vect)\n",
        "valEmb = promedio_de_lista_de_vectores(val_x_vect)\n",
        "testEmb = promedio_de_lista_de_vectores(test_x_vect)\n",
        "\n",
        "print('Las dimensiones de trainEmb son: {}\\n'.format(trainEmb.shape))\n",
        "print('Las dimensiones de valEmb son: {}\\n'.format(valEmb.shape))\n",
        "print('Las dimensiones de testEmb son: {}\\n'.format(testEmb.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iWFHhXxwTJQ"
      },
      "source": [
        "**¿Se podrían usar para su representación matrices dispersas (sparse matrices) como en el caso de la matriz Tf-idf?**\n",
        "\n",
        "Si, se podrían usar matrices dispersas (sparse matrices) para representar los vectores embebidos en documentos de texto, al igual que en el caso de la matriz Tf-idf. Pero este no operaría de manera eficiente, sus resultados serían más generalizados.\n",
        "\n",
        "Para nuestro caso, al obtener el promedio de todos los vectores presentes en cada documento, se obtuvieron vectores de 300 elementos cada uno con valores distintos a cero. En este caso se tiene una matriz densa, no una dispersa como las que se tratan en Ti-idf.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-unf1wrx3aA"
      },
      "source": [
        "# **Pregunta 8**\n",
        "Utiliza los modelos de regresión lineal y bosque aleatorio (random forest) y encuentra sus\n",
        "desempeños (accuracy). Compara los resultados con los de la semana anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_ByrS1Kvytfg"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jco9mqiqhHRd"
      },
      "source": [
        "Definición del modelo y rango inicial de hiperparámetros:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "AdB-BfdjhA0H"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Definir rango inicial de hiperparámetros\n",
        "max_depth_range = [5, 10, 15, 20, None]\n",
        "min_samples_leaf_range = [1, 2, 4, 8]\n",
        "min_samples_split_range = [2, 5, 10, 20]\n",
        "\n",
        "# Crear modelo inicial\n",
        "rf_model = RandomForestClassifier()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PazpfYN4hJBW"
      },
      "source": [
        "Evaluación manual de diferentes combinaciones de hiperparámetros:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "BfNVosWehD0T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "0e9e90d5-28aa-4d51-9cfb-f8912e9d985d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mejores hiperparámetros: {'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
            "Mejor puntaje de validación cruzada: 0.774\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "best_score = 0\n",
        "best_params = {}\n",
        "\n",
        "for max_depth in max_depth_range:\n",
        "    for min_samples_leaf in min_samples_leaf_range:\n",
        "        for min_samples_split in min_samples_split_range:\n",
        "            rf_model = RandomForestClassifier(max_depth=max_depth,\n",
        "                                              min_samples_leaf=min_samples_leaf,\n",
        "                                              min_samples_split=min_samples_split)\n",
        "\n",
        "            scores = cross_val_score(rf_model, trainEmb, train_y, cv=5)\n",
        "            mean_score = scores.mean()\n",
        "\n",
        "            if mean_score > best_score:\n",
        "                best_score = mean_score\n",
        "                best_params = {\n",
        "                    'max_depth': max_depth,\n",
        "                    'min_samples_leaf': min_samples_leaf,\n",
        "                    'min_samples_split': min_samples_split\n",
        "                }\n",
        "\n",
        "print(f\"Mejores hiperparámetros: {best_params}\")\n",
        "print(f\"Mejor puntaje de validación cruzada: {best_score:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEvhGEiahMdW"
      },
      "source": [
        "Ajuste final con GridSearchCV o RandomizedSearchCV en un rango acotado:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1q93vdDThFT_"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Rango acotado de hiperparámetros basado en resultados previos\n",
        "param_dist = {\n",
        "    'max_depth': [best_params['max_depth'] - 2, best_params['max_depth'], best_params['max_depth'] + 2, None],\n",
        "    'min_samples_leaf': [best_params['min_samples_leaf'] - 1, best_params['min_samples_leaf'], best_params['min_samples_leaf'] + 1],\n",
        "    'min_samples_split': [best_params['min_samples_split'] - 2, best_params['min_samples_split'], best_params['min_samples_split'] + 2]\n",
        "}\n",
        "\n",
        "rf_random = RandomizedSearchCV(rf_model, param_distributions=param_dist, n_iter=20, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "rf_random.fit(trainEmb, train_y)\n",
        "\n",
        "print(f\"Mejores hiperparámetros: {rf_random.best_params_}\")\n",
        "print(f\"Mejor puntaje de validación cruzada: {rf_random.best_score_:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py6ask3whO1T"
      },
      "source": [
        "Evaluación en el conjunto de prueba:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZfVQ2yihU-8"
      },
      "outputs": [],
      "source": [
        "final_model = rf_random.best_estimator_\n",
        "test_accuracy = final_model.score(testEmb, test_y)\n",
        "print(f\"Exactitud en el conjunto de prueba: {test_accuracy:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definición del Modelo y Rango Inicial de Hiperparámetros\n"
      ],
      "metadata": {
        "id": "8vPKtRFMRXNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Definir rango inicial de hiperparámetros\n",
        "C_range = [0.01, 0.1, 1, 10, 100]\n",
        "penalty_options = ['l1', 'l2', 'elasticnet', 'none']\n",
        "\n",
        "# Crear modelo inicial\n",
        "lr_model = LogisticRegression(solver='saga', max_iter=5000)\n"
      ],
      "metadata": {
        "id": "lKcqt3twRT9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluación Manual de Diferentes Combinaciones de Hiperparámetros\n"
      ],
      "metadata": {
        "id": "GZsvjP9RRagD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_score = 0\n",
        "best_params = {}\n",
        "\n",
        "for C in C_range:\n",
        "    for penalty in penalty_options:\n",
        "        try:\n",
        "            lr_model = LogisticRegression(C=C, penalty=penalty, solver='saga', max_iter=5000)\n",
        "            scores = cross_val_score(lr_model, trainEmb, train_y, cv=5)\n",
        "            mean_score = scores.mean()\n",
        "            if mean_score > best_score:\n",
        "                best_score = mean_score\n",
        "                best_params = {\n",
        "                    'C': C,\n",
        "                    'penalty': penalty\n",
        "                }\n",
        "        except ValueError:\n",
        "            # Algunos penaltys pueden no ser compatibles con todos los solvers\n",
        "            continue\n",
        "\n",
        "print(f\"Mejores hiperparámetros: {best_params}\")\n",
        "print(f\"Mejor puntaje de validación cruzada: {best_score:.3f}\")\n"
      ],
      "metadata": {
        "id": "gVSQaK3GRa2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ajuste Final con GridSearchCV o RandomizedSearchCV\n"
      ],
      "metadata": {
        "id": "etxl8Ry4NpiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Rango acotado de hiperparámetros basado en resultados previos\n",
        "param_dist = {\n",
        "    'C': [best_params['C'] / 2, best_params['C'], best_params['C'] * 2],\n",
        "    'penalty': [best_params['penalty']],\n",
        "    'solver': ['saga'],  # Usamos saga para elasticnet y l1\n",
        "    'l1_ratio': [0.5] if best_params['penalty'] == 'elasticnet' else [None]  # Solo necesario para elasticnet\n",
        "}\n",
        "\n",
        "# Crear y ajustar el modelo con RandomizedSearchCV\n",
        "lr_random = RandomizedSearchCV(lr_model, param_distributions=param_dist, n_iter=20, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "lr_random.fit(trainEmb, train_y)\n",
        "\n",
        "print(f\"Mejores hiperparámetros: {lr_random.best_params_}\")\n",
        "print(f\"Mejor puntaje de validación cruzada: {lr_random.best_score_:.3f}\")\n"
      ],
      "metadata": {
        "id": "bXVuMYdFNqUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluación en el Conjunto de Prueba\n"
      ],
      "metadata": {
        "id": "bLmMyklBNwpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = lr_random.best_estimator_\n",
        "test_accuracy = final_model.score(testEmb, test_y)\n",
        "print(f\"Exactitud en el conjunto de prueba: {test_accuracy:.3f}\")\n"
      ],
      "metadata": {
        "id": "0mse2OZVNxor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcHKK4bIb2j-"
      },
      "source": [
        "*Se obtienen resultados similares a los obtenidos en la semana anterior. El modelo de regresión logística logra exactitudes por encima del 72% y el modelo de Bosques Aleatorios logra resultados similares y conserva su tendencia a sobreentrenarse. Para evitar lo anterios, se probaron varios métodos:*\n",
        "\n",
        "1. *Implementar validación cruzada.*\n",
        "2. *Reducir el número de estimadores.*\n",
        "\n",
        "*El primero no ayudó a reducir la tendencia a sobreentrenar el modelo. El seguno al reducirse a un valor de entr 2 y 10, logró con efectividad reducir el sobreentrenamiento. Sin embargo, la exactitud del conjunto de validación, también resultaba afectada negativamente, por lo tanto, se conservó el modelo sobreentrenado, ya que a pesar de esto, también logra los mejores resultados en el conjunto de validación. Entrenar el modelo con más datos, parece ser la mejor opción para lograr reducir el sobreentrenamiento en el modelo de bosques aleatorios.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3jy454oblEv"
      },
      "source": [
        "# **Pregunta 9**\n",
        "Obtener la matriz de confusión e interpretar sus valores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkibSchL_T_x"
      },
      "outputs": [],
      "source": [
        "def mi_cm(yreal, ypred):\n",
        "  cm = confusion_matrix(yreal, ypred)\n",
        "  txt = ['Verdaderos Negativos\\n(VN)','Falsos Positivos\\n(FP)',\\\n",
        "         'Falsos Negativos\\n(FN)', 'Verdaderos Positivos\\n(VP)']\n",
        "  frecuencia = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\n",
        "  porcentaje = [\"{0:.1%}\".format(value) for value in cm.flatten()/np.sum(cm)]\n",
        "\n",
        "  labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(txt,frecuencia,porcentaje)]\n",
        "  labels = np.asarray(labels).reshape(2,2)\n",
        "\n",
        "  ax = sns.heatmap(cm, annot=labels, fmt='', cmap='Spectral', cbar=False)\n",
        "  ax.set(ylabel=\"Etiquetas Reales\", xlabel=\"Etiquetas de Predicción\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGlYDiAof3Wq"
      },
      "outputs": [],
      "source": [
        "#Se imprimen los resultados del conjunto de prueba y las matrices de confusión para cada modelo\n",
        "print('Test-accuracy con el mejor modelo de Regresión Logística: %.2f%%' % (100*modeloLRcount.score(testEmb, test_y)))\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "pred1 = modeloLRcount.predict(testEmb)\n",
        "print('\\nMatriz de confusión con el mejor Regresión Logística:\\n')\n",
        "mi_cm(test_y, pred1)\n",
        "\n",
        "print('\\nTest-accuracy con el mejor modelo de Bosques Aleatorios: %.2f%%' % (100*modeloRFcount.score(testEmb, test_y)))\n",
        "print('\\nMatriz de confusión con el mejor Bosques Aleatorios:\\n')\n",
        "pred2 = modeloRFcount.predict(testEmb)\n",
        "mi_cm(test_y, pred2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QglSgTQkelAN"
      },
      "source": [
        "*Con relación a los resultados de las matrices de confusión por medio de ambos métodos, el modelo de bosques aleatorios obtuvo valor menor de clasificaciones verdaderas positivas, pero mayor en verdaderas negativas, logrando una cantidad neta de clasificaciones correctas mayor que el método de regresión logística. Se logró reducir el número de clasificaciones falsas negativas y se incrementó proporcionalmente el número falsos positivos. Al igual que lo descrito anteriormente, el valor neto clasificaciones incorrectas es menor que por el método de conteo. Lo anterior en resumen, implica que el método de bosques aleatorios es mejor para los algoritmos de clasificación, para diferenciar mejor las clases negativas en general que el método de conteo a cambio de una ligera degradación en la clasificación de clases positivas.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4CmKx0z6NNb"
      },
      "source": [
        "# **Pregunta 10**\n",
        "Comenta con tus compañeros de equipo los pasos realizados en esta actividad e incluyan sus conclusiones finales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGcUa5OhB_oP"
      },
      "source": [
        ">>> *Agregar aquí conclusiones del trabajo*\n",
        "\n",
        "*Dentro de la limpieza realizada se encontraron algunas validaciones que mejoraron el desempeño del modelo dado que no se procesaron ciertos valores o caracteres que no son relevantes para el estudio y alteran el resultado esperado. Posteriormente, se ejecutaron las particiones de los elementos para el desarrollo de los modelos; para ello, se acotaron los caracteres dentro del diccionario del conjunto de entrenamiento dado que se involucraron únicamente palabras relevantes las cuales tienen una mayor cantidad de frecuencia dentro de los datos.*\n",
        "\n",
        "*Luego del preprocesamiento anterior, se inició la validación del modelo **FastText** con el fin de identificar la relación que tienen las palabras que contiene el diccionario definido. Luego de inicializarlo, se desarrollaron las funciones requeridas para obtener el promedio de los vectores y de allí otra función que transforme los documentos a vectores embebidos con el fin de asignarles valores a los vectores e identificar la correlación de cada uno entre ellos.*\n",
        "\n",
        "*Con los procedimientos anteriores, se utilizaron dichos vectores para optimizar los modelos de Random Forest y Linear Regression, obteniendo su desempeño y la validación de estos. Detectando los hiperparámetros requeridos para cada modelo y obteniendo los resultados del desempeño de cada uno. **FastText** permite una mejor predicción dado que asigna una mejor relación entre las palabras que únicamente valores de 1 y 0 para los caracteres obtenidos, distorsionando el valor del desempeño de los modelos.*\n",
        "\n",
        "*Para este caso, el método de transformar el corpus a sus vectores preentrenados embebidos, después de el proceso de limpieza y lematización, para después calcular el promedio de los vectores para cada comentario, resultó en una desempeño menor que el que se obtuvo para el mismo conjunto de datos y el uso de la matriz dispersa por conteo de frecuencias y de tf-idf. Con lo anterior, se pudo comprender el valor de tener cada palabra representada como un elemento de un espacio vectorial para encontrar la relación entre las palabras cercanas, sin embargo, para este caso, el uso del promedio de dichos vectores no resultó superior en exactitud a los resultados logrados con las matrices dispersas. Posiblemente, los resultados se pueden mejorar si emplea una simplificación del comentario diferente al promedio de los vectores, como lo sería un proceso de reducción tomando en cuenta la relación que tiene una palabra con otra.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK_RJaCNBEoj"
      },
      "source": [
        "##### **Fuentes bibliográficas y de datos:**\n",
        "- https://raw.githubusercontent.com/Handrum/NLP_EQ_6/main/amazon5.txt\n",
        "- https://raw.githubusercontent.com/Handrum/NLP_EQ_6/main/imdb5.txt\n",
        "- https://raw.githubusercontent.com/Handrum/NLP_EQ_6/main/yelp5.txt\n",
        "- https://fasttext.cc/\n",
        "- https://fasttext.cc/docs/en/crawl-vectors.html\n",
        "- https://code.google.com/archive/p/word2vec/\n",
        "- https://nlp.stanford.edu/projects/glove/\n",
        "- https://medium.com/analytics-vidhya/word2vec-glove-fasttext-and-baseline-word-embeddings-step-by-step-d0489c15d10b\n",
        "\n",
        "- Vajjala, S., Majumder, B., Gupta, A., y Surana, H. (2020). Practical Natural Language Processing: A Comprehensive Guide to Building Real-World NLP Systems. O'Reilly. https://learning.oreilly.com/library/view/practical-natural-language/9781492054047/\n",
        "\n",
        "\n",
        "- Khurana, D., Koli, A., Khatter, K., y Singh, S. (2023). Natural language processing: state of the art, current trends and challenges. Multimed Tools Appl 82, 3713–3744. https://link.springer.com/article/10.1007/s11042-022-13428-4Links to an external site.\n",
        "\n",
        "- Falcón Morales, L. E. (2023). Bolsa de palabras: BOW [PDF]. Maestría en Inteligencia Artificial Aplicada. ITESM. Acceso al material Download Acceso al material"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}